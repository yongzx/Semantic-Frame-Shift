{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "UMAP-frame-embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "813026cde56c4ea2b1f014ac05461257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8569dd16accc4196bbd02fb6cded321d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_502e86512b7e4c0c9af833112313b95c",
              "IPY_MODEL_bab9dacd86d644c6a58ed4b1255b97c8"
            ]
          }
        },
        "8569dd16accc4196bbd02fb6cded321d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "502e86512b7e4c0c9af833112313b95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_69f53b2b2c814dd4871684cbd94ed397",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fa94da568b248a7a6e52b105f7e840c"
          }
        },
        "bab9dacd86d644c6a58ed4b1255b97c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a65f5d3b4c8c45acb2f495e0d9b6ab98",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466/466 [00:12&lt;00:00, 36.0B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e90b87f836994e99ac464ffc73a11b31"
          }
        },
        "69f53b2b2c814dd4871684cbd94ed397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5fa94da568b248a7a6e52b105f7e840c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a65f5d3b4c8c45acb2f495e0d9b6ab98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e90b87f836994e99ac464ffc73a11b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0e76bdbf0d14c00b3b14eda83c261fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f9137988060e4ac9a779fde44cac9a5c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8818fc5f0a1b47709b1e7ed51a7c1645",
              "IPY_MODEL_65ef6888909340c5a7f9834cc70c4063"
            ]
          }
        },
        "f9137988060e4ac9a779fde44cac9a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8818fc5f0a1b47709b1e7ed51a7c1645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_505a8736c0c344db900a68ba92173c53",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_427118a9f92f42a7bae489951b8acc43"
          }
        },
        "65ef6888909340c5a7f9834cc70c4063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c9277ee7a4e84536945deff49933e58b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 3.62MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e00aed8c1683446cbbc93505c5fb1ab9"
          }
        },
        "505a8736c0c344db900a68ba92173c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "427118a9f92f42a7bae489951b8acc43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9277ee7a4e84536945deff49933e58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e00aed8c1683446cbbc93505c5fb1ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ac024736f594b6ea33d7efcbce89f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f93cb0f5cd784602abffc32c7bc38a60",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c306f5750c1455abc48f1fb0f5d7cfb",
              "IPY_MODEL_46e1d348965c4483a37cd6b4a18b98af"
            ]
          }
        },
        "f93cb0f5cd784602abffc32c7bc38a60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c306f5750c1455abc48f1fb0f5d7cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_accfc19b31244075a228c49edf441c64",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 541808922,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 541808922,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a53ee3be2ce4817bf5ada5aab64b32e"
          }
        },
        "46e1d348965c4483a37cd6b4a18b98af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8be7833d20543458cf843ce3b5d412f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 542M/542M [00:08&lt;00:00, 62.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2d08293152341ef965e9de1de552a4b"
          }
        },
        "accfc19b31244075a228c49edf441c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a53ee3be2ce4817bf5ada5aab64b32e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8be7833d20543458cf843ce3b5d412f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2d08293152341ef965e9de1de552a4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie1KhuhinkEJ"
      },
      "source": [
        "## Download Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaLKIUg85u6J",
        "outputId": "36e33e37-ee8c-410e-d7c3-01dcd034dd79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.1\n",
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSUjlM_66HLg",
        "outputId": "9fa18b94-b577-490c-db45-1ff41f3454af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-geometric\n",
        "!pip install flair\n",
        "!pip install laserembeddings\n",
        "!pip install dataclasses\n",
        "!pip install dill"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-scatter==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (11.9MB)\n",
            "\u001b[K     |████████████████████████████████| 11.9MB 256kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-sparse==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (24.3MB)\n",
            "\u001b[K     |████████████████████████████████| 24.3MB 139kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==latest+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.8\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-cluster==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 166kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.8\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-spline-conv==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_spline_conv-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.4MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4MB 5.2MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/67/6c0bce6b6e6bc806e25d996e46a686e5a11254d89257983265a988bb02ee/torch_geometric-1.6.1.tar.gz (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.4)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/78/edadb45c7f26f8fbb99da81feadb561c26bb0393b6c5d1ac200ecdc12d61/ase-3.20.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 15.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (3.7.4.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.17.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (50.3.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.1-cp36-none-any.whl size=308552 sha256=ec9194d97a1c837bd212cc7ada650cbe92341a586649660ad7f09e76713d2f10\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/25/ea/3d71d2088dccc63214fa59259dcc598ded4150a5f8b41d84ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.20.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.1\n",
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/49/a812ed93088ba9519cbb40eb9f52341694b31cfa126bfddcd9db3761f3ac/flair-0.6.1.post1-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 31.6MB/s \n",
            "\u001b[?25hCollecting transformers>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 35.1MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.2MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->flair) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown->flair) (2.23.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (3.0.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 64.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.17.0)\n",
            "Collecting overrides==3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (2.10)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=3.0.0->flair) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: langdetect, ftfy, segtok, mpld3, sqlitedict, sacremoses, overrides\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=fa6c77285a2a4d84f4794db6843cb5206071c7f141093ad7297ed8411eda9c13\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=2f265cbac2f2b99c027443731eefedf5763a6259520e397a370758a12153962a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25021 sha256=936b301e74a2ec1ada2ba032b44e817914bb9b14d1762419d96d12fad7869bc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116677 sha256=dd18cfbbd53b865ec3fc16591629d09156912f3988468be19f0ff267044c112a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14377 sha256=4237574bd1f0389d7ab73f11f38596dc87f91bb03b99998966e0921b7a7a542c\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=02188446009fde1a0395d9f995e2c9e159433f4ef50108eee3c176abd8974e52\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=2b8445f765d08644d1c03cbf291e46e319b66ec53694dc0dec69fcf8c2f342e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "Successfully built langdetect ftfy segtok mpld3 sqlitedict sacremoses overrides\n",
            "Installing collected packages: deprecated, langdetect, sentencepiece, ftfy, segtok, mpld3, sacremoses, tokenizers, transformers, sqlitedict, janome, bpemb, overrides, konoha, flair\n",
            "Successfully installed bpemb-0.3.2 deprecated-1.2.10 flair-0.6.1.post1 ftfy-5.8 janome-0.4.1 konoha-4.6.2 langdetect-1.0.8 mpld3-0.3 overrides-3.0.0 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.94 sqlitedict-1.7.0 tokenizers-0.9.2 transformers-3.4.0\n",
            "Collecting laserembeddings\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/4b/a9e3ee9f4825bd2bb6b48f26370e2c341860ec0cb2a9a27deea9be6c2299/laserembeddings-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: torch<2.0.0,>=1.0.1.post2 in /usr/local/lib/python3.6/dist-packages (from laserembeddings) (1.7.0+cu101)\n",
            "Collecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 4.3MB/s \n",
            "\u001b[?25hCollecting transliterate==1.10.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/6e/9a9d597dbdd6d0172427c8cc07c35736471e631060df9e59eeb87687f817/transliterate-1.10.2-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hCollecting subword-nmt<0.4.0,>=0.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from laserembeddings) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings) (0.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings) (4.41.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=d68b90457f224f9a5d4381e3cbf486bff1178865ea1ef6d4504013869fab54c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, transliterate, subword-nmt, laserembeddings\n",
            "  Found existing installation: sacremoses 0.0.43\n",
            "    Uninstalling sacremoses-0.0.43:\n",
            "      Successfully uninstalled sacremoses-0.0.43\n",
            "Successfully installed laserembeddings-1.1.0 sacremoses-0.0.35 subword-nmt-0.3.7 transliterate-1.10.2\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (0.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (0.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da262N3UESEU"
      },
      "source": [
        "### **Load FrameNet Graph into Graph Attention Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JdDHW6Mcv0C",
        "outputId": "e01e68e0-3763-4da8-fc1b-b14cba7c80bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(2020) # seed for reproducible numbers\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib notebook\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"framenet_v17\")\n",
        "from nltk.corpus import framenet as fn\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "print(\"...creating networkx FN...\")\n",
        "G = nx.DiGraph()\n",
        "for frame in fn.frames():\n",
        "    G.add_node(frame.ID)\n",
        "    for adj in frame.frameRelations:\n",
        "        G.add_edge(adj.superFrame.ID, adj.subFrame.ID)\n",
        "        G.add_edge(adj.subFrame.ID, adj.superFrame.ID)\n",
        "\n",
        "# initialize frame embeddings with LASER sentence representations \n",
        "print(\"...embedding frames...\")\n",
        "!python -m laserembeddings download-models\n",
        "from laserembeddings import Laser\n",
        "laser = Laser()\n",
        "sentences = [fn.frame(frameID).definition for frameID in G.nodes]\n",
        "frame_embeddings = laser.embed_sentences(sentences, lang='en')\n",
        "\n",
        "# convert networkx G into torch.geometric graph\n",
        "print(\"...generating torch_geometric graph...\")\n",
        "# x = torch.from_numpy(np.array(G.nodes).reshape(-1, 1)).float()  # x.shape = (1221, 1)\n",
        "x = torch.from_numpy(frame_embeddings)  # x.shape = (1221, 1024)\n",
        "nodes_to_x = {node: i for i, node in enumerate(G.nodes)}  # map frame ID to index position in x\n",
        "x_to_nodes = {i: node for i, node in enumerate(G.nodes)}  # reverse of nodes_to_x\n",
        "edge_index = torch.Tensor(list(set([(nodes_to_x[src], nodes_to_x[tgt]) for src, tgt in G.edges]))).long()\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = data.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/framenet_v17.zip.\n",
            "...creating networkx FN...\n",
            "...embedding frames...\n",
            "Downloading models into /usr/local/lib/python3.6/dist-packages/laserembeddings/data\n",
            "\n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
            "\n",
            "✨ You're all set!\n",
            "...generating torch_geometric graph...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sHklG7UcYK2"
      },
      "source": [
        "### **Auxiliary Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDX9J3_IfrMz"
      },
      "source": [
        "#### Any-Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tDPuWh8H7N2",
        "outputId": "d8c92caf-a47d-4312-99c2-fcc61b677681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# monolingual task\n",
        "import nltk\n",
        "nltk.download('framenet_v17')\n",
        "from nltk.corpus import framenet as fn\n",
        "import torch\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    annofile: str\n",
        "    frameName: str\n",
        "    luName: str = ''\n",
        "    lu_idx: list = field(default_factory=list)  # [(start_LU_idx, end_LU_idx [exclusive of space], id), ...]\n",
        "    fe_idx: list = field(default_factory=list) # [(start_FE_idx, end_FE_idx [exclusive of space], feName, id), ...]\n",
        "\n",
        "    # tokenized by flair\n",
        "    tokenized_text: str = ''\n",
        "    tokenized_lu_idx: list = field(default_factory=list)  # [(token_idx, LU), ...]\n",
        "    tokenized_frame_idx: list = field(default_factory=list)  # [(token_idx, frame), ...]\n",
        "    tokenized_fe_idx: list = field(default_factory=list)  # [(token_idx, FE), ...]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KKvsjfo2lbK"
      },
      "source": [
        "#### Load Auxiliary Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvKC1HpeZ6MU"
      },
      "source": [
        "# torch.save(annos, \"any-language-frames/annos.pt\")\n",
        "any_annos = torch.load(\"any-language-frames/annos_fn_pos_tags.pt\")\n",
        "bfn_annos = torch.load(\"/content/Capstone/frame_embeddings_BFN/annos.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th32t1MXd-0e"
      },
      "source": [
        "### **Actual Task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2UhFAddnyto"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK24PCHwFWR6",
        "outputId": "c63cba6d-7e81-4336-ae51-e8e6305a2bb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# actual task\n",
        "import nltk\n",
        "nltk.download('framenet_v17')\n",
        "from nltk.corpus import framenet as fn\n",
        "import globalfn\n",
        "from globalfn.annotations import annotation, all_annotations, annotation_annoID\n",
        "from globalfn.alignments import all_alignments\n",
        "import re\n",
        "\n",
        "def extract_annoID(line):\n",
        "    \"\"\"Extract annoID from line\"\"\"\n",
        "    return int(re.findall(r'\\d+', line)[0])\n",
        "\n",
        "div_D = {}\n",
        "\n",
        "# same en-pt annotation pairs\n",
        "print(\"Load en-pt.results.txt\")\n",
        "with open(\"Capstone/en-pt.results.txt\", \"r\") as rf:\n",
        "    anno1 = anno2 = None\n",
        "    for line in rf:\n",
        "        if line.strip() == \"===============================\":\n",
        "            anno1 = anno2 = None\n",
        "\n",
        "        if \"annoID\" in line and anno1 is None:\n",
        "            anno1 = extract_annoID(line)\n",
        "        elif \"annoID\" in line and anno2 is None:\n",
        "            anno2 = extract_annoID(line)\n",
        "\n",
        "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
        "            tgt_frame_id = fn.frames(annotation_annoID('pt', anno2).frameName)[0].ID\n",
        "            div_D[(anno1, anno2, 'pt')] = (src_frame_id, tgt_frame_id)\n",
        "\n",
        "# same en-de annotation pairs\n",
        "print(\"Load en-de.results.txt\")\n",
        "with open(\"Capstone/en-de.results.txt\", \"r\") as rf:\n",
        "    anno1 = anno2 = None\n",
        "    for line in rf:\n",
        "        if line.strip() == \"===============================\":\n",
        "            anno1 = anno2 = None\n",
        "\n",
        "        if \"annoID\" in line and anno1 is None:\n",
        "            anno1 = extract_annoID(line)\n",
        "        elif \"annoID\" in line and anno2 is None:\n",
        "            anno2 = extract_annoID(line)\n",
        "\n",
        "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
        "            tgt_frame_id = fn.frames(annotation_annoID('de', anno2).frameName)[0].ID\n",
        "            div_D[(anno1, anno2, 'de')] = (src_frame_id, tgt_frame_id)\n",
        "\n",
        "# diverging frames en-pt annotation pairs\n",
        "print(\"Load en-pt.same.results.txt\")\n",
        "with open(\"Capstone/en-pt.same.results.txt\", \"r\") as rf:\n",
        "    anno1 = anno2 = None\n",
        "    for line in rf:\n",
        "        if line.strip() == \"===============================\":\n",
        "            anno1 = anno2 = None\n",
        "\n",
        "        if \"annoID\" in line and anno1 is None:\n",
        "            anno1 = extract_annoID(line)\n",
        "        elif \"annoID\" in line and anno2 is None:\n",
        "            anno2 = extract_annoID(line)\n",
        "\n",
        "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
        "            tgt_frame_id = fn.frames(annotation_annoID('pt', anno2).frameName)[0].ID\n",
        "            div_D[(anno1, anno2, 'pt')] = (src_frame_id, tgt_frame_id)\n",
        "\n",
        "# diverging frames en-de annotation pairs\n",
        "print(\"Load en-de.same.results.txt\")\n",
        "with open(\"Capstone/en-de.same.results.txt\", \"r\") as rf:\n",
        "    anno1 = anno2 = None\n",
        "    for line in rf:\n",
        "        if line.strip() == \"===============================\":\n",
        "            anno1 = anno2 = None\n",
        "\n",
        "        if \"annoID\" in line and anno1 is None:\n",
        "            anno1 = extract_annoID(line)\n",
        "        elif \"annoID\" in line and anno2 is None:\n",
        "            anno2 = extract_annoID(line)\n",
        "\n",
        "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
        "            tgt_frame_id = fn.frames(annotation_annoID('de', anno2).frameName)[0].ID\n",
        "            div_D[(anno1, anno2, 'de')] = (src_frame_id, tgt_frame_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Package framenet_v17 is already up-to-date!\n",
            "Load en-pt.results.txt\n",
            "Load en-de.results.txt\n",
            "Load en-pt.same.results.txt\n",
            "Load en-de.same.results.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb0aKoqrKzj_",
        "outputId": "46eba287-5e04-4a88-979c-d393bd2f1d83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# load frames, LUs, and sentence annotations\n",
        "\n",
        "frames = set()\n",
        "lus = set()\n",
        "sents = set()\n",
        "\n",
        "for (anno1, anno2, lang), (src_frame_id, tgt_frame_id) in div_D.items():\n",
        "    frames.add(src_frame_id)\n",
        "    frames.add(tgt_frame_id)\n",
        "\n",
        "    lus.add(annotation_annoID('en', anno1).luName)\n",
        "    lus.add(annotation_annoID(lang, anno2).luName)\n",
        "\n",
        "    sents.add(anno1)\n",
        "    sents.add(anno2)\n",
        "print(len(frames), len(lus), len(sents))  # check length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179 476 788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrXSz16T4JjA",
        "outputId": "d65d1b9d-694f-4cd4-f024-a749085f00a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "813026cde56c4ea2b1f014ac05461257",
            "8569dd16accc4196bbd02fb6cded321d",
            "502e86512b7e4c0c9af833112313b95c",
            "bab9dacd86d644c6a58ed4b1255b97c8",
            "69f53b2b2c814dd4871684cbd94ed397",
            "5fa94da568b248a7a6e52b105f7e840c",
            "a65f5d3b4c8c45acb2f495e0d9b6ab98",
            "e90b87f836994e99ac464ffc73a11b31",
            "a0e76bdbf0d14c00b3b14eda83c261fd",
            "f9137988060e4ac9a779fde44cac9a5c",
            "8818fc5f0a1b47709b1e7ed51a7c1645",
            "65ef6888909340c5a7f9834cc70c4063",
            "505a8736c0c344db900a68ba92173c53",
            "427118a9f92f42a7bae489951b8acc43",
            "c9277ee7a4e84536945deff49933e58b",
            "e00aed8c1683446cbbc93505c5fb1ab9",
            "7ac024736f594b6ea33d7efcbce89f7f",
            "f93cb0f5cd784602abffc32c7bc38a60",
            "9c306f5750c1455abc48f1fb0f5d7cfb",
            "46e1d348965c4483a37cd6b4a18b98af",
            "accfc19b31244075a228c49edf441c64",
            "5a53ee3be2ce4817bf5ada5aab64b32e",
            "c8be7833d20543458cf843ce3b5d412f",
            "a2d08293152341ef965e9de1de552a4b"
          ]
        }
      },
      "source": [
        "# load multilingual BERT model for LU embedding\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "mbert = TransformerWordEmbeddings('distilbert-base-multilingual-cased', layers='-1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "813026cde56c4ea2b1f014ac05461257",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0e76bdbf0d14c00b3b14eda83c261fd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ac024736f594b6ea33d7efcbce89f7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=541808922.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peQZE00iM7kz",
        "outputId": "0c60ff81-4e4a-4159-8139-1b272b898697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create embeddings for LUs\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_lu_embedding(lang_model, anno_lang, anno_ID):\n",
        "    tokenized_text = annotation_annoID(anno_lang, anno_ID).tokenized_text\n",
        "    sent = Sentence(tokenized_text, use_tokenizer=False)\n",
        "    lang_model.embed(sent)\n",
        "\n",
        "    tmp_embeds = list()\n",
        "    for i, tok in enumerate(sent):\n",
        "        if annotation_annoID(anno_lang, anno_ID).tokenized_frame_idx[i] != '-':\n",
        "            tmp_embeds.append(tok.embedding)\n",
        "    final_embeds = torch.mean(torch.stack(tmp_embeds), dim=0)\n",
        "    return final_embeds\n",
        "\n",
        "tgt_L = list()\n",
        "for _, anno_ID, lang in div_D.keys():\n",
        "    print(anno_ID)\n",
        "    embedding = get_lu_embedding(mbert, lang, anno_ID)\n",
        "    tgt_L.append(embedding)\n",
        "\n",
        "src_L = list()\n",
        "for anno_ID, _, lang in div_D.keys():\n",
        "    print(anno_ID)\n",
        "    embedding = get_lu_embedding(mbert, 'en', anno_ID)\n",
        "    src_L.append(embedding)\n",
        "\n",
        "tgt_lus = torch.stack(tgt_L).float().to(device)\n",
        "src_lus = torch.stack(src_L).float().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1250\n",
            "1294\n",
            "2275\n",
            "6923\n",
            "2359\n",
            "2313\n",
            "7698\n",
            "6983\n",
            "2307\n",
            "7292\n",
            "1394\n",
            "2386\n",
            "1292\n",
            "1580\n",
            "2347\n",
            "1131\n",
            "1105\n",
            "1212\n",
            "4337\n",
            "1328\n",
            "1346\n",
            "2256\n",
            "1488\n",
            "1107\n",
            "6953\n",
            "2215\n",
            "1340\n",
            "1501\n",
            "1471\n",
            "2264\n",
            "2268\n",
            "1466\n",
            "2345\n",
            "7626\n",
            "1590\n",
            "7357\n",
            "6987\n",
            "1502\n",
            "1465\n",
            "1347\n",
            "2182\n",
            "1552\n",
            "7329\n",
            "1291\n",
            "6768\n",
            "2260\n",
            "1381\n",
            "1061\n",
            "7341\n",
            "7496\n",
            "6885\n",
            "1410\n",
            "6680\n",
            "4296\n",
            "1592\n",
            "4299\n",
            "1248\n",
            "2258\n",
            "1085\n",
            "7298\n",
            "7535\n",
            "6790\n",
            "1872\n",
            "6978\n",
            "1400\n",
            "7368\n",
            "7527\n",
            "6602\n",
            "7471\n",
            "2205\n",
            "6984\n",
            "10790\n",
            "9316\n",
            "9900\n",
            "10377\n",
            "9336\n",
            "10804\n",
            "9720\n",
            "10373\n",
            "10780\n",
            "9159\n",
            "10373\n",
            "9159\n",
            "9310\n",
            "9716\n",
            "9347\n",
            "10713\n",
            "9321\n",
            "10715\n",
            "10794\n",
            "9304\n",
            "9340\n",
            "9327\n",
            "9724\n",
            "9266\n",
            "9097\n",
            "10420\n",
            "9297\n",
            "9438\n",
            "10378\n",
            "10784\n",
            "10737\n",
            "9094\n",
            "10798\n",
            "9691\n",
            "10717\n",
            "2206\n",
            "1882\n",
            "7302\n",
            "4353\n",
            "1178\n",
            "1496\n",
            "1216\n",
            "1395\n",
            "6930\n",
            "1158\n",
            "7361\n",
            "4338\n",
            "2240\n",
            "2383\n",
            "6763\n",
            "7303\n",
            "6901\n",
            "1150\n",
            "7160\n",
            "1125\n",
            "7257\n",
            "1582\n",
            "7194\n",
            "2213\n",
            "7244\n",
            "2343\n",
            "6802\n",
            "1253\n",
            "1155\n",
            "7191\n",
            "1359\n",
            "6678\n",
            "6554\n",
            "1540\n",
            "7388\n",
            "2326\n",
            "1493\n",
            "2296\n",
            "4301\n",
            "1882\n",
            "6686\n",
            "2328\n",
            "7297\n",
            "2237\n",
            "2394\n",
            "7627\n",
            "2239\n",
            "1267\n",
            "7193\n",
            "1161\n",
            "2384\n",
            "4361\n",
            "1499\n",
            "1030\n",
            "4351\n",
            "7669\n",
            "2358\n",
            "2187\n",
            "1498\n",
            "1483\n",
            "7650\n",
            "6555\n",
            "7468\n",
            "7417\n",
            "6846\n",
            "2393\n",
            "1143\n",
            "1140\n",
            "1184\n",
            "2380\n",
            "1323\n",
            "1142\n",
            "6726\n",
            "6942\n",
            "1259\n",
            "1076\n",
            "7287\n",
            "1149\n",
            "1176\n",
            "6600\n",
            "1124\n",
            "7550\n",
            "7296\n",
            "4377\n",
            "7251\n",
            "1585\n",
            "6814\n",
            "1287\n",
            "7624\n",
            "2331\n",
            "2371\n",
            "7416\n",
            "4356\n",
            "1539\n",
            "4302\n",
            "7469\n",
            "6888\n",
            "1097\n",
            "7533\n",
            "1332\n",
            "1153\n",
            "6568\n",
            "2245\n",
            "1315\n",
            "6985\n",
            "6671\n",
            "6958\n",
            "6868\n",
            "6564\n",
            "6872\n",
            "1319\n",
            "2298\n",
            "1297\n",
            "1138\n",
            "6967\n",
            "1327\n",
            "4352\n",
            "1868\n",
            "6990\n",
            "7425\n",
            "7356\n",
            "1164\n",
            "1288\n",
            "2253\n",
            "6862\n",
            "7300\n",
            "1495\n",
            "7398\n",
            "6717\n",
            "1336\n",
            "2319\n",
            "6711\n",
            "7484\n",
            "6861\n",
            "4354\n",
            "1283\n",
            "7661\n",
            "7285\n",
            "1587\n",
            "6841\n",
            "1404\n",
            "7239\n",
            "1154\n",
            "1221\n",
            "1199\n",
            "6965\n",
            "7682\n",
            "1159\n",
            "6924\n",
            "1104\n",
            "2246\n",
            "6994\n",
            "6816\n",
            "2243\n",
            "4355\n",
            "7539\n",
            "6583\n",
            "7242\n",
            "7240\n",
            "1263\n",
            "6669\n",
            "4369\n",
            "1479\n",
            "1032\n",
            "7254\n",
            "6842\n",
            "2201\n",
            "7474\n",
            "2353\n",
            "7118\n",
            "1108\n",
            "1139\n",
            "7163\n",
            "6767\n",
            "1068\n",
            "2392\n",
            "1106\n",
            "4360\n",
            "7342\n",
            "1467\n",
            "2244\n",
            "7338\n",
            "1345\n",
            "1494\n",
            "1354\n",
            "7174\n",
            "1144\n",
            "1581\n",
            "4297\n",
            "1591\n",
            "2338\n",
            "7352\n",
            "1278\n",
            "2344\n",
            "2354\n",
            "2356\n",
            "1348\n",
            "7117\n",
            "7607\n",
            "1866\n",
            "1235\n",
            "1095\n",
            "1344\n",
            "7336\n",
            "4392\n",
            "1017\n",
            "7175\n",
            "2214\n",
            "6989\n",
            "1379\n",
            "7488\n",
            "1475\n",
            "1163\n",
            "4357\n",
            "4290\n",
            "1215\n",
            "2236\n",
            "1014\n",
            "6725\n",
            "6848\n",
            "4358\n",
            "1285\n",
            "1382\n",
            "7415\n",
            "1542\n",
            "1016\n",
            "1162\n",
            "6604\n",
            "1525\n",
            "6716\n",
            "7375\n",
            "2200\n",
            "6569\n",
            "4292\n",
            "6571\n",
            "1500\n",
            "1217\n",
            "2202\n",
            "1043\n",
            "1341\n",
            "7313\n",
            "1012\n",
            "7492\n",
            "1172\n",
            "1093\n",
            "9695\n",
            "9690\n",
            "9096\n",
            "9246\n",
            "9343\n",
            "9718\n",
            "10708\n",
            "9314\n",
            "9299\n",
            "10370\n",
            "10376\n",
            "10817\n",
            "9097\n",
            "9323\n",
            "9162\n",
            "10381\n",
            "9355\n",
            "9313\n",
            "10375\n",
            "10781\n",
            "9318\n",
            "10723\n",
            "9903\n",
            "10813\n",
            "9890\n",
            "9896\n",
            "10819\n",
            "9895\n",
            "9315\n",
            "10707\n",
            "10738\n",
            "9330\n",
            "9291\n",
            "10812\n",
            "9303\n",
            "9094\n",
            "10810\n",
            "9319\n",
            "10720\n",
            "9893\n",
            "10806\n",
            "9329\n",
            "9334\n",
            "9283\n",
            "9713\n",
            "9901\n",
            "9305\n",
            "10788\n",
            "9339\n",
            "9250\n",
            "10379\n",
            "10777\n",
            "9344\n",
            "10822\n",
            "10797\n",
            "9322\n",
            "9095\n",
            "10785\n",
            "9285\n",
            "9311\n",
            "10714\n",
            "9341\n",
            "9821\n",
            "9942\n",
            "4176\n",
            "9654\n",
            "9802\n",
            "8824\n",
            "9729\n",
            "2483\n",
            "10315\n",
            "10120\n",
            "9659\n",
            "4423\n",
            "10027\n",
            "9856\n",
            "9569\n",
            "2414\n",
            "4522\n",
            "9676\n",
            "4253\n",
            "10031\n",
            "9806\n",
            "2507\n",
            "4279\n",
            "9744\n",
            "10003\n",
            "9748\n",
            "10029\n",
            "4281\n",
            "9765\n",
            "2552\n",
            "2537\n",
            "9766\n",
            "4415\n",
            "2583\n",
            "10036\n",
            "10146\n",
            "2482\n",
            "4285\n",
            "4282\n",
            "9805\n",
            "9608\n",
            "4218\n",
            "8837\n",
            "9637\n",
            "10019\n",
            "2251\n",
            "9827\n",
            "2578\n",
            "10174\n",
            "4198\n",
            "9959\n",
            "9812\n",
            "2566\n",
            "9761\n",
            "10033\n",
            "4227\n",
            "2415\n",
            "2250\n",
            "9618\n",
            "10113\n",
            "2589\n",
            "9664\n",
            "2605\n",
            "2600\n",
            "9823\n",
            "10168\n",
            "8791\n",
            "4509\n",
            "10258\n",
            "9613\n",
            "10001\n",
            "4257\n",
            "2553\n",
            "2439\n",
            "2546\n",
            "2558\n",
            "4285\n",
            "2506\n",
            "2508\n",
            "4519\n",
            "4396\n",
            "9761\n",
            "4397\n",
            "2507\n",
            "4271\n",
            "2561\n",
            "2449\n",
            "9765\n",
            "2489\n",
            "2555\n",
            "4276\n",
            "4057\n",
            "4472\n",
            "2532\n",
            "4411\n",
            "4224\n",
            "2423\n",
            "2526\n",
            "2540\n",
            "4421\n",
            "2554\n",
            "9668\n",
            "9760\n",
            "9662\n",
            "4404\n",
            "2490\n",
            "2572\n",
            "9904\n",
            "10156\n",
            "2517\n",
            "9626\n",
            "2534\n",
            "4528\n",
            "9824\n",
            "9989\n",
            "10169\n",
            "10150\n",
            "4254\n",
            "2439\n",
            "9988\n",
            "10014\n",
            "10163\n",
            "2604\n",
            "9598\n",
            "10086\n",
            "9622\n",
            "2597\n",
            "9858\n",
            "10093\n",
            "5932\n",
            "2598\n",
            "2170\n",
            "9725\n",
            "9632\n",
            "10159\n",
            "10095\n",
            "9833\n",
            "9767\n",
            "4062\n",
            "9844\n",
            "10201\n",
            "10297\n",
            "2533\n",
            "10346\n",
            "2536\n",
            "9876\n",
            "2567\n",
            "10323\n",
            "10112\n",
            "2422\n",
            "10041\n",
            "10250\n",
            "4244\n",
            "10055\n",
            "10096\n",
            "10080\n",
            "9987\n",
            "2525\n",
            "2532\n",
            "10153\n",
            "2522\n",
            "10134\n",
            "4226\n",
            "9728\n",
            "2535\n",
            "9813\n",
            "10129\n",
            "2560\n",
            "10257\n",
            "4192\n",
            "2161\n",
            "2431\n",
            "9772\n",
            "2569\n",
            "9566\n",
            "9997\n",
            "9835\n",
            "9596\n",
            "10012\n",
            "10084\n",
            "9633\n",
            "2412\n",
            "10152\n",
            "4473\n",
            "9625\n",
            "9940\n",
            "2420\n",
            "10281\n",
            "10109\n",
            "4266\n",
            "2476\n",
            "9860\n",
            "10349\n",
            "9634\n",
            "10247\n",
            "10319\n",
            "9957\n",
            "4194\n",
            "2519\n",
            "9843\n",
            "4416\n",
            "10260\n",
            "2465\n",
            "9991\n",
            "8839\n",
            "10060\n",
            "10154\n",
            "9581\n",
            "4521\n",
            "10064\n",
            "8835\n",
            "4519\n",
            "10124\n",
            "9861\n",
            "2561\n",
            "2612\n",
            "10057\n",
            "2603\n",
            "9831\n",
            "9595\n",
            "10243\n",
            "9842\n",
            "2516\n",
            "9866\n",
            "8834\n",
            "8803\n",
            "10328\n",
            "10062\n",
            "9635\n",
            "9867\n",
            "9927\n",
            "10114\n",
            "9814\n",
            "8800\n",
            "9985\n",
            "9789\n",
            "8825\n",
            "9984\n",
            "4209\n",
            "9925\n",
            "2518\n",
            "2574\n",
            "10126\n",
            "10151\n",
            "9845\n",
            "2158\n",
            "9811\n",
            "10307\n",
            "10158\n",
            "9679\n",
            "10155\n",
            "10005\n",
            "10130\n",
            "10170\n",
            "9916\n",
            "2421\n",
            "9755\n",
            "10004\n",
            "4173\n",
            "2453\n",
            "2521\n",
            "8793\n",
            "2579\n",
            "4199\n",
            "10313\n",
            "9660\n",
            "2489\n",
            "4265\n",
            "2505\n",
            "10165\n",
            "10295\n",
            "2159\n",
            "9568\n",
            "10272\n",
            "4400\n",
            "9966\n",
            "2451\n",
            "9770\n",
            "10088\n",
            "10018\n",
            "10053\n",
            "2162\n",
            "9621\n",
            "2523\n",
            "10007\n",
            "4284\n",
            "4520\n",
            "10148\n",
            "2457\n",
            "2531\n",
            "9648\n",
            "9998\n",
            "2570\n",
            "9857\n",
            "2509\n",
            "9859\n",
            "2157\n",
            "10009\n",
            "2454\n",
            "4217\n",
            "4399\n",
            "4403\n",
            "9807\n",
            "9965\n",
            "2582\n",
            "9865\n",
            "9779\n",
            "9956\n",
            "9803\n",
            "10147\n",
            "2502\n",
            "2541\n",
            "10068\n",
            "9746\n",
            "4190\n",
            "9830\n",
            "8821\n",
            "4283\n",
            "10063\n",
            "2520\n",
            "2607\n",
            "4527\n",
            "2424\n",
            "2165\n",
            "10020\n",
            "10304\n",
            "4426\n",
            "9782\n",
            "9828\n",
            "4193\n",
            "9846\n",
            "9661\n",
            "10082\n",
            "4513\n",
            "4061\n",
            "9986\n",
            "10171\n",
            "9734\n",
            "4501\n",
            "4398\n",
            "9582\n",
            "9815\n",
            "2539\n",
            "9735\n",
            "2411\n",
            "9799\n",
            "10195\n",
            "2164\n",
            "8816\n",
            "2410\n",
            "9631\n",
            "2431\n",
            "4403\n",
            "2523\n",
            "2432\n",
            "4062\n",
            "2502\n",
            "4225\n",
            "2534\n",
            "4267\n",
            "2165\n",
            "4111\n",
            "4498\n",
            "2503\n",
            "4056\n",
            "4253\n",
            "4266\n",
            "4504\n",
            "2533\n",
            "2543\n",
            "4277\n",
            "4283\n",
            "4523\n",
            "9575\n",
            "4242\n",
            "2162\n",
            "4410\n",
            "4261\n",
            "2542\n",
            "4281\n",
            "4278\n",
            "9589\n",
            "2557\n",
            "2524\n",
            "4240\n",
            "4272\n",
            "4398\n",
            "9661\n",
            "4284\n",
            "9592\n",
            "10040\n",
            "4282\n",
            "4395\n",
            "4061\n",
            "4254\n",
            "2525\n",
            "9855\n",
            "4431\n",
            "4415\n",
            "2559\n",
            "2541\n",
            "4418\n",
            "9588\n",
            "2560\n",
            "2426\n",
            "4223\n",
            "9578\n",
            "2509\n",
            "4063\n",
            "4497\n",
            "4222\n",
            "4518\n",
            "4058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XYOVdKnhKC9"
      },
      "source": [
        "# Load frames for both source and target LUs\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = data.to(device)\n",
        "src_frames = torch.Tensor([nodes_to_x[src_frame_id] for src_frame_id, _ in div_D.values()]).long().to(device)\n",
        "tgt_frames = torch.Tensor([nodes_to_x[tgt_frame_id] for _, tgt_frame_id in div_D.values()]).long().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfexGc5SWuKm"
      },
      "source": [
        "# Load POS tags for both source and target LUs\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pos_to_ind = {\n",
        "    'a': 2,\n",
        "    'adv': 4,\n",
        "    'art': 3,\n",
        "    'c': 8,\n",
        "    'idio': 6,\n",
        "    'intj': 10,\n",
        "    'n': 1,\n",
        "    'num': 9,\n",
        "    'prep': 5,\n",
        "    'pron': 11,\n",
        "    'scon': 7,\n",
        "    'v': 0,\n",
        "    'avp': 12\n",
        "}\n",
        "\n",
        "src_pos = list()\n",
        "tgt_pos = list()\n",
        "\n",
        "for _, anno_ID, lang in div_D.keys():\n",
        "    tgt_pos.append(pos_to_ind[annotation_annoID(lang, anno_ID).luName.split('.')[1]])\n",
        "\n",
        "for anno_ID, _, lang in div_D.keys():\n",
        "    src_pos.append(pos_to_ind[annotation_annoID('en', anno_ID).luName.split('.')[1]])\n",
        "\n",
        "src_pos = torch.LongTensor(src_pos).to(device)\n",
        "tgt_pos = torch.LongTensor(tgt_pos).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi0iC9Rhq0Ga"
      },
      "source": [
        "## **Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_PfQTQFqzmI"
      },
      "source": [
        "# Graph Attention Network models\n",
        "from torch_geometric.utils import dropout_adj\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class NodeNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Node normalization (regularization technique)\n",
        "    \"\"\"\n",
        "    def __init__(self, unbiased=False, eps=1e-5):\n",
        "        super(NodeNorm, self).__init__()\n",
        "        self.unbiased = unbiased\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = torch.mean(x, dim=1, keepdim=True)\n",
        "        std = (torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps).sqrt()\n",
        "        x = (x - mean) / std\n",
        "        return x\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, data, hid=109, hid2=256, in_head=9, out_head=10):\n",
        "        super(GAT, self).__init__()\n",
        "        self.hid = hid\n",
        "        self.hid2 = hid2\n",
        "        self.in_head = in_head\n",
        "        self.out_head = out_head\n",
        "        \n",
        "        self.node_norm = NodeNorm()\n",
        "        self.conv1 = GATConv(data.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
        "        self.conv2 = GATConv(self.hid*self.in_head, self.hid2, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "    def forward(self, data, training=True):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        \n",
        "        # DropEdge\n",
        "        edge_index, _ = dropout_adj(data.edge_index, training=training)\n",
        "\n",
        "        # Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.\n",
        "        # One can skip them if the dataset is sufficiently large.\n",
        "        x = nn.Dropout(p=0.4)(x)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.gelu(x)\n",
        "        x = self.node_norm(x)\n",
        "        x = nn.Dropout(p=0.4)(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.node_norm(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BekqXgEAfT07"
      },
      "source": [
        "### Multi-Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoeTBbyj8XR8"
      },
      "source": [
        "def get_lu_embedding_helper(lang_model, anno):\n",
        "    \"\"\"\n",
        "    Use the `lang_model` to embed the sentence in `anno` to retrieve the word embedding\n",
        "    for the lexical unit.\n",
        "    \"\"\"\n",
        "    tokenized_text = anno.tokenized_text\n",
        "    sent = Sentence(tokenized_text, use_tokenizer=False)\n",
        "    lang_model.embed(sent)\n",
        "\n",
        "    tmp_embeds = list()\n",
        "    for i, tok in enumerate(sent):\n",
        "        if anno.tokenized_frame_idx[i] != '-':\n",
        "            tmp_embeds.append(tok.embedding)\n",
        "    \n",
        "    try:\n",
        "        final_embeds = torch.mean(torch.stack(tmp_embeds), dim=0)\n",
        "    except:\n",
        "        print(anno)\n",
        "        for i, tok in enumerate(sent):\n",
        "            print(tok, anno.tokenized_frame_idx[i])\n",
        "\n",
        "        print(tmp_embeds, len(anno.tokenized_frame_idx))\n",
        "        assert False\n",
        "    return final_embeds\n",
        "\n",
        "def get_lu_embeddings(annos, start, end):\n",
        "    \"\"\"\n",
        "    Get embeddings for lexical units for annos[start:end]\n",
        "    annos: list of annotations\n",
        "    start: start index\n",
        "    end: end index\n",
        "    \"\"\"\n",
        "    L = list()\n",
        "    for i, anno in enumerate(annos[start:end]):\n",
        "        if i % 1000 == 0:\n",
        "            print(anno)\n",
        "        embedding = get_lu_embedding_helper(mbert, anno)\n",
        "        L.append(embedding)\n",
        "    pre_lus = torch.stack(L).float().to(device)  # shape: (19927, 768)\n",
        "    return pre_lus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVgAjE1r8gzd"
      },
      "source": [
        "def get_src_tgt_frames(annos, start, end):\n",
        "    \"\"\"\n",
        "    Get source and target frames for annos[start:end]\n",
        "    annos: list of annotations\n",
        "    start: start index\n",
        "    end: end index\n",
        "    \"\"\"\n",
        "    fn15_to_fn17_mapping = torch.load(\"any-language-frames/fn15_to_fn17_mapping.pt\")\n",
        "    pre_src_frames = list()\n",
        "    unavailable_frames = set()\n",
        "    for i, anno in enumerate(annos[start:end]):\n",
        "        if i % 1000 == 0:\n",
        "            print(anno)\n",
        "            \n",
        "        frameName = anno.frameName\n",
        "        frameName = fn15_to_fn17_mapping.get(frameName, frameName)\n",
        "        retrieved_frame = fn.frames(frameName)[0]\n",
        "        pre_src_frames.append(retrieved_frame.ID)\n",
        "        try:\n",
        "            assert retrieved_frame.name == frameName\n",
        "        except:\n",
        "            print(frameName, retrieved_frame.name)\n",
        "            print(fn.frames(frameName))\n",
        "\n",
        "    # correct frame-to-frame\n",
        "    pre_src_frames = torch.Tensor([nodes_to_x[src_frame_id] for src_frame_id in pre_src_frames]).long().to(device)\n",
        "    pre_tgt_frames = pre_src_frames.clone()\n",
        "    return pre_src_frames, pre_tgt_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3zbiykYfIp3"
      },
      "source": [
        "def get_pos(annos, start, end):\n",
        "    \"\"\"\n",
        "    Get source and target POS tags for annos[start:end]\n",
        "    annos: list of annotations\n",
        "    start: start index\n",
        "    end: end index\n",
        "    \"\"\"\n",
        "    pos_to_ind = {\n",
        "        'a': 2,\n",
        "        'adv': 4,\n",
        "        'art': 3,\n",
        "        'c': 8,\n",
        "        'idio': 6,\n",
        "        'intj': 10,\n",
        "        'n': 1,\n",
        "        'num': 9,\n",
        "        'prep': 5,\n",
        "        'pron': 11,\n",
        "        'scon': 7,\n",
        "        'v': 0,\n",
        "        'avp': 12\n",
        "    }\n",
        "\n",
        "    pre_pos = list()\n",
        "    for i, anno in enumerate(annos[start:end]):\n",
        "        pre_pos.append(pos_to_ind[anno.luName.split('.')[1]])\n",
        "\n",
        "    pre_pos = torch.LongTensor(pre_pos).to(device)\n",
        "    return pre_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLVVTr3tndn4",
        "outputId": "c9f31a81-2115-464f-84e2-61f1634054e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "chosen_bfn_annos = random.choices(bfn_annos, k=15000)\n",
        "annos = chosen_bfn_annos + any_annos\n",
        "pre_lus = get_lu_embeddings(annos, 0, len(annos))\n",
        "pre_src_frames, pre_tgt_frames = get_src_tgt_frames(annos, 0, len(annos))\n",
        "pre_pos = get_pos(annos, 0, len(annos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Annotation(annofile='fn.sents(315763)', frameName='Buildings', luName='dwelling.n', lu_idx=[(58, 67)], fe_idx=[], tokenized_text='At the same time , there is no incentive to construct new dwellings for the sector owing to the existence of controlled rents , below the market rate . ', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'dwelling.n', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Buildings', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'])\n",
            "Annotation(annofile='fn.sents(1404070)', frameName='Becoming_attached', luName='attach.v', lu_idx=[(91, 99)], fe_idx=[(52, 84, 'Item'), (85, 90, 'Item'), (100, 118, 'Goal')], tokenized_text='The main point is that prose varies a great deal in the amount of aesthetic interest which attaches to linguistic form .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'attach.v', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Becoming_attached', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Goal'])\n",
            "Annotation(annofile='fn.sents(4103902)', frameName='Color', luName='red.a', lu_idx=[(126, 129)], fe_idx=[(126, 129, 'Color'), (130, 141, 'Entity')], tokenized_text=\"Such sights include the colorful `` tams '' -- knitted hats worn by Jamaican men to cover their mane of dreadlocks -- and the red ackee fruit that ripens at the beginning of the year .\", tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'red.a', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Color', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Entity'])\n",
            "Annotation(annofile='fn.sents(1177847)', frameName='Behind_the_scenes', luName='direct.v', lu_idx=[(149, 155)], fe_idx=[(89, 91, 'Artist'), (156, 164, 'Production'), (165, 178, 'Place')], tokenized_text='At the helm was a Producer , although in addition to supervising the administrative side he was expected to liaise with the writer on the script and direct the play in the studio . ', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'direct.v', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Behind_the_scenes', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Place'])\n",
            "Annotation(annofile='fn.sents(300705)', frameName='Clothing', luName='cloak.n', lu_idx=[(78, 83)], fe_idx=[(74, 77, 'Wearer'), (78, 83, 'Garment')], tokenized_text='Apart from Benjamin and I , he was the only one who had left the manor so his cloak and boots would have been covered in snow . ', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'cloak.n', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Clothing', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Garment'])\n",
            "Annotation(annofile='fn.sents(4097289)', frameName='Degree_of_processing', luName='enriched.a', lu_idx=[(48, 56)], fe_idx=[(57, 64, 'Material')], tokenized_text='What evidence is there that Iran is stockpiling enriched uranium ?', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', 'enriched.a', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', 'Degree_of_processing', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Material'])\n",
            "Annotation(annofile='fn.sents(4122492)', frameName='Text', luName='history.n', lu_idx=[(18, 25)], fe_idx=[(18, 25, 'Text')], tokenized_text='This rewriting of history is punctuated by an array of four-letter invectives applied to males and by adjectives like brilliant , unusual , inspiring , and so forth to women .', tokenized_lu_idx=['-', '-', '-', 'history.n', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Text', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Text'])\n",
            "Annotation(annofile='fn.sents(4140711)', frameName='Attack', luName='assault.n', lu_idx=[(19, 26)], fe_idx=[(0, 6, 'Assailant'), (27, 38, 'Victim'), (39, 46, 'Time')], tokenized_text='Russia launched an assault on Chechnya in 1999 , wresting control of the breakaway republic from rebels in a protracted and bloody campaign .', tokenized_lu_idx=['-', '-', '-', 'assault.n', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Attack', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Time'])\n",
            "Annotation(annofile='fn.sents(4097139)', frameName='Certainty', luName='believe.v', lu_idx=[(18, 26)], fe_idx=[(0, 17, 'Cognizer'), (27, 106, 'Content')], tokenized_text=\"The United States believes that nuclear energy is not necessary in a country with Iran 's large oil supply .\", tokenized_lu_idx=['-', '-', '-', 'believe.v', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Certainty', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Content'])\n",
            "Annotation(annofile='fn.sents(1337318)', frameName='Amassing', luName='amass.v', lu_idx=[(105, 110)], fe_idx=[(38, 91, 'Recipient'), (94, 97, 'Recipient'), (111, 120, 'Mass_theme'), (121, 186, 'Means')], tokenized_text='Thereafter Sutton was overshadowed by his ambitious , avaricious , and energetic second son , who was to amass a fortune by publicizing and exploiting the Suttonian method of inoculation .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'amass.v', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Amassing', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Means'])\n",
            "Annotation(annofile='fn.sents(1452278)', frameName='Domain', luName='emotionally.adv', lu_idx=[(9, 20)], fe_idx=[(9, 20, 'Domain'), (21, 27, 'Predicate')], tokenized_text='They are emotionally strong : but can it be right , they wonder , to be so strong ?', tokenized_lu_idx=['-', '-', 'emotionally.adv', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', 'Domain', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Predicate'])\n",
            "Annotation(annofile='fn.sents(1265856)', frameName='Replacing', luName='replace.v', lu_idx=[(18, 25)], fe_idx=[(0, 2, 'Agent'), (26, 30, 'Old'), (31, 88, 'New')], tokenized_text='We miss meals and replace them with snacks which may mean missing out on vital nutrients .', tokenized_lu_idx=['-', '-', '-', '-', 'replace.v', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', 'Replacing', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'New'])\n",
            "Annotation(annofile='fn.sents(462016)', frameName='Natural_features', luName='reef.n', lu_idx=[(141, 146)], fe_idx=[(141, 146, 'Locale')], tokenized_text='Birkeland commented that it was not useful to blame poor populations for exploiting the few resources that they had , when fishing or mining reefs could earn them millions of dollars . ', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'reef.n', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Natural_features', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Locale'])\n",
            "Annotation(annofile='fn.sents(4100800)', frameName='Military', luName='military.a', lu_idx=[(33, 41)], fe_idx=[(33, 41, 'Force')], tokenized_text=\"A key uncertainty is how ongoing military modernization efforts will ultimately reshape China 's strategic nuclear capabilities , but U.S. deployments of missile defenses are likely to be a key variable .\", tokenized_lu_idx=['-', '-', '-', '-', '-', '-', 'military.a', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', 'Military', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Force'])\n",
            "Annotation(annofile='fn.sents(4097036)', frameName='Delivery', luName='delivery.n', lu_idx=[(155, 163)], fe_idx=[(151, 154, 'Theme')], tokenized_text='In 2002 , Iran tested a new version of its Muajar-4 ( Mohajer ) unmanned aerial vehicle ( UAV ) , which some experts claim could be used for potential CBW delivery .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'delivery.n', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Delivery', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Theme'])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/de_out_daniela/de_angelina_jolie014.fn'), frameName='Kinship', luName='Eltern.n', lu_idx=[], fe_idx='{\"Ego\": \"1\", \"Alter\": \"2\"}', tokenized_text='Jolies Eltern , die am 12 .', tokenized_lu_idx=['-', '_', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', 'Kinship', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/de_out_daniela/de_angelina_jolie_twitter035.fn'), frameName='People', luName='man.pron', lu_idx=[], fe_idx='{\"Person\": \"7\"}', tokenized_text='Wie desillusionierend muss es sein , man ist mit Angelina Jolie im Bett und sie lässt dann einen fahren #nurmalso http://t.co/MY9B8jD19d', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', 'People', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_gavin/en_christiano_ronaldo009.fn'), frameName='Aggregate', luName='Community.n', lu_idx=[], fe_idx='{\"Aggregate\": \"35\"}', tokenized_text='With Manchester United and Real Madrid , Ronaldo has won three Premier Leagues , one La Liga , one FA Cup , two Football League Cups , two Copas del Rey , one FA Community Shield , one Supercopas de España , two UEFA Champions Leagues , one UEFA Super Cup and two FIFA Club World Cups .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Aggregate', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_gavin/en_harry_potter028.fn'), frameName='Cardinal_numbers', luName='a.art', lu_idx=[], fe_idx='{\"Number\": \"11\", \"Entity\": \"12\"}', tokenized_text=\"After the introductory chapter , the book leaps forward to a time shortly before Harry Potter's eleventh birthday , and it is at this point that his magical background begins to be revealed .\", tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Cardinal_numbers', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_gavin/en_christiano_ronaldo_twitter026.fn'), frameName='Performers', luName='players.n', lu_idx=[], fe_idx='{\"Persistent_characteristic\": \"8\", \"Person\": \"9\"}', tokenized_text='Who kids most want to meet : #WorldCup players Christiano Ronaldo and Lionel Messi made the long list . #kidscoop http://t.co/TuXDY8aiOx', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', 'Performers', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_gavin/en_google005.fn'), frameName='Part_whole', luName='shares.n', lu_idx=[], fe_idx='{\"Part\": \"9\"}', tokenized_text='Together they own about 14 percent of its shares but control 56 of the stockholder voting power through supervoting stock .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', 'Part_whole', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/da_out_peter/da_harry_potter026.fn'), frameName='Cardinal_numbers', luName='tre.num', lu_idx=[], fe_idx='{\"Number\": \"32\", \"Entity\": \"34\"}', tokenized_text='Lige fra barndomshistorierne om det lille barn , der trænger ondskaben tilbage omgivet af sendebud ( ugler/engle ) , himmelfænomener ( stjerneskud/stjerne ) og en jublende menneskehed ( troldmænd/hyrder og de tre vise mænd ) frem til døden og opstandelsen , minder Harry Potters og Jesu liv om hinanden .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Cardinal_numbers', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/it_out_giulia/it_harry_potter030.fn'), frameName='Process_end', luName='finale.n', lu_idx=[], fe_idx='{}', tokenized_text=\"Tutta l'opera è caratterizzata da regole appositamente create per rendere logico l'iter della trama , e l'insieme di queste costituisce un corredo di preziose nozioni messe a disposizione del lettore per risolvere , potenzialmente , i vari enigmi e misteri che libro dopo libro si accumulano fino a risolversi nel finale .\", tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Process_end', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/it_out_giulia/it_google_twitter026.fn'), frameName='Cardinal_numbers', luName='5.num', lu_idx=[], fe_idx='{\"Number\": \"4\", \"Entity\": \"6\"}', tokenized_text='@timtom_it #Google Le 5 migliori applicazioni Android per gli screenshot : Vi sarà sicuram ... http://t.co/QMoYO7XwWB #Android #Tecnologia', tokenized_lu_idx=['-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Cardinal_numbers', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/es_out_esther/es_christiano_ronaldo011.fn'), frameName='Finish_competition', luName='Liga.n', lu_idx=[], fe_idx='{\"Competitor\": \"3\"}', tokenized_text='Conquistó como madridista dos Copas del Rey , una Liga , y una Supercopa de España , completando así la triple corona española , más una Liga de Campeones y una Supercopa de Europa .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', 'Finish_competition', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/es_out_esther/es_google_twitter028.fn'), frameName='Assistance', luName='hice.v', lu_idx=[], fe_idx='{\"Benefited_party\": \"7\", \"Goal\": \"5\", \"Helper\": \"2\", \"Focal_entity\": \"7\"}', tokenized_text='Google traductor hizo su parte y yo hice la mia jajaja espero haberte ayudado un poquito . @JohannVera1 Saludos ♥', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', 'Assistance', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/es_out_esther/es_angelina_jolie017.fn'), frameName='Calendric_unit', luName='julio.n', lu_idx=[], fe_idx='{\"Whole\": \"6\", \"Salient_event\": \"18\", \"Unit\": \"2\"}', tokenized_text='El 31 de julio de 2005 , el rey de Camboya , Norodom Sihamoní , emitió un decreto a través del cual la nombró \" ciudadana del país por sus esfuerzos para preservar el alma humanitaria \" .', tokenized_lu_idx=['-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Calendric_unit', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/sv_out_karin/sv_google043.fn'), frameName='Having_or_lacking_access', luName='tillgång.n', lu_idx=[], fe_idx='{\"Theme\": \"8\", \"Degree\": \"5\"}', tokenized_text='Man vill ge dem opartisk tillgång till information och behandla dem med respekt , samt att allmänt göra det rätta och agera hederligt .', tokenized_lu_idx=['-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', 'Having_or_lacking_access', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/fr_out_tim/fr_google048.fn'), frameName='Request', luName='requêtes.n', lu_idx=[], fe_idx='{\"Addressee\": \"12\"}', tokenized_text='Alors que le moteur est toujours en phase version bêta , il répond à près de requêtes par jour .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Request', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/fr_out_tim/fr_harry_potter_twitter003.fn'), frameName='Performing_arts', luName='MUSIQUES.n', lu_idx=[], fe_idx='{\"Medium\": \"2\"}', tokenized_text=\"LES MUSIQUES D'HARRY POTTER 😍 MEILLEURE PLAYLIST DU MONDE 😍💕\", tokenized_lu_idx=['-', '_', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', 'Performing_arts', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/da_out_sara/da_christiano_ronaldo050.fn'), frameName='Desiring', luName='ville.v', lu_idx=[], fe_idx='{\"Experiencer\": \"1\"}', tokenized_text='Ronaldo sagde han havde \" lært en masse \" af den erfaring og ville ikke lade andre spillere \" provokere \" ham i fremtiden .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Desiring', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_sarah/en_harry_potter_twitter027.fn'), frameName='Activity_pause', luName='break.n', lu_idx=[], fe_idx='{\"Time\": \"10\"}', tokenized_text='@cestlaviolett I just finished my rewatch this morning ! Christmas break and harry potter go great together', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Activity_pause', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_sarah/en_angelina_jolie_twitter032.fn'), frameName='Existence', luName='was.v', lu_idx=[], fe_idx='{\"Entity\": \"12\"}', tokenized_text='Had a dream that was like a cabin horror film . It was valentines day n Angelina Jolie was a crazy b tryna kill me', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Existence', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/es_out_mariana/es_google_twitter033.fn'), frameName='Setting_fire', luName='arder.n', lu_idx=[], fe_idx='{\"Place\": \"31\"}', tokenized_text='Voy a buscar en Google \" mujer luchadora \" , a dar a la pestaña de imágenes , y lo que voy a hacer , es porque quiero ver el mundo arder .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Setting_fire', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile='fn.sents(315763)', frameName='Buildings', luName='dwelling.n', lu_idx=[(58, 67)], fe_idx=[], tokenized_text='At the same time , there is no incentive to construct new dwellings for the sector owing to the existence of controlled rents , below the market rate . ', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'dwelling.n', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Buildings', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'])\n",
            "Annotation(annofile='fn.sents(1404070)', frameName='Becoming_attached', luName='attach.v', lu_idx=[(91, 99)], fe_idx=[(52, 84, 'Item'), (85, 90, 'Item'), (100, 118, 'Goal')], tokenized_text='The main point is that prose varies a great deal in the amount of aesthetic interest which attaches to linguistic form .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'attach.v', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Becoming_attached', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Goal'])\n",
            "Annotation(annofile='fn.sents(4103902)', frameName='Color', luName='red.a', lu_idx=[(126, 129)], fe_idx=[(126, 129, 'Color'), (130, 141, 'Entity')], tokenized_text=\"Such sights include the colorful `` tams '' -- knitted hats worn by Jamaican men to cover their mane of dreadlocks -- and the red ackee fruit that ripens at the beginning of the year .\", tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'red.a', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Color', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Entity'])\n",
            "Annotation(annofile='fn.sents(1177847)', frameName='Behind_the_scenes', luName='direct.v', lu_idx=[(149, 155)], fe_idx=[(89, 91, 'Artist'), (156, 164, 'Production'), (165, 178, 'Place')], tokenized_text='At the helm was a Producer , although in addition to supervising the administrative side he was expected to liaise with the writer on the script and direct the play in the studio . ', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'direct.v', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Behind_the_scenes', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Place'])\n",
            "Annotation(annofile='fn.sents(300705)', frameName='Clothing', luName='cloak.n', lu_idx=[(78, 83)], fe_idx=[(74, 77, 'Wearer'), (78, 83, 'Garment')], tokenized_text='Apart from Benjamin and I , he was the only one who had left the manor so his cloak and boots would have been covered in snow . ', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'cloak.n', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Clothing', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Garment'])\n",
            "Annotation(annofile='fn.sents(4097289)', frameName='Degree_of_processing', luName='enriched.a', lu_idx=[(48, 56)], fe_idx=[(57, 64, 'Material')], tokenized_text='What evidence is there that Iran is stockpiling enriched uranium ?', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', 'enriched.a', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', 'Degree_of_processing', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Material'])\n",
            "Annotation(annofile='fn.sents(4122492)', frameName='Text', luName='history.n', lu_idx=[(18, 25)], fe_idx=[(18, 25, 'Text')], tokenized_text='This rewriting of history is punctuated by an array of four-letter invectives applied to males and by adjectives like brilliant , unusual , inspiring , and so forth to women .', tokenized_lu_idx=['-', '-', '-', 'history.n', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Text', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Text'])\n",
            "Annotation(annofile='fn.sents(4140711)', frameName='Attack', luName='assault.n', lu_idx=[(19, 26)], fe_idx=[(0, 6, 'Assailant'), (27, 38, 'Victim'), (39, 46, 'Time')], tokenized_text='Russia launched an assault on Chechnya in 1999 , wresting control of the breakaway republic from rebels in a protracted and bloody campaign .', tokenized_lu_idx=['-', '-', '-', 'assault.n', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Attack', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Time'])\n",
            "Annotation(annofile='fn.sents(4097139)', frameName='Certainty', luName='believe.v', lu_idx=[(18, 26)], fe_idx=[(0, 17, 'Cognizer'), (27, 106, 'Content')], tokenized_text=\"The United States believes that nuclear energy is not necessary in a country with Iran 's large oil supply .\", tokenized_lu_idx=['-', '-', '-', 'believe.v', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Certainty', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Content'])\n",
            "Annotation(annofile='fn.sents(1337318)', frameName='Amassing', luName='amass.v', lu_idx=[(105, 110)], fe_idx=[(38, 91, 'Recipient'), (94, 97, 'Recipient'), (111, 120, 'Mass_theme'), (121, 186, 'Means')], tokenized_text='Thereafter Sutton was overshadowed by his ambitious , avaricious , and energetic second son , who was to amass a fortune by publicizing and exploiting the Suttonian method of inoculation .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'amass.v', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Amassing', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Means'])\n",
            "Annotation(annofile='fn.sents(1452278)', frameName='Domain', luName='emotionally.adv', lu_idx=[(9, 20)], fe_idx=[(9, 20, 'Domain'), (21, 27, 'Predicate')], tokenized_text='They are emotionally strong : but can it be right , they wonder , to be so strong ?', tokenized_lu_idx=['-', '-', 'emotionally.adv', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', 'Domain', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Predicate'])\n",
            "Annotation(annofile='fn.sents(1265856)', frameName='Replacing', luName='replace.v', lu_idx=[(18, 25)], fe_idx=[(0, 2, 'Agent'), (26, 30, 'Old'), (31, 88, 'New')], tokenized_text='We miss meals and replace them with snacks which may mean missing out on vital nutrients .', tokenized_lu_idx=['-', '-', '-', '-', 'replace.v', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', 'Replacing', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'New'])\n",
            "Annotation(annofile='fn.sents(462016)', frameName='Natural_features', luName='reef.n', lu_idx=[(141, 146)], fe_idx=[(141, 146, 'Locale')], tokenized_text='Birkeland commented that it was not useful to blame poor populations for exploiting the few resources that they had , when fishing or mining reefs could earn them millions of dollars . ', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'reef.n', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Natural_features', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Locale'])\n",
            "Annotation(annofile='fn.sents(4100800)', frameName='Military', luName='military.a', lu_idx=[(33, 41)], fe_idx=[(33, 41, 'Force')], tokenized_text=\"A key uncertainty is how ongoing military modernization efforts will ultimately reshape China 's strategic nuclear capabilities , but U.S. deployments of missile defenses are likely to be a key variable .\", tokenized_lu_idx=['-', '-', '-', '-', '-', '-', 'military.a', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', 'Military', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Force'])\n",
            "Annotation(annofile='fn.sents(4097036)', frameName='Delivery', luName='delivery.n', lu_idx=[(155, 163)], fe_idx=[(151, 154, 'Theme')], tokenized_text='In 2002 , Iran tested a new version of its Muajar-4 ( Mohajer ) unmanned aerial vehicle ( UAV ) , which some experts claim could be used for potential CBW delivery .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'delivery.n', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Delivery', '-'], tokenized_fe_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Theme'])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/de_out_daniela/de_angelina_jolie014.fn'), frameName='Kinship', luName='Eltern.n', lu_idx=[], fe_idx='{\"Ego\": \"1\", \"Alter\": \"2\"}', tokenized_text='Jolies Eltern , die am 12 .', tokenized_lu_idx=['-', '_', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', 'Kinship', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/de_out_daniela/de_angelina_jolie_twitter035.fn'), frameName='People', luName='man.pron', lu_idx=[], fe_idx='{\"Person\": \"7\"}', tokenized_text='Wie desillusionierend muss es sein , man ist mit Angelina Jolie im Bett und sie lässt dann einen fahren #nurmalso http://t.co/MY9B8jD19d', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', 'People', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_gavin/en_christiano_ronaldo009.fn'), frameName='Aggregate', luName='Community.n', lu_idx=[], fe_idx='{\"Aggregate\": \"35\"}', tokenized_text='With Manchester United and Real Madrid , Ronaldo has won three Premier Leagues , one La Liga , one FA Cup , two Football League Cups , two Copas del Rey , one FA Community Shield , one Supercopas de España , two UEFA Champions Leagues , one UEFA Super Cup and two FIFA Club World Cups .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Aggregate', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_gavin/en_harry_potter028.fn'), frameName='Cardinal_numbers', luName='a.art', lu_idx=[], fe_idx='{\"Number\": \"11\", \"Entity\": \"12\"}', tokenized_text=\"After the introductory chapter , the book leaps forward to a time shortly before Harry Potter's eleventh birthday , and it is at this point that his magical background begins to be revealed .\", tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Cardinal_numbers', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_gavin/en_christiano_ronaldo_twitter026.fn'), frameName='Performers', luName='players.n', lu_idx=[], fe_idx='{\"Persistent_characteristic\": \"8\", \"Person\": \"9\"}', tokenized_text='Who kids most want to meet : #WorldCup players Christiano Ronaldo and Lionel Messi made the long list . #kidscoop http://t.co/TuXDY8aiOx', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', 'Performers', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_gavin/en_google005.fn'), frameName='Part_whole', luName='shares.n', lu_idx=[], fe_idx='{\"Part\": \"9\"}', tokenized_text='Together they own about 14 percent of its shares but control 56 of the stockholder voting power through supervoting stock .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', 'Part_whole', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/da_out_peter/da_harry_potter026.fn'), frameName='Cardinal_numbers', luName='tre.num', lu_idx=[], fe_idx='{\"Number\": \"32\", \"Entity\": \"34\"}', tokenized_text='Lige fra barndomshistorierne om det lille barn , der trænger ondskaben tilbage omgivet af sendebud ( ugler/engle ) , himmelfænomener ( stjerneskud/stjerne ) og en jublende menneskehed ( troldmænd/hyrder og de tre vise mænd ) frem til døden og opstandelsen , minder Harry Potters og Jesu liv om hinanden .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Cardinal_numbers', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/it_out_giulia/it_harry_potter030.fn'), frameName='Process_end', luName='finale.n', lu_idx=[], fe_idx='{}', tokenized_text=\"Tutta l'opera è caratterizzata da regole appositamente create per rendere logico l'iter della trama , e l'insieme di queste costituisce un corredo di preziose nozioni messe a disposizione del lettore per risolvere , potenzialmente , i vari enigmi e misteri che libro dopo libro si accumulano fino a risolversi nel finale .\", tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Process_end', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/it_out_giulia/it_google_twitter026.fn'), frameName='Cardinal_numbers', luName='5.num', lu_idx=[], fe_idx='{\"Number\": \"4\", \"Entity\": \"6\"}', tokenized_text='@timtom_it #Google Le 5 migliori applicazioni Android per gli screenshot : Vi sarà sicuram ... http://t.co/QMoYO7XwWB #Android #Tecnologia', tokenized_lu_idx=['-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Cardinal_numbers', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/es_out_esther/es_christiano_ronaldo011.fn'), frameName='Finish_competition', luName='Liga.n', lu_idx=[], fe_idx='{\"Competitor\": \"3\"}', tokenized_text='Conquistó como madridista dos Copas del Rey , una Liga , y una Supercopa de España , completando así la triple corona española , más una Liga de Campeones y una Supercopa de Europa .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', 'Finish_competition', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/es_out_esther/es_google_twitter028.fn'), frameName='Assistance', luName='hice.v', lu_idx=[], fe_idx='{\"Benefited_party\": \"7\", \"Goal\": \"5\", \"Helper\": \"2\", \"Focal_entity\": \"7\"}', tokenized_text='Google traductor hizo su parte y yo hice la mia jajaja espero haberte ayudado un poquito . @JohannVera1 Saludos ♥', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', 'Assistance', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/es_out_esther/es_angelina_jolie017.fn'), frameName='Calendric_unit', luName='julio.n', lu_idx=[], fe_idx='{\"Whole\": \"6\", \"Salient_event\": \"18\", \"Unit\": \"2\"}', tokenized_text='El 31 de julio de 2005 , el rey de Camboya , Norodom Sihamoní , emitió un decreto a través del cual la nombró \" ciudadana del país por sus esfuerzos para preservar el alma humanitaria \" .', tokenized_lu_idx=['-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', 'Calendric_unit', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/sv_out_karin/sv_google043.fn'), frameName='Having_or_lacking_access', luName='tillgång.n', lu_idx=[], fe_idx='{\"Theme\": \"8\", \"Degree\": \"5\"}', tokenized_text='Man vill ge dem opartisk tillgång till information och behandla dem med respekt , samt att allmänt göra det rätta och agera hederligt .', tokenized_lu_idx=['-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', 'Having_or_lacking_access', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/fr_out_tim/fr_google048.fn'), frameName='Request', luName='requêtes.n', lu_idx=[], fe_idx='{\"Addressee\": \"12\"}', tokenized_text='Alors que le moteur est toujours en phase version bêta , il répond à près de requêtes par jour .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Request', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/fr_out_tim/fr_harry_potter_twitter003.fn'), frameName='Performing_arts', luName='MUSIQUES.n', lu_idx=[], fe_idx='{\"Medium\": \"2\"}', tokenized_text=\"LES MUSIQUES D'HARRY POTTER 😍 MEILLEURE PLAYLIST DU MONDE 😍💕\", tokenized_lu_idx=['-', '_', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', 'Performing_arts', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/da_out_sara/da_christiano_ronaldo050.fn'), frameName='Desiring', luName='ville.v', lu_idx=[], fe_idx='{\"Experiencer\": \"1\"}', tokenized_text='Ronaldo sagde han havde \" lært en masse \" af den erfaring og ville ikke lade andre spillere \" provokere \" ham i fremtiden .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Desiring', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_sarah/en_harry_potter_twitter027.fn'), frameName='Activity_pause', luName='break.n', lu_idx=[], fe_idx='{\"Time\": \"10\"}', tokenized_text='@cestlaviolett I just finished my rewatch this morning ! Christmas break and harry potter go great together', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Activity_pause', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/en_out_sarah/en_angelina_jolie_twitter032.fn'), frameName='Existence', luName='was.v', lu_idx=[], fe_idx='{\"Entity\": \"12\"}', tokenized_text='Had a dream that was like a cabin horror film . It was valentines day n Angelina Jolie was a crazy b tryna kill me', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Existence', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'], tokenized_fe_idx=[])\n",
            "Annotation(annofile=PosixPath('/content/any-language-frames/annotated/es_out_mariana/es_google_twitter033.fn'), frameName='Setting_fire', luName='arder.n', lu_idx=[], fe_idx='{\"Place\": \"31\"}', tokenized_text='Voy a buscar en Google \" mujer luchadora \" , a dar a la pestaña de imágenes , y lo que voy a hacer , es porque quiero ver el mundo arder .', tokenized_lu_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '_', '-'], tokenized_frame_idx=['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Setting_fire', '-'], tokenized_fe_idx=[])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hN9n9DmuSSW"
      },
      "source": [
        "class MultiTaskLossWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-task Loss Function weighted by homoscedastic uncertainty\n",
        "    \"\"\"\n",
        "    def __init__(self, task_num):\n",
        "        super(MultiTaskLossWrapper, self).__init__()\n",
        "        self.task_num = task_num\n",
        "        self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
        "\n",
        "    def forward(self, losses):\n",
        "        total_loss = 0\n",
        "        for i in range(len(losses)):\n",
        "            precision = torch.exp(-self.log_vars[i])\n",
        "            total_loss += torch.sum(precision * losses[i] + self.log_vars[i], -1)\n",
        "        total_loss = torch.mean(total_loss)\n",
        "        return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrJHGf-H4Bpz"
      },
      "source": [
        "### All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJLEN2-7APfS"
      },
      "source": [
        "##### All + Nested CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O876ZQYZZytI"
      },
      "source": [
        "# for c in nx.connected_components(G.to_undirected()):\n",
        "#     print(nx.diameter(G.subgraph(c)))  # result: 0 - 14\n",
        "\n",
        "import random\n",
        "def frame_pairs_in_relation_helper(frame_id1, frame_id2):\n",
        "    frame1 = fn.frame(frame_id1)\n",
        "    for frame_x in frame1.frameRelations:\n",
        "        if frame_id2 == frame_x.subID or frame_id2 == frame_x.supID:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def generate_frame_pairs_in_relation(num_pairs):\n",
        "    \"\"\"\n",
        "    Generate frame-LU pairs for binary frame prediction.\n",
        "    num_pairs: number of frame-LU pairs\n",
        "    \"\"\"\n",
        "    frame_pairs_in_relation = list()\n",
        "    count = 0\n",
        "    while len(frame_pairs_in_relation) < num_pairs:\n",
        "        if random.random() < 0.5:\n",
        "            f1 = random.choice(fn.frames()).ID\n",
        "            f2 = random.choice(fn.frames()).ID\n",
        "        else:\n",
        "            f1 = random.choice(fn.frames())\n",
        "            while not f1.frameRelations:\n",
        "                f1 = random.choice(fn.frames())\n",
        "            f2 = random.choice(f1.frameRelations)\n",
        "            f2 = f2.subID if f2.subID != f1.ID else f2.supID\n",
        "            f1 = f1.ID\n",
        "\n",
        "        if f1 == f2:\n",
        "            continue\n",
        "        \n",
        "        count += frame_pairs_in_relation_helper(f1, f2)\n",
        "        frame_pairs_in_relation.append((f1, f2, frame_pairs_in_relation_helper(f1, f2)))\n",
        "\n",
        "    fr_frame_1 = torch.LongTensor([nodes_to_x[f1] for f1, _, _ in frame_pairs_in_relation]).to(device)\n",
        "    fr_frame_2 = torch.LongTensor([nodes_to_x[f1] for _, f2, _ in frame_pairs_in_relation]).to(device)\n",
        "    fr_targets = torch.LongTensor([target for _, _, target in frame_pairs_in_relation]).to(device)\n",
        "    return fr_frame_1, fr_frame_2, fr_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkAVDT02-c1s",
        "outputId": "08c1aea9-540c-4a30-dd51-7a046f4aaecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"framenet_v17\")\n",
        "from nltk.corpus import framenet as fn\n",
        "import networkx as nx\n",
        "\n",
        "# Load FrameNet into networkx graph\n",
        "G = nx.DiGraph()\n",
        "for frame in fn.frames():\n",
        "    G.add_node(frame.ID)\n",
        "    for adj in frame.frameRelations:\n",
        "        G.add_edge(adj.superFrame.ID, adj.subFrame.ID)\n",
        "        G.add_edge(adj.subFrame.ID, adj.superFrame.ID)\n",
        "\n",
        "def generate_fr_dist_data(G, size=20000):\n",
        "    \"\"\"\n",
        "    Generate frame-to-frame relations as auxiliary training data.\n",
        "    G: FrameNet graph (in networkx)\n",
        "    \"\"\"\n",
        "    src_nodes = []\n",
        "    tgt_nodes = []\n",
        "    dist = []\n",
        "    all_nodes = list(G)\n",
        "    while len(dist) < size:\n",
        "        src_node = random.choice(all_nodes)\n",
        "        tgt_node = random.choice(all_nodes)\n",
        "        if src_node == tgt_node:\n",
        "            continue\n",
        "        src_nodes.append(nodes_to_x[src_node])\n",
        "        tgt_nodes.append(nodes_to_x[tgt_node])\n",
        "        if not nx.has_path(G, src_node, tgt_node):\n",
        "            dist.append(0)\n",
        "        else:\n",
        "            dist.append(nx.shortest_path_length(G, src_node, tgt_node))\n",
        "\n",
        "    src_nodes = torch.LongTensor(src_nodes).to(device)\n",
        "    tgt_nodes = torch.LongTensor(tgt_nodes).to(device)\n",
        "    dist = torch.FloatTensor(dist).to(device)\n",
        "    return src_nodes, tgt_nodes, dist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUCAJvIuaRMR"
      },
      "source": [
        "def scramble_frames(pre_tgt_frames, p=0.3):\n",
        "    \"\"\"\n",
        "    Randomly perturb frames for frame label reconstruction\n",
        "    pre_tgt_frames: correct frame labels for annotations\n",
        "    p: probability of perturbation\n",
        "    \"\"\"\n",
        "    import random\n",
        "    pre_scrambled_tgt_frames = list()\n",
        "    pre_scrambled_targets = list()\n",
        "\n",
        "    for i in range(len(pre_tgt_frames)):\n",
        "        if random.random() < p:\n",
        "            tmp = random.choice(list(x_to_nodes.keys()))\n",
        "            pre_scrambled_tgt_frames.append(tmp)\n",
        "            if tmp != pre_tgt_frames[i].item():\n",
        "                pre_scrambled_targets.append(0)\n",
        "            else:\n",
        "                pre_scrambled_targets.append(1)\n",
        "        else:\n",
        "            pre_scrambled_tgt_frames.append(pre_tgt_frames[i].item())\n",
        "            pre_scrambled_targets.append(1)\n",
        "\n",
        "    pre_scrambled_tgt_frames = torch.LongTensor(pre_scrambled_tgt_frames).to(device)\n",
        "    pre_scrambled_targets = torch.LongTensor(pre_scrambled_targets).to(device)\n",
        "    return pre_scrambled_tgt_frames, pre_scrambled_targets\n",
        "\n",
        "pre_scrambled_tgt_frames, pre_scrambled_targets = scramble_frames(pre_tgt_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMX-UjzDNoP6"
      },
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].flatten().float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "        return res\n",
        "\n",
        "# accuracy(y, test_tgt_frames, topk=(1, 5,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLGh7Zbk7JXk",
        "outputId": "7bd1846f-fed0-4d9e-8abb-9a04592c6174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### all with nested CV\n",
        "class FFN_frame(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-class classifier layer for frame shift prediction and frame label reconstruction\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_class, pos_dim=16):\n",
        "        super(FFN_frame, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_class = num_class\n",
        "        self.pos_dim = pos_dim\n",
        "\n",
        "        self.pos_embedding = nn.Embedding(13, pos_dim)\n",
        "        self.linear1 = nn.Linear(self.input_dim + pos_dim * 2, self.num_class)\n",
        "    \n",
        "    def forward(self, x, src_pos, tgt_pos):\n",
        "        src_pos = self.pos_embedding(src_pos)\n",
        "        tgt_pos = self.pos_embedding(tgt_pos)\n",
        "        x = torch.cat([x, src_pos, tgt_pos], dim=1)\n",
        "        out = self.linear1(x)\n",
        "        return out\n",
        "\n",
        "class FFN_pred_frame(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Binary frame prediction classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_class, pos_dim=16):\n",
        "        super(FFN_pred_frame, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.pos_embedding = nn.Embedding(13, pos_dim)\n",
        "        self.linear1 = nn.Linear(self.input_dim + pos_dim, 2)\n",
        "    \n",
        "    def forward(self, x, pos):\n",
        "        pos = self.pos_embedding(pos)\n",
        "        x = torch.cat([x, pos], dim=1)\n",
        "        out = self.linear1(x)\n",
        "        return out\n",
        "\n",
        "class FFN_relation_classifier(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Binary frame-to-frame relation classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, ffn_hid = 8, pos_dim=16):\n",
        "        super(FFN_relation_classifier, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(self.input_dim, 2)\n",
        "    \n",
        "    def forward(self, frame1, frame2):\n",
        "        x = torch.cat([frame1, frame2], dim=1)\n",
        "        out = self.linear1(x)\n",
        "        return out\n",
        "\n",
        "class FFN_pred_fr_dist(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Nodes-apart frame distance prediction layer\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(FFN_pred_fr_dist, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.linear1 = nn.Linear(self.input_dim, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 1)\n",
        "    \n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        out = self.linear1(x)\n",
        "        out = nn.ReLU()(out)\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def cross_val_split(data, fold, i):\n",
        "    \"\"\"\n",
        "    Cross-validation split\n",
        "    data: data (training/evaluation split)\n",
        "    fold: total number of folds\n",
        "    i: i-th fold\n",
        "    \"\"\"\n",
        "    splits = [len(data) // fold] * (fold - 1) + [len(data) - sum([len(data) // fold] * (fold - 1))]\n",
        "    held_out = data[i * splits[i]:(i + 1) * splits[i]]\n",
        "    non_held_out = torch.cat([data[:i * splits[i]], data[(i + 1) * splits[i]:]])\n",
        "    return non_held_out, held_out\n",
        "\n",
        "\n",
        "def run():\n",
        "    OUTER_FOLD = 5\n",
        "    INNER_FOLD = 5\n",
        "    ES_PATIENCE = 2\n",
        "    test_accs = list()\n",
        "    for fold_idx in range(OUTER_FOLD):\n",
        "        print(\"Outer CV:\", fold_idx)\n",
        "\n",
        "        # outer CV\n",
        "        all_train_src_frames, test_src_frames = cross_val_split(src_frames, OUTER_FOLD, fold_idx)\n",
        "        all_train_src_lus, test_src_lus = cross_val_split(src_lus, OUTER_FOLD, fold_idx)\n",
        "        all_train_src_pos, test_src_pos = cross_val_split(src_pos, OUTER_FOLD, fold_idx)\n",
        "        all_train_tgt_lus, test_tgt_lus = cross_val_split(tgt_lus, OUTER_FOLD, fold_idx)\n",
        "        all_train_tgt_frames, test_tgt_frames = cross_val_split(tgt_frames, OUTER_FOLD, fold_idx)\n",
        "        all_train_tgt_pos, test_tgt_pos = cross_val_split(tgt_pos, OUTER_FOLD, fold_idx)\n",
        "\n",
        "        # inner CV: features selection\n",
        "        features = {}\n",
        "        for inner_fold_idx in range(INNER_FOLD):\n",
        "            # inner CV\n",
        "            print(\"Inner CV:\", inner_fold_idx)\n",
        "\n",
        "            # initialize models, criterion, and optimizer\n",
        "            model = GAT(data).to(device)\n",
        "            aux_fr_dist_classifier = FFN_pred_fr_dist(256 * 2).to(device)\n",
        "            aux_fr_classifier = FFN_relation_classifier(256 * 2).to(device)\n",
        "            aux_frame_classifier = FFN_pred_frame(256 + 768, 1221).to(device)\n",
        "            frame_classifier = FFN_frame(256 + 768 + 768, 1221).to(device)\n",
        "\n",
        "            aux_fr_dist_criterion = nn.MSELoss()\n",
        "            aux_fr_criterion = nn.CrossEntropyLoss()\n",
        "            aux_frame_criterion = nn.CrossEntropyLoss()\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            wrapper = MultiTaskLossWrapper(5)\n",
        "\n",
        "            optimizer = torch.optim.Adam(list(model.parameters()) + list(frame_classifier.parameters()) + list(aux_frame_classifier.parameters()) + list(aux_fr_classifier.parameters()) + list(aux_fr_dist_classifier.parameters()), \n",
        "                                        lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "            # data for actual\n",
        "            train_src_frames, val_src_frames = cross_val_split(all_train_src_frames, INNER_FOLD, inner_fold_idx)\n",
        "            train_src_lus, val_src_lus = cross_val_split(all_train_src_lus, INNER_FOLD, inner_fold_idx)\n",
        "            train_src_pos, val_src_pos = cross_val_split(all_train_src_pos, INNER_FOLD, inner_fold_idx)\n",
        "            train_tgt_pos, val_tgt_pos = cross_val_split(all_train_tgt_pos, INNER_FOLD, inner_fold_idx)\n",
        "            train_tgt_lus, val_tgt_lus = cross_val_split(all_train_tgt_lus, INNER_FOLD, inner_fold_idx)\n",
        "            train_tgt_frames, val_tgt_frames = cross_val_split(all_train_tgt_frames, INNER_FOLD, inner_fold_idx)\n",
        "\n",
        "            # randomly generated data for aux\n",
        "            fr_frame_1, fr_frame_2, fr_targets = generate_frame_pairs_in_relation(20000)  # aux - fr\n",
        "            src_nodes, tgt_nodes, dist = generate_fr_dist_data(G)  # aux - fr dist\n",
        "            pre_scrambled_tgt_frames, pre_scrambled_targets = scramble_frames(pre_tgt_frames)  # aux - frame\n",
        "\n",
        "            # model development\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)  \n",
        "\n",
        "\n",
        "            min_val_loss = float('inf')\n",
        "            epochs_no_improve = 0\n",
        "            for epoch in range(1000):\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                out = model(data)\n",
        "\n",
        "                # auxiliary task training - fr dist\n",
        "                y = aux_fr_dist_classifier(out[src_nodes], out[tgt_nodes])\n",
        "                aux_fr_dist_loss = aux_fr_dist_criterion(y, dist)\n",
        "\n",
        "                # auxiliary task training - fr binary\n",
        "                y = aux_fr_classifier(out[fr_frame_1], out[fr_frame_2])\n",
        "                aux_fr_loss = aux_fr_criterion(y, fr_targets)\n",
        "\n",
        "                # auxiliary task training - binary frame induction prediction \n",
        "                tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus], dim=1)\n",
        "                y = aux_frame_classifier(tmp_out, pre_pos)\n",
        "                aux_frame_loss = aux_frame_criterion(y, pre_scrambled_targets)\n",
        "\n",
        "                # auxiliary task training - frame restoration\n",
        "                tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus, pre_lus], dim=1)\n",
        "                y = frame_classifier(tmp_out, pre_pos, pre_pos)\n",
        "                aux_frame_2_loss = criterion(y, pre_tgt_frames)\n",
        "\n",
        "                # actual task training\n",
        "                tmp_out = torch.cat([out[train_src_frames], train_src_lus, train_tgt_lus], dim=1)\n",
        "                y = frame_classifier(tmp_out, train_src_pos, train_tgt_pos)\n",
        "                main_loss = criterion(y, train_tgt_frames)\n",
        "\n",
        "                # total_loss = aux_fr_dist_loss + aux_frame_loss + aux_frame_2_loss + main_loss \n",
        "                total_loss = wrapper([aux_fr_dist_loss, aux_fr_loss, aux_frame_loss, aux_frame_2_loss, main_loss])\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(\"Losses Breakdown:\", total_loss.item(), aux_fr_dist_loss.item(), aux_fr_loss.item(), aux_frame_loss.item(), aux_frame_2_loss.item(), main_loss.item())\n",
        "                    with torch.no_grad():\n",
        "                        model.eval()\n",
        "                        out = model(data, training=False)\n",
        "                        out = torch.cat([out[train_src_frames], train_src_lus, train_tgt_lus], dim=1)\n",
        "                        y = frame_classifier(out, train_src_pos, train_tgt_pos)\n",
        "                        train_acc = sum(torch.argmax(y, dim=1) == train_tgt_frames).item()/torch.argmax(y, dim=1).shape[0]\n",
        "                        # print(\"Actual Train Loss:\", loss.item())\n",
        "                        # print(f\"Actual Train Accuracy: {train_acc}\")\n",
        "\n",
        "                        out = model(data, training=False)\n",
        "                        out = torch.cat([out[val_src_frames], val_src_lus, val_tgt_lus], dim=1)\n",
        "                        y = frame_classifier(out, val_src_pos, val_tgt_pos)\n",
        "                        val_acc = sum(torch.argmax(y, dim=1) == val_tgt_frames).item()/torch.argmax(y, dim=1).shape[0]\n",
        "                        val_loss = criterion(y, val_tgt_frames)\n",
        "                        # print(f\">>>> Val Accuracy: {val_acc}\")\n",
        "\n",
        "                        # early stopping\n",
        "                        if val_loss.item() < min_val_loss:\n",
        "                            min_val_loss = val_loss.item()\n",
        "                            epochs_no_improve = 0\n",
        "                        else:\n",
        "                            epochs_no_improve += 1\n",
        "                            if epochs_no_improve >= ES_PATIENCE:\n",
        "                                print(\"early stopping\")\n",
        "                                features[val_loss.item()] = {\"best_epoches\": epoch, \n",
        "                                                            'aux_fr': (fr_frame_1, fr_frame_2, fr_targets), \n",
        "                                                            'aux_fr_dist': (src_nodes, tgt_nodes, dist), \n",
        "                                                            'aux_frame': (pre_scrambled_tgt_frames, pre_scrambled_targets)}\n",
        "                                break\n",
        "                \n",
        "                        \n",
        "        ##################################################################################################################\n",
        "        # model testing: retrain with all data -> eval with test\n",
        "        # choose features\n",
        "        min_val_loss = min(features.keys())\n",
        "        best_features = features[min_val_loss]\n",
        "\n",
        "        # initialize models, criterion, and optimizer\n",
        "        model = GAT(data).to(device)\n",
        "        aux_fr_dist_classifier = FFN_pred_fr_dist(256 * 2).to(device)\n",
        "        aux_fr_classifier = FFN_relation_classifier(256 * 2).to(device)\n",
        "        aux_frame_classifier = FFN_pred_frame(256 + 768, 1221).to(device)\n",
        "        frame_classifier = FFN_frame(256 + 768 + 768, 1221).to(device)\n",
        "\n",
        "        aux_fr_dist_criterion = nn.MSELoss()\n",
        "        aux_fr_criterion = nn.CrossEntropyLoss()\n",
        "        aux_frame_criterion = nn.CrossEntropyLoss()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        wrapper = MultiTaskLossWrapper(5)\n",
        "\n",
        "        optimizer = torch.optim.Adam(list(model.parameters()) + list(frame_classifier.parameters()) + list(aux_frame_classifier.parameters()) + list(aux_fr_classifier.parameters()) + list(aux_fr_dist_classifier.parameters()), \n",
        "                                    lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "        fr_frame_1, fr_frame_2, fr_targets = best_features['aux_fr']\n",
        "        src_nodes, tgt_nodes, dist = best_features['aux_fr_dist']\n",
        "        pre_scrambled_tgt_frames, pre_scrambled_targets = best_features['aux_frame']\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(best_features['best_epoches']):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "\n",
        "            # auxiliary task training - fr dist\n",
        "            y = aux_fr_dist_classifier(out[src_nodes], out[tgt_nodes])\n",
        "            aux_fr_dist_loss = aux_fr_dist_criterion(y, dist)\n",
        "\n",
        "            # auxiliary task training - fr\n",
        "            y = aux_fr_classifier(out[fr_frame_1], out[fr_frame_2])\n",
        "            aux_fr_loss = aux_fr_criterion(y, fr_targets)\n",
        "\n",
        "            # auxiliary task training - binary frame\n",
        "            tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus], dim=1)\n",
        "            y = aux_frame_classifier(tmp_out, pre_pos)\n",
        "            aux_frame_loss = aux_frame_criterion(y, pre_scrambled_targets)\n",
        "\n",
        "            # auxiliary task training - restoration\n",
        "            tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus, pre_lus], dim=1)\n",
        "            y = frame_classifier(tmp_out, pre_pos, pre_pos)\n",
        "            aux_frame_2_loss = criterion(y, pre_tgt_frames)\n",
        "\n",
        "            # actual task training\n",
        "            tmp_out = torch.cat([out[all_train_src_frames], all_train_src_lus, all_train_tgt_lus], dim=1)\n",
        "            y = frame_classifier(tmp_out, all_train_src_pos, all_train_tgt_pos)\n",
        "            main_loss = criterion(y, all_train_tgt_frames)\n",
        "\n",
        "            # cheat task training\n",
        "            tmp_out = torch.cat([out[test_src_frames], test_src_lus, test_tgt_lus], dim=1)\n",
        "            y = frame_classifier(tmp_out, test_src_pos, test_tgt_pos)\n",
        "            main_loss_cheat = criterion(y, test_tgt_frames)\n",
        "\n",
        "            # total_loss = aux_fr_dist_loss + aux_frame_loss + aux_frame_2_loss + main_loss \n",
        "            # total_loss = wrapper([aux_fr_loss, aux_fr_dist_loss, aux_frame_loss, aux_frame_2_loss, main_loss, main_loss_cheat])\n",
        "            total_loss = wrapper([aux_fr_loss, aux_fr_dist_loss, aux_frame_loss, aux_frame_2_loss, main_loss])\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            out = model(data, training=False)\n",
        "            out = torch.cat([out[test_src_frames], test_src_lus, test_tgt_lus], dim=1)\n",
        "            y = frame_classifier(out, test_src_pos, test_tgt_pos)\n",
        "            test_accs.append(accuracy(y, test_tgt_frames, topk=(5,))[0])\n",
        "                \n",
        "    return sum(test_accs)/len(test_accs)\n",
        "\n",
        "results = [run() for _ in range(10)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Outer CV: 0\n",
            "Inner CV: 0\n",
            "Losses Breakdown: 46.221282958984375 30.290754318237305 0.7739318013191223 0.869878351688385 7.1447014808654785 7.142016410827637\n",
            "Losses Breakdown: 10.752655029296875 7.137301921844482 0.7160197496414185 0.6958602070808411 2.1707866191864014 0.03268662840127945\n",
            "Losses Breakdown: 7.459760665893555 4.875702381134033 0.6977753639221191 0.6180656552314758 1.2654703855514526 0.0027465568855404854\n",
            "Losses Breakdown: 6.670661449432373 4.3966264724731445 0.7208347916603088 0.6252976059913635 0.9263280034065247 0.0015746551798656583\n",
            "Losses Breakdown: 6.329504013061523 4.210605144500732 0.6962615847587585 0.6163058876991272 0.8048178553581238 0.0015138810267671943\n",
            "Losses Breakdown: 6.241342067718506 4.164508819580078 0.7074829339981079 0.6104202270507812 0.7572982907295227 0.0016317549161612988\n",
            "Losses Breakdown: 6.210468769073486 4.173654556274414 0.7127192616462708 0.6099432110786438 0.7121124863624573 0.002039021346718073\n",
            "Losses Breakdown: 6.164359092712402 4.164924621582031 0.700647234916687 0.6133766770362854 0.6828634738922119 0.002547320444136858\n",
            "Losses Breakdown: 6.1310834884643555 4.165333271026611 0.7003021240234375 0.6093622446060181 0.6529654264450073 0.0031202647369354963\n",
            "Losses Breakdown: 6.103325843811035 4.162949562072754 0.694807231426239 0.60466068983078 0.6371278762817383 0.0037808259949088097\n",
            "Losses Breakdown: 6.110267639160156 4.162985801696777 0.7111882567405701 0.6093814373016357 0.6223605871200562 0.004351346287876368\n",
            "Losses Breakdown: 6.102975845336914 4.162715911865234 0.7181384563446045 0.6067769527435303 0.6106120347976685 0.004732205532491207\n",
            "Losses Breakdown: 6.090760707855225 4.1625518798828125 0.7167059779167175 0.6062312126159668 0.6002440452575684 0.005027854349464178\n",
            "Losses Breakdown: 6.061464786529541 4.162648677825928 0.6961119771003723 0.6064594388008118 0.5910857319831848 0.005158239509910345\n",
            "Losses Breakdown: 6.061473846435547 4.162619590759277 0.705986499786377 0.6037322282791138 0.5837271809577942 0.005408450961112976\n",
            "Losses Breakdown: 6.068633079528809 4.162514686584473 0.7166408896446228 0.606158971786499 0.5777884125709534 0.005530407652258873\n",
            "Losses Breakdown: 6.040572643280029 4.162456512451172 0.6952451467514038 0.6042969226837158 0.5729228258132935 0.005650958511978388\n",
            "Losses Breakdown: 6.0356125831604 4.162295818328857 0.6938608288764954 0.6058569550514221 0.5679346323013306 0.00566452881321311\n",
            "Losses Breakdown: 6.140232563018799 4.1624321937561035 0.8049665093421936 0.6035303473472595 0.5636096596717834 0.005693716462701559\n",
            "Losses Breakdown: 6.026576995849609 4.162839889526367 0.694146990776062 0.6022768020629883 0.5615059733390808 0.00580759160220623\n",
            "Losses Breakdown: 6.05883264541626 4.162720203399658 0.7286871671676636 0.6040698289871216 0.5575953722000122 0.0057601104490458965\n",
            "Losses Breakdown: 6.021344184875488 4.162593364715576 0.6948876976966858 0.6030665636062622 0.5549801588058472 0.005816409830003977\n",
            "Losses Breakdown: 6.040911674499512 4.162271022796631 0.718482255935669 0.6015023589134216 0.5528523921966553 0.005803022999316454\n",
            "Losses Breakdown: 6.023646354675293 4.162131309509277 0.702703595161438 0.6033743619918823 0.5496318936347961 0.005805046297609806\n",
            "Losses Breakdown: 6.034956932067871 4.162126541137695 0.7144377827644348 0.6040334105491638 0.5486263632774353 0.005732758902013302\n",
            "Losses Breakdown: 6.0174560546875 4.162103652954102 0.7018733024597168 0.6014470458030701 0.5462718605995178 0.00576022919267416\n",
            "early stopping\n",
            "Inner CV: 1\n",
            "Losses Breakdown: 46.08900833129883 30.3527889251709 0.7339506149291992 0.6579269170761108 7.143830299377441 7.200512886047363\n",
            "Losses Breakdown: 8.272696495056152 4.691687107086182 0.7100313901901245 0.638227105140686 2.1850600242614746 0.047690264880657196\n",
            "Losses Breakdown: 6.980709552764893 4.308285713195801 0.7651311755180359 0.6279614567756653 1.2751500606536865 0.004181221593171358\n",
            "Losses Breakdown: 6.531093120574951 4.174832820892334 0.789315402507782 0.6284311413764954 0.9361859560012817 0.0023277883883565664\n",
            "Losses Breakdown: 6.273970127105713 4.144127368927002 0.6976917386054993 0.6203057169914246 0.8098551034927368 0.001990221906453371\n",
            "Losses Breakdown: 6.24125862121582 4.1455397605896 0.7245925068855286 0.6101974248886108 0.758660614490509 0.002267640084028244\n",
            "Losses Breakdown: 6.16464900970459 4.14166259765625 0.6962653398513794 0.6111125349998474 0.7129402756690979 0.0026685683988034725\n",
            "Losses Breakdown: 6.141633033752441 4.1417012214660645 0.7131832242012024 0.6047908663749695 0.6786274313926697 0.0033299927599728107\n",
            "Losses Breakdown: 6.09749698638916 4.1415557861328125 0.6955391764640808 0.6031641364097595 0.653247058391571 0.003991317935287952\n",
            "Losses Breakdown: 6.16383171081543 4.14158296585083 0.778191864490509 0.6033942103385925 0.6359567046165466 0.004705802071839571\n",
            "Losses Breakdown: 6.090364456176758 4.141673564910889 0.7163113355636597 0.607040286064148 0.6196932196617126 0.005645906552672386\n",
            "Losses Breakdown: 6.315082550048828 4.142314910888672 0.9422338604927063 0.6144284009933472 0.610345721244812 0.00575935048982501\n",
            "Losses Breakdown: 6.053952693939209 4.142119884490967 0.7009676098823547 0.604542076587677 0.6002128720283508 0.006109660491347313\n",
            "Losses Breakdown: 6.73008394241333 4.141765117645264 1.3878819942474365 0.6034722924232483 0.5906562805175781 0.006308692507445812\n",
            "Losses Breakdown: 6.038308143615723 4.142006874084473 0.704112708568573 0.602207601070404 0.5836296081542969 0.006351652555167675\n",
            "Losses Breakdown: 6.724521636962891 4.141814231872559 1.3943500518798828 0.6052475571632385 0.576749861240387 0.006359999068081379\n",
            "Losses Breakdown: 6.031984329223633 4.141403675079346 0.7069354057312012 0.6042924523353577 0.5729183554649353 0.006434338632971048\n",
            "Losses Breakdown: 6.013385772705078 4.141788959503174 0.6966331005096436 0.6013003587722778 0.567124605178833 0.006539465859532356\n",
            "Losses Breakdown: 6.049664497375488 4.145534038543701 0.7324187755584717 0.601188063621521 0.563934862613678 0.006588981486856937\n",
            "Losses Breakdown: 6.048053741455078 4.14338493347168 0.7357720732688904 0.6027071475982666 0.5596558451652527 0.006534191779792309\n",
            "Losses Breakdown: 6.004673480987549 4.141507625579834 0.6989930272102356 0.6008756756782532 0.5566442608833313 0.006652227137237787\n",
            "Losses Breakdown: 6.035593032836914 4.141074180603027 0.7323496341705322 0.6006994247436523 0.5547817349433899 0.006688135210424662\n",
            "Losses Breakdown: 6.01512336730957 4.140982151031494 0.715376079082489 0.6017868518829346 0.5503977537155151 0.006580157205462456\n",
            "Losses Breakdown: 6.0746564865112305 4.140867710113525 0.7802264094352722 0.5967363715171814 0.5502064228057861 0.006619251798838377\n",
            "Losses Breakdown: 5.994720458984375 4.140763282775879 0.6990895867347717 0.6000186800956726 0.5482834577560425 0.006565726827830076\n",
            "Losses Breakdown: 5.996775150299072 4.1408772468566895 0.7030733823776245 0.6008234620094299 0.5455015897750854 0.006499066483229399\n",
            "Losses Breakdown: 6.017165184020996 4.140759468078613 0.7275802493095398 0.5979791879653931 0.5443311929702759 0.006515334825962782\n",
            "Losses Breakdown: 6.000143051147461 4.140632629394531 0.7143344879150391 0.5964320302009583 0.5424371957778931 0.006306630093604326\n",
            "Losses Breakdown: 6.000033855438232 4.1405863761901855 0.7106159925460815 0.5987676382064819 0.5436275005340576 0.006435860879719257\n",
            "Losses Breakdown: 5.993610382080078 4.141989231109619 0.707556426525116 0.5979733467102051 0.5396763682365417 0.006415506824851036\n",
            "Losses Breakdown: 5.978667736053467 4.141247272491455 0.6940138339996338 0.5975183844566345 0.5395131707191467 0.006374902557581663\n",
            "Losses Breakdown: 5.986293315887451 4.145137310028076 0.7009190320968628 0.5942180156707764 0.539614737033844 0.006404615473002195\n",
            "Losses Breakdown: 5.976226329803467 4.145427703857422 0.6945195198059082 0.5930913686752319 0.5367987155914307 0.006389178801327944\n",
            "Losses Breakdown: 5.988373279571533 4.144513130187988 0.7060918211936951 0.5960955023765564 0.5353659391403198 0.006306413095444441\n",
            "early stopping\n",
            "Inner CV: 2\n",
            "Losses Breakdown: 43.30833435058594 27.34394645690918 0.8211700320243835 0.8039678931236267 7.169969081878662 7.169281482696533\n",
            "Losses Breakdown: 10.239075660705566 6.166522979736328 0.8390156030654907 0.7074741721153259 2.4781415462493896 0.04792096093297005\n",
            "Losses Breakdown: 9.390401840209961 6.293830871582031 1.072205662727356 0.6231347322463989 1.3971606492996216 0.004070131573826075\n",
            "Losses Breakdown: 7.111681938171387 4.747750282287598 0.7532174587249756 0.6186935305595398 0.9895190596580505 0.0025016670115292072\n",
            "Losses Breakdown: 6.396775722503662 4.235540866851807 0.7040757536888123 0.6180087924003601 0.8370680809020996 0.0020821939688175917\n",
            "Losses Breakdown: 6.38032341003418 4.263051986694336 0.7300944328308105 0.6129229068756104 0.7721022963523865 0.0021515085827559233\n",
            "Losses Breakdown: 6.328761577606201 4.2364020347595215 0.7482765913009644 0.6107797026634216 0.7307729721069336 0.00253027374856174\n",
            "Losses Breakdown: 6.224052906036377 4.212138652801514 0.711094081401825 0.60776686668396 0.689960777759552 0.0030926158651709557\n",
            "Losses Breakdown: 6.1919941902160645 4.223044395446777 0.6953209638595581 0.6079779863357544 0.6619431376457214 0.0037078331224620342\n",
            "Losses Breakdown: 6.159256458282471 4.205874443054199 0.696306586265564 0.6074754595756531 0.6451858878135681 0.0044140988029539585\n",
            "Losses Breakdown: 6.149846076965332 4.211519241333008 0.6978002190589905 0.6056991219520569 0.6297613978385925 0.005066480953246355\n",
            "Losses Breakdown: 6.131308555603027 4.209027290344238 0.6941336989402771 0.6080160737037659 0.6146026253700256 0.0055288211442530155\n",
            "Losses Breakdown: 6.117240905761719 4.202973365783691 0.6959840059280396 0.6068734526634216 0.6054960489273071 0.005914431996643543\n",
            "Losses Breakdown: 6.129508972167969 4.203680038452148 0.7189566493034363 0.6053230166435242 0.5954769253730774 0.006072502117604017\n",
            "Losses Breakdown: 6.0956902503967285 4.19994592666626 0.6937265396118164 0.6056589484214783 0.590095579624176 0.006263117305934429\n",
            "Losses Breakdown: 6.268219470977783 4.237556457519531 0.834336519241333 0.6088688969612122 0.5811949372291565 0.006262584123760462\n",
            "Losses Breakdown: 6.095884323120117 4.216098308563232 0.6943238973617554 0.6039862632751465 0.575176477432251 0.006298976019024849\n",
            "Losses Breakdown: 6.102463722229004 4.2125396728515625 0.707116425037384 0.6044936776161194 0.5719155073165894 0.006398429162800312\n",
            "Losses Breakdown: 6.0981550216674805 4.216995716094971 0.701228678226471 0.6040593981742859 0.5694750547409058 0.006396416574716568\n",
            "Losses Breakdown: 6.068065643310547 4.20084810256958 0.6939763426780701 0.6032968759536743 0.563575029373169 0.0063697220757603645\n",
            "Losses Breakdown: 6.065879821777344 4.198558807373047 0.696329653263092 0.6035686135292053 0.561004638671875 0.006418074481189251\n",
            "Losses Breakdown: 6.058610439300537 4.195775508880615 0.6949132084846497 0.604163408279419 0.5573412775993347 0.006417087744921446\n",
            "Losses Breakdown: 6.0594377517700195 4.194244384765625 0.7021135687828064 0.6021218299865723 0.5545372366905212 0.006420457735657692\n",
            "Losses Breakdown: 6.075981616973877 4.19418478012085 0.719460129737854 0.6024887561798096 0.5535115003585815 0.006336499936878681\n",
            "Losses Breakdown: 6.046150207519531 4.192728519439697 0.693442702293396 0.6017110347747803 0.5518767833709717 0.0063904281705617905\n",
            "Losses Breakdown: 6.057220458984375 4.194179058074951 0.7054006457328796 0.6023239493370056 0.5490092635154724 0.006307398900389671\n",
            "Losses Breakdown: 6.04377555847168 4.192108154296875 0.6963952779769897 0.6014596819877625 0.5475136637687683 0.00629889452829957\n",
            "Losses Breakdown: 6.041131973266602 4.192507266998291 0.6942066550254822 0.6023064255714417 0.5457978248596191 0.006313899531960487\n",
            "Losses Breakdown: 6.038027763366699 4.192875862121582 0.6939653754234314 0.5997905135154724 0.5451592803001404 0.006236720364540815\n",
            "early stopping\n",
            "Inner CV: 3\n",
            "Losses Breakdown: 44.88432312011719 29.193584442138672 0.7345865964889526 0.7027440667152405 7.14321231842041 7.110197067260742\n",
            "Losses Breakdown: 10.161015510559082 6.370270729064941 0.9194855690002441 0.6445744037628174 2.1812665462493896 0.045418016612529755\n",
            "Losses Breakdown: 7.541254043579102 4.847808837890625 0.7624117732048035 0.6571755409240723 1.2695804834365845 0.004277690313756466\n",
            "Losses Breakdown: 6.704381942749023 4.418951034545898 0.7283912301063538 0.6247796416282654 0.9302754402160645 0.0019845685455948114\n",
            "Losses Breakdown: 6.350219249725342 4.238365650177002 0.69329434633255 0.60722416639328 0.8095656037330627 0.0017695557326078415\n",
            "Losses Breakdown: 6.212675094604492 4.149328708648682 0.6936032772064209 0.6096001863479614 0.7580577731132507 0.002085529500618577\n",
            "Losses Breakdown: 6.184010028839111 4.153812885284424 0.7004457712173462 0.6118204593658447 0.715361475944519 0.0025697730015963316\n",
            "Losses Breakdown: 6.11932897567749 4.144095420837402 0.6933988928794861 0.5995389819145203 0.6791035532951355 0.0031923246569931507\n",
            "Losses Breakdown: 6.124539375305176 4.1440935134887695 0.7161223888397217 0.6051385998725891 0.6553512811660767 0.003833235939964652\n",
            "Losses Breakdown: 6.088100433349609 4.145188808441162 0.6967567801475525 0.6074492335319519 0.6342304348945618 0.004475165158510208\n",
            "Losses Breakdown: 6.070650577545166 4.143062591552734 0.6932604312896729 0.6076545119285583 0.6215304732322693 0.005142505280673504\n",
            "Losses Breakdown: 6.061506271362305 4.142956256866455 0.6963556408882141 0.6068485379219055 0.6097021102905273 0.005643374286592007\n",
            "Losses Breakdown: 6.0501861572265625 4.1428751945495605 0.6930862069129944 0.6082837581634521 0.6000744104385376 0.00586644047871232\n",
            "Losses Breakdown: 6.038174629211426 4.1426873207092285 0.6934849619865417 0.6035220623016357 0.5924364924430847 0.006043892353773117\n",
            "Losses Breakdown: 6.03162956237793 4.142653942108154 0.6936137676239014 0.6044530868530273 0.5846904516220093 0.006218410562723875\n",
            "Losses Breakdown: 6.023351192474365 4.142836570739746 0.6929346919059753 0.6022160053253174 0.579021155834198 0.006342936772853136\n",
            "Losses Breakdown: 6.03004789352417 4.142529487609863 0.7028715014457703 0.6016424894332886 0.5766077041625977 0.006396947894245386\n",
            "Losses Breakdown: 6.015466690063477 4.14260721206665 0.693405032157898 0.605379045009613 0.5676395893096924 0.006435197312384844\n",
            "Losses Breakdown: 6.018038749694824 4.1431450843811035 0.7001001238822937 0.6001299023628235 0.5681102275848389 0.006553650833666325\n",
            "Losses Breakdown: 6.061281204223633 4.142589569091797 0.7515063285827637 0.5999221205711365 0.5607452988624573 0.006517936009913683\n",
            "Losses Breakdown: 6.0324201583862305 4.142548561096191 0.7249438762664795 0.6005154252052307 0.5579043030738831 0.006507859565317631\n",
            "Losses Breakdown: 6.003676891326904 4.142487525939941 0.7004312872886658 0.5987386107444763 0.5555794835090637 0.006439820863306522\n",
            "Losses Breakdown: 6.018093585968018 4.142547607421875 0.7192659378051758 0.5960751175880432 0.5536861419677734 0.006518677342683077\n",
            "Losses Breakdown: 6.011624813079834 4.142571926116943 0.7077808976173401 0.6034826636314392 0.5513826012611389 0.006406955886632204\n",
            "early stopping\n",
            "Inner CV: 4\n",
            "Losses Breakdown: 44.15857696533203 28.39085578918457 0.7502968907356262 0.6677915453910828 7.1425065994262695 7.207128047943115\n",
            "Losses Breakdown: 7.868254661560059 4.196944713592529 0.7246537804603577 0.7194175720214844 2.184359550476074 0.042878903448581696\n",
            "Losses Breakdown: 6.769803047180176 4.182765960693359 0.7011426687240601 0.6050522923469543 1.2770264148712158 0.0038155699148774147\n",
            "Losses Breakdown: 6.708628177642822 4.441030502319336 0.6978220343589783 0.6252485513687134 0.9424211382865906 0.0021061592269688845\n",
            "Losses Breakdown: 6.46205997467041 4.332244396209717 0.7071883082389832 0.6093385815620422 0.8113765120506287 0.0019122300436720252\n",
            "Losses Breakdown: 6.327084064483643 4.238151550292969 0.7166466116905212 0.6097475290298462 0.7603382468223572 0.0022000183816999197\n",
            "Losses Breakdown: 6.293213367462158 4.199756622314453 0.7719245553016663 0.6042270660400391 0.714531660079956 0.002773965010419488\n",
            "Losses Breakdown: 6.323742866516113 4.189700603485107 0.8497364521026611 0.6033473014831543 0.6776737570762634 0.003284537699073553\n",
            "Losses Breakdown: 6.1906867027282715 4.181529998779297 0.7478423118591309 0.6043041348457336 0.6529682278633118 0.004041624255478382\n",
            "Losses Breakdown: 6.1179728507995605 4.180771350860596 0.6962113976478577 0.601855993270874 0.6344512104988098 0.004682568367570639\n",
            "Losses Breakdown: 6.108191967010498 4.180598258972168 0.6995902061462402 0.5996760725975037 0.6230446696281433 0.005283109378069639\n",
            "Losses Breakdown: 6.1198859214782715 4.181203365325928 0.7211702466011047 0.5999917984008789 0.611839234828949 0.005681244190782309\n",
            "Losses Breakdown: 6.147017002105713 4.180804252624512 0.7581936717033386 0.6006614565849304 0.6014536619186401 0.005904339719563723\n",
            "Losses Breakdown: 6.078602313995361 4.184271812438965 0.6967504620552063 0.5981519222259521 0.593254804611206 0.006172499153763056\n",
            "Losses Breakdown: 6.067515850067139 4.179872512817383 0.6957996487617493 0.6010735034942627 0.5845668911933899 0.0062031312845647335\n",
            "Losses Breakdown: 6.060811519622803 4.180332660675049 0.695995569229126 0.6001477837562561 0.5780442357063293 0.006290758959949017\n",
            "Losses Breakdown: 6.055495262145996 4.179780006408691 0.6950905323028564 0.6014066338539124 0.5728102326393127 0.006408379878848791\n",
            "Losses Breakdown: 6.053016662597656 4.179839134216309 0.6969938278198242 0.600976288318634 0.5687379240989685 0.006469418294727802\n",
            "Losses Breakdown: 6.0446696281433105 4.1796345710754395 0.6942799091339111 0.5995372533798218 0.5647416710853577 0.006475804373621941\n",
            "Losses Breakdown: 6.057159423828125 4.179683685302734 0.7082718014717102 0.6017968058586121 0.5609482526779175 0.006458611693233252\n",
            "Losses Breakdown: 6.045107364654541 4.179769515991211 0.7006089687347412 0.5996255278587341 0.5586358904838562 0.0064672729931771755\n",
            "Losses Breakdown: 6.0557861328125 4.179842948913574 0.7129746079444885 0.6011475920677185 0.5553194880485535 0.0065016476437449455\n",
            "Losses Breakdown: 6.087149620056152 4.179671764373779 0.7482881546020508 0.5981457829475403 0.5545262098312378 0.0065175318159163\n",
            "early stopping\n",
            "Outer CV: 1\n",
            "Inner CV: 0\n",
            "Losses Breakdown: 42.355892181396484 26.614547729492188 0.7475405931472778 0.7230003476142883 7.167458534240723 7.103342533111572\n",
            "Losses Breakdown: 11.80077075958252 8.07343578338623 0.744533360004425 0.642885148525238 2.292586088180542 0.047330282628536224\n",
            "Losses Breakdown: 6.842782974243164 4.16180944442749 0.7374464273452759 0.6101407408714294 1.3269671201705933 0.006419418845325708\n",
            "Losses Breakdown: 6.395895481109619 4.112924575805664 0.7062602639198303 0.6092746257781982 0.9647623300552368 0.0026739337481558323\n",
            "early stopping\n",
            "Inner CV: 1\n",
            "Losses Breakdown: 44.296695709228516 28.62355613708496 0.7358171343803406 0.6698489785194397 7.1560187339782715 7.111454963684082\n",
            "Losses Breakdown: 7.822720050811768 4.172778129577637 0.7207841873168945 0.6688967943191528 2.215236186981201 0.04502510651946068\n",
            "Losses Breakdown: 7.188197612762451 4.43833065032959 0.8037258982658386 0.6536574959754944 1.2883756160736084 0.004108093678951263\n",
            "Losses Breakdown: 6.83604621887207 4.5706939697265625 0.6968803405761719 0.6214927434921265 0.9447039365768433 0.002274908125400543\n",
            "early stopping\n",
            "Inner CV: 2\n",
            "Losses Breakdown: 43.81119918823242 27.925867080688477 0.797258198261261 0.6944284439086914 7.185795307159424 7.207852363586426\n",
            "Losses Breakdown: 9.626788139343262 5.964286804199219 0.7226322293281555 0.6822366714477539 2.2086057662963867 0.0490267388522625\n",
            "Losses Breakdown: 7.393271446228027 4.713348388671875 0.7446978092193604 0.6385447382926941 1.2916845083236694 0.004996242932975292\n",
            "Losses Breakdown: 6.6636962890625 4.29808235168457 0.7944045066833496 0.6197478175163269 0.9485428333282471 0.0029191377107053995\n",
            "Losses Breakdown: 6.315263748168945 4.187251091003418 0.6971443295478821 0.6110721230506897 0.817528247833252 0.0022678261157125235\n",
            "Losses Breakdown: 6.225780010223389 4.157464981079102 0.6967484354972839 0.6075752377510071 0.761562705039978 0.0024287630803883076\n",
            "Losses Breakdown: 6.228476524353027 4.136147975921631 0.7646981477737427 0.6084272265434265 0.7162731289863586 0.0029301592148840427\n",
            "Losses Breakdown: 6.157585620880127 4.123605251312256 0.7424172163009644 0.6088188290596008 0.6790311932563782 0.003712438978254795\n",
            "Losses Breakdown: 6.100480079650879 4.12473726272583 0.7060886025428772 0.6076582670211792 0.6574464440345764 0.0045493789948523045\n",
            "Losses Breakdown: 6.073094367980957 4.12405252456665 0.7030543684959412 0.60504549741745 0.6356020569801331 0.005340317729860544\n",
            "Losses Breakdown: 6.1424970626831055 4.124032497406006 0.78426194190979 0.606137752532959 0.6220144629478455 0.006049989257007837\n",
            "Losses Breakdown: 6.062851428985596 4.1234259605407715 0.7246416211128235 0.5993443131446838 0.6088161468505859 0.006623467430472374\n",
            "Losses Breakdown: 6.036673545837402 4.123424053192139 0.7045280337333679 0.6022616624832153 0.5994786620140076 0.006981217302381992\n",
            "Losses Breakdown: 6.076624870300293 4.123382568359375 0.7486257553100586 0.6048856973648071 0.5925273895263672 0.007203385699540377\n",
            "Losses Breakdown: 6.086368560791016 4.123426914215088 0.770207405090332 0.6015581488609314 0.5838256478309631 0.007350416854023933\n",
            "Losses Breakdown: 6.022885799407959 4.124346733093262 0.7108306288719177 0.6015112400054932 0.5787599682807922 0.007437257561832666\n",
            "Losses Breakdown: 6.11174201965332 4.123216152191162 0.807002067565918 0.6021046042442322 0.5719045996665955 0.007514694705605507\n",
            "Losses Breakdown: 6.02250337600708 4.123435020446777 0.7166110277175903 0.6071335077285767 0.5677945017814636 0.007529819384217262\n",
            "Losses Breakdown: 6.054078578948975 4.123363018035889 0.7537595629692078 0.6056306958198547 0.5638163685798645 0.00750888092443347\n",
            "Losses Breakdown: 6.001058578491211 4.123108386993408 0.7091347575187683 0.600600004196167 0.5606463551521301 0.0075686355121433735\n",
            "Losses Breakdown: 5.982635498046875 4.123030185699463 0.6990228295326233 0.5954853892326355 0.5575525760650635 0.007544432766735554\n",
            "Losses Breakdown: 6.019556522369385 4.123458385467529 0.7384863495826721 0.5959520936012268 0.5541154742240906 0.007544626481831074\n",
            "early stopping\n",
            "Inner CV: 3\n",
            "Losses Breakdown: 42.09933090209961 26.37192153930664 0.7395882606506348 0.7280216813087463 7.151710033416748 7.108088970184326\n",
            "Losses Breakdown: 9.926146507263184 5.940855503082275 0.7506551146507263 0.6398045420646667 2.5369019508361816 0.0579293817281723\n",
            "Losses Breakdown: 9.245657920837402 6.426382541656494 0.7360628843307495 0.6516468524932861 1.4258707761764526 0.005694834981113672\n",
            "Losses Breakdown: 7.1653642654418945 4.758471488952637 0.7753370404243469 0.6201267242431641 1.0088560581207275 0.002572895959019661\n",
            "Losses Breakdown: 6.303689956665039 4.144906044006348 0.6949005126953125 0.6173887848854065 0.8443787693977356 0.0021156177390366793\n",
            "Losses Breakdown: 6.3388261795043945 4.243698596954346 0.6977476477622986 0.6138638854026794 0.7811944484710693 0.0023211080115288496\n",
            "Losses Breakdown: 6.273353099822998 4.194415092468262 0.7280129790306091 0.6132824420928955 0.7349106073379517 0.0027324603870511055\n",
            "Losses Breakdown: 6.180291175842285 4.1373796463012695 0.7357650995254517 0.6094405055046082 0.694249153137207 0.003457004437223077\n",
            "Losses Breakdown: 6.113933086395264 4.134169101715088 0.698238730430603 0.6119046807289124 0.6653288006782532 0.004291302990168333\n",
            "Losses Breakdown: 6.101999282836914 4.134821891784668 0.7073813080787659 0.6100374460220337 0.6445997953414917 0.005159011576324701\n",
            "Losses Breakdown: 6.073493003845215 4.133663654327393 0.6938475370407104 0.6107996702194214 0.6292139291763306 0.005968320649117231\n",
            "Losses Breakdown: 6.0610246658325195 4.134191989898682 0.6936314105987549 0.610693097114563 0.6159064769744873 0.006601165048778057\n",
            "Losses Breakdown: 6.051292419433594 4.134997844696045 0.6963039636611938 0.6080260872840881 0.60496985912323 0.006994923576712608\n",
            "Losses Breakdown: 6.038815021514893 4.132964611053467 0.6950053572654724 0.6071339249610901 0.5964150428771973 0.007295871153473854\n",
            "Losses Breakdown: 6.029863357543945 4.133466720581055 0.693368673324585 0.6072308421134949 0.5883414149284363 0.007455875165760517\n",
            "Losses Breakdown: 6.025003433227539 4.132882595062256 0.6940984129905701 0.6086491346359253 0.5818383693695068 0.007534828037023544\n",
            "Losses Breakdown: 6.024080276489258 4.13425350189209 0.6981332302093506 0.6078183054924011 0.5762365460395813 0.00763860484585166\n",
            "Losses Breakdown: 6.017547607421875 4.130918502807617 0.70127272605896 0.6069251298904419 0.5708078145980835 0.007623298093676567\n",
            "Losses Breakdown: 6.008448600769043 4.134687423706055 0.6946015357971191 0.6054527163505554 0.5660454630851746 0.007661596406251192\n",
            "Losses Breakdown: 6.032101154327393 4.132687091827393 0.7233177423477173 0.6059891581535339 0.5624588131904602 0.007648434955626726\n",
            "Losses Breakdown: 6.00738525390625 4.131194591522217 0.7017514109611511 0.6070947051048279 0.5596956014633179 0.007648812606930733\n",
            "Losses Breakdown: 5.994040489196777 4.131607532501221 0.6933106184005737 0.6054894924163818 0.5560141205787659 0.0076190209947526455\n",
            "Losses Breakdown: 5.990911483764648 4.132045269012451 0.6933850049972534 0.6041817665100098 0.553730309009552 0.007569068111479282\n",
            "Losses Breakdown: 5.992940425872803 4.131972312927246 0.6978558301925659 0.604401707649231 0.5511301755905151 0.0075804986990988255\n",
            "Losses Breakdown: 6.051712989807129 4.131926536560059 0.7574042081832886 0.6053265929222107 0.5495076775550842 0.007547805085778236\n",
            "Losses Breakdown: 5.991065502166748 4.133790969848633 0.6959983110427856 0.6060035228729248 0.5477442145347595 0.007528539281338453\n",
            "Losses Breakdown: 5.996054649353027 4.132452964782715 0.7084529995918274 0.6021694540977478 0.5454818606376648 0.007497148588299751\n",
            "Losses Breakdown: 6.000972270965576 4.1323089599609375 0.7137801051139832 0.6037061810493469 0.5437142848968506 0.007463193964213133\n",
            "Losses Breakdown: 5.985450267791748 4.132688045501709 0.6987133026123047 0.6040949821472168 0.5425063371658325 0.007447569165378809\n",
            "Losses Breakdown: 6.014906883239746 4.132462501525879 0.7278834581375122 0.6057609915733337 0.5413910746574402 0.00740895327180624\n",
            "Losses Breakdown: 5.976531028747559 4.129880428314209 0.6949374079704285 0.6045195460319519 0.5397895574569702 0.007404390722513199\n",
            "Losses Breakdown: 5.976100444793701 4.131375789642334 0.6952918171882629 0.6026593446731567 0.5393964052200317 0.007376937661319971\n",
            "Losses Breakdown: 5.975491046905518 4.131412029266357 0.6946462392807007 0.6040979623794556 0.5379763245582581 0.007358658127486706\n",
            "early stopping\n",
            "Inner CV: 4\n",
            "Losses Breakdown: 44.91084671020508 29.022939682006836 0.7465035319328308 0.8098974227905273 7.167016983032227 7.164485454559326\n",
            "Losses Breakdown: 8.722640037536621 5.06861686706543 0.7006942629814148 0.6991268396377563 2.2073512077331543 0.04685152322053909\n",
            "Losses Breakdown: 7.433340549468994 4.824463844299316 0.7027986645698547 0.6129047274589539 1.288862705230713 0.004310477524995804\n",
            "Losses Breakdown: 6.698681354522705 4.345367431640625 0.7815852761268616 0.6251645684242249 0.944070041179657 0.0024941719602793455\n",
            "Losses Breakdown: 6.321497440338135 4.184871196746826 0.7010659575462341 0.6186086535453796 0.8149132132530212 0.0020382923539727926\n",
            "Losses Breakdown: 6.428545951843262 4.101922988891602 0.9469509720802307 0.6148248314857483 0.7626469731330872 0.0022002537734806538\n",
            "Losses Breakdown: 6.107193946838379 4.079068660736084 0.699932336807251 0.6084940433502197 0.7169113159179688 0.002787727629765868\n",
            "Losses Breakdown: 6.122990131378174 4.079214096069336 0.7513715028762817 0.6090592741966248 0.679800271987915 0.0035453676246106625\n",
            "Losses Breakdown: 6.066956520080566 4.079885959625244 0.7219804525375366 0.6045685410499573 0.6561421155929565 0.004379328340291977\n",
            "Losses Breakdown: 6.028472423553467 4.078759670257568 0.7028983235359192 0.6064034104347229 0.6351880431175232 0.0052229659631848335\n",
            "Losses Breakdown: 6.0589165687561035 4.078235626220703 0.7462323904037476 0.6056236028671265 0.6228746175765991 0.0059500583447515965\n",
            "Losses Breakdown: 6.0137434005737305 4.082088470458984 0.7066313028335571 0.6090078949928284 0.6095924973487854 0.006423659157007933\n",
            "Losses Breakdown: 6.081447124481201 4.0779900550842285 0.7908325791358948 0.606593668460846 0.5992204546928406 0.006810673512518406\n",
            "Losses Breakdown: 6.008792877197266 4.077810764312744 0.7282557487487793 0.6048704385757446 0.5907816290855408 0.00707421638071537\n",
            "Losses Breakdown: 6.070246696472168 4.077841758728027 0.7986771464347839 0.6025389432907104 0.5838597416877747 0.007328904699534178\n",
            "Losses Breakdown: 6.024280071258545 4.077938079833984 0.7549550533294678 0.606271505355835 0.5776882767677307 0.007426772732287645\n",
            "Losses Breakdown: 6.082107067108154 4.078305244445801 0.8199268579483032 0.6034900546073914 0.5729190111160278 0.007466443348675966\n",
            "Losses Breakdown: 5.961367130279541 4.078361988067627 0.7034828066825867 0.6055842041969299 0.56650710105896 0.007431353442370892\n",
            "Losses Breakdown: 5.956523895263672 4.0787248611450195 0.699654221534729 0.6065773367881775 0.5640205144882202 0.007546914275735617\n",
            "Losses Breakdown: 5.954420566558838 4.078413963317871 0.7071625590324402 0.6015252470970154 0.5597376227378845 0.0075813960283994675\n",
            "Losses Breakdown: 5.950927257537842 4.07957124710083 0.6974281072616577 0.608391284942627 0.5579419136047363 0.0075944396667182446\n",
            "Losses Breakdown: 5.9465556144714355 4.081732273101807 0.7019909620285034 0.6003568172454834 0.554805338382721 0.007670288439840078\n",
            "Losses Breakdown: 5.99146032333374 4.077544212341309 0.7546945810317993 0.5995360612869263 0.5521659851074219 0.007519488222897053\n",
            "Losses Breakdown: 5.9290242195129395 4.077399730682373 0.6947224140167236 0.6002407073974609 0.5491902232170105 0.007471258286386728\n",
            "early stopping\n",
            "Outer CV: 2\n",
            "Inner CV: 0\n",
            "Losses Breakdown: 43.533443450927734 27.54852294921875 0.8774237036705017 0.743245005607605 7.167446613311768 7.196803092956543\n",
            "Losses Breakdown: 7.7250776290893555 4.138547420501709 0.6984757781028748 0.6208438277244568 2.215308904647827 0.0519016869366169\n",
            "Losses Breakdown: 7.6584858894348145 4.992629528045654 0.6960397362709045 0.6695428490638733 1.2930433750152588 0.007230347488075495\n",
            "Losses Breakdown: 6.5836968421936035 4.264850616455078 0.7364159226417542 0.6290120482444763 0.9504702091217041 0.0029481234960258007\n",
            "early stopping\n",
            "Inner CV: 1\n",
            "Losses Breakdown: 44.94022750854492 29.124530792236328 0.7374224066734314 0.6796009540557861 7.210849285125732 7.1878228187561035\n",
            "Losses Breakdown: 10.004990577697754 6.1816229820251465 0.8091291785240173 0.7342888712882996 2.2329752445220947 0.046974021941423416\n",
            "Losses Breakdown: 6.977118492126465 4.3351263999938965 0.7143065333366394 0.6154984831809998 1.3082587718963623 0.00392803642898798\n",
            "Losses Breakdown: 6.382352352142334 4.016538143157959 0.7779762148857117 0.6276118159294128 0.9577081203460693 0.002518158406019211\n",
            "early stopping\n",
            "Inner CV: 2\n",
            "Losses Breakdown: 45.66511154174805 29.747150421142578 0.7560237050056458 0.8231525421142578 7.169322490692139 7.16946268081665\n",
            "Losses Breakdown: 9.419755935668945 5.774767875671387 0.6934935450553894 0.6647484302520752 2.233358860015869 0.0533868633210659\n",
            "Losses Breakdown: 7.132937431335449 4.464188575744629 0.72298663854599 0.6288900375366211 1.3110835552215576 0.005788723006844521\n",
            "Losses Breakdown: 6.443418502807617 4.16887092590332 0.6956921815872192 0.6182118058204651 0.9576431512832642 0.0030006105080246925\n",
            "Losses Breakdown: 6.304683685302734 4.145621299743652 0.7026557922363281 0.6297183036804199 0.8241074681282043 0.0025807858910411596\n",
            "Losses Breakdown: 6.229368209838867 4.148545742034912 0.695231556892395 0.6136409640312195 0.7689907550811768 0.002959171077236533\n",
            "Losses Breakdown: 6.175739288330078 4.152674674987793 0.6946393251419067 0.6049385070800781 0.720051646232605 0.003435360034927726\n",
            "Losses Breakdown: 6.1410298347473145 4.146617412567139 0.6961976289749146 0.6115666627883911 0.6824517846107483 0.004196483641862869\n",
            "Losses Breakdown: 6.102179050445557 4.139408111572266 0.6932780146598816 0.6071943640708923 0.6571595668792725 0.005139010027050972\n",
            "Losses Breakdown: 6.092002868652344 4.13886022567749 0.7004979252815247 0.6083039045333862 0.6382617950439453 0.006078601814806461\n",
            "Losses Breakdown: 6.067925453186035 4.13855504989624 0.6934202909469604 0.6058446168899536 0.6232336759567261 0.006871581077575684\n",
            "Losses Breakdown: 6.05753755569458 4.138888835906982 0.6968382596969604 0.603141188621521 0.611292839050293 0.00737601425498724\n",
            "Losses Breakdown: 6.047293663024902 4.138168811798096 0.6973694562911987 0.603787899017334 0.6003103852272034 0.0076570454984903336\n",
            "Losses Breakdown: 6.045714378356934 4.137730121612549 0.7000281810760498 0.6075972318649292 0.5924387574195862 0.007920432835817337\n",
            "Losses Breakdown: 6.038608074188232 4.1450910568237305 0.6945651769638062 0.6043263673782349 0.5865141153335571 0.008111700415611267\n",
            "Losses Breakdown: 6.0242486000061035 4.137438774108887 0.6938900351524353 0.6038243174552917 0.5808834433555603 0.008212033659219742\n",
            "Losses Breakdown: 6.020391941070557 4.1370158195495605 0.6947646737098694 0.6054611802101135 0.5749260187149048 0.008224517107009888\n",
            "Losses Breakdown: 6.006004333496094 4.136703968048096 0.6938110589981079 0.5974796414375305 0.5697887539863586 0.008220630697906017\n",
            "Losses Breakdown: 6.009133338928223 4.137223243713379 0.6930495500564575 0.6050282120704651 0.5656521916389465 0.008180101402103901\n",
            "Losses Breakdown: 6.001809597015381 4.136697769165039 0.6930505037307739 0.6029651165008545 0.5609005093574524 0.00819526705890894\n",
            "Losses Breakdown: 5.995802879333496 4.136438846588135 0.6937633156776428 0.5990066528320312 0.5583655834197998 0.008228453807532787\n",
            "Losses Breakdown: 5.992043495178223 4.136318206787109 0.6947308778762817 0.5960476994514465 0.5567905902862549 0.008155829273164272\n",
            "Losses Breakdown: 5.99857759475708 4.136105060577393 0.7004331946372986 0.5998352766036987 0.5540690422058105 0.008134757168591022\n",
            "Losses Breakdown: 5.988039016723633 4.1365556716918945 0.6950513124465942 0.5972099900245667 0.5511499047279358 0.008072610944509506\n",
            "early stopping\n",
            "Inner CV: 3\n",
            "Losses Breakdown: 43.55165100097656 27.649658203125 0.7272784113883972 0.7994590997695923 7.19542932510376 7.179825305938721\n",
            "Losses Breakdown: 14.749608039855957 10.85708236694336 0.7536054849624634 0.79143226146698 2.2883949279785156 0.05909208580851555\n",
            "Losses Breakdown: 8.481720924377441 5.627256393432617 0.7801755666732788 0.7425675392150879 1.3250128030776978 0.006707864347845316\n",
            "Losses Breakdown: 6.550785541534424 4.20125675201416 0.7097111940383911 0.6726383566856384 0.9642128944396973 0.002966349944472313\n",
            "Losses Breakdown: 6.540899753570557 4.343352794647217 0.7319396734237671 0.632762610912323 0.8302276730537415 0.0026168799959123135\n",
            "Losses Breakdown: 6.383940696716309 4.294890880584717 0.6965557336807251 0.6167264580726624 0.772879958152771 0.0028878359589725733\n",
            "Losses Breakdown: 6.272761821746826 4.230940341949463 0.7022361755371094 0.6104369163513184 0.7257161736488342 0.0034323979634791613\n",
            "Losses Breakdown: 6.214633941650391 4.2089409828186035 0.706121563911438 0.6092222332954407 0.6861612200737 0.004188294522464275\n",
            "Losses Breakdown: 6.180215835571289 4.20216178894043 0.7000311613082886 0.6092755794525146 0.6636142134666443 0.005132679827511311\n",
            "Losses Breakdown: 6.142908096313477 4.1945881843566895 0.6937431693077087 0.6073165535926819 0.6411818265914917 0.006078267935663462\n",
            "Losses Breakdown: 6.188092231750488 4.194396495819092 0.7500936985015869 0.6076700091362 0.6290660500526428 0.006866107229143381\n",
            "Losses Breakdown: 6.117781162261963 4.1936869621276855 0.6951757669448853 0.606198251247406 0.6151707172393799 0.007549857255071402\n",
            "Losses Breakdown: 6.106860160827637 4.194145679473877 0.6958746910095215 0.6055684685707092 0.6034138202667236 0.00785753782838583\n",
            "Losses Breakdown: 6.126811504364014 4.193120002746582 0.7247450947761536 0.6040026545524597 0.5968156456947327 0.008127957582473755\n",
            "Losses Breakdown: 6.120479583740234 4.19335412979126 0.7237445116043091 0.6049033403396606 0.5901510715484619 0.008326602168381214\n",
            "Losses Breakdown: 6.094171047210693 4.192845344543457 0.7064517140388489 0.6040100455284119 0.5825426578521729 0.008321402594447136\n",
            "Losses Breakdown: 6.076608657836914 4.192523002624512 0.6951362490653992 0.6050266623497009 0.5755890011787415 0.008333593606948853\n",
            "Losses Breakdown: 6.087159633636475 4.192416191101074 0.711853563785553 0.6039029359817505 0.5707014799118042 0.008285285905003548\n",
            "Losses Breakdown: 6.0685625076293945 4.193838119506836 0.6941138505935669 0.6030531525611877 0.5691848993301392 0.008372850716114044\n",
            "Losses Breakdown: 6.065943241119385 4.192343711853027 0.6958321332931519 0.6060208082199097 0.5633849501609802 0.008361155167222023\n",
            "Losses Breakdown: 6.0798492431640625 4.1926422119140625 0.7170128226280212 0.6020321249961853 0.5599244832992554 0.008237509056925774\n",
            "Losses Breakdown: 6.128364562988281 4.192413806915283 0.7640970945358276 0.6038587093353271 0.5594279766082764 0.008566685020923615\n",
            "Losses Breakdown: 6.054936408996582 4.192755222320557 0.6964777112007141 0.6029283404350281 0.5546319484710693 0.008143622428178787\n",
            "Losses Breakdown: 6.064545631408691 4.192224502563477 0.7074384093284607 0.6035845279693604 0.5531406998634338 0.008157930336892605\n",
            "Losses Breakdown: 6.078327655792236 4.192132949829102 0.724190890789032 0.6032620668411255 0.5506695508956909 0.00807214342057705\n",
            "Losses Breakdown: 6.0761613845825195 4.192931175231934 0.7229287028312683 0.6025601625442505 0.549611508846283 0.008130173198878765\n",
            "Losses Breakdown: 6.247660160064697 4.19202184677124 0.8986930847167969 0.6025521755218506 0.5462493896484375 0.008144034072756767\n",
            "Losses Breakdown: 6.068655490875244 4.191966533660889 0.7212655544281006 0.6003390550613403 0.5470246076583862 0.008059828542172909\n",
            "early stopping\n",
            "Inner CV: 4\n",
            "Losses Breakdown: 42.61906433105469 26.852127075195312 0.744306206703186 0.6713048815727234 7.176229000091553 7.175097942352295\n",
            "Losses Breakdown: 15.769047737121582 11.8267240524292 0.7032164335250854 0.6449102759361267 2.523585557937622 0.0706116184592247\n",
            "Losses Breakdown: 7.335942268371582 4.485764026641846 0.761435329914093 0.6557362079620361 1.4269239902496338 0.006082616280764341\n",
            "Losses Breakdown: 6.681236743927002 4.338252544403076 0.6990278959274292 0.6292235851287842 1.0114988088607788 0.003233732422813773\n",
            "Losses Breakdown: 6.351568222045898 4.180676460266113 0.7037664651870728 0.6161592602729797 0.8483235239982605 0.0026424082461744547\n",
            "Losses Breakdown: 6.274729251861572 4.183857440948486 0.6944833397865295 0.6100341081619263 0.7835050225257874 0.0028495530132204294\n",
            "Losses Breakdown: 6.230670928955078 4.182837009429932 0.6942752003669739 0.6125726699829102 0.7376656532287598 0.003320094896480441\n",
            "Losses Breakdown: 6.182587623596191 4.17277193069458 0.6949673891067505 0.6128473877906799 0.6978508234024048 0.004150230437517166\n",
            "Losses Breakdown: 6.15049934387207 4.17340087890625 0.6934227347373962 0.609744668006897 0.6688644289970398 0.005066660698503256\n",
            "Losses Breakdown: 6.127814292907715 4.170765399932861 0.6935821175575256 0.6093078255653381 0.6480749249458313 0.00608407985419035\n",
            "Losses Breakdown: 6.111876010894775 4.170385837554932 0.6931741237640381 0.608681321144104 0.632709801197052 0.006925338413566351\n",
            "Losses Breakdown: 6.1001787185668945 4.170072078704834 0.6950705051422119 0.6081680059432983 0.6192893981933594 0.007578446064144373\n",
            "Losses Breakdown: 6.09007453918457 4.169859886169434 0.6980628371238708 0.6068720817565918 0.6073536276817322 0.007925753481686115\n",
            "Losses Breakdown: 6.078919887542725 4.169888019561768 0.6945234537124634 0.6075811386108398 0.5987827181816101 0.008145044557750225\n",
            "Losses Breakdown: 6.077447891235352 4.169923782348633 0.7014849185943604 0.606641411781311 0.5911201238632202 0.00827787909656763\n",
            "Losses Breakdown: 6.08396053314209 4.17159366607666 0.7130467891693115 0.6064814925193787 0.5844764709472656 0.008361934684216976\n",
            "Losses Breakdown: 6.062012195587158 4.173996925354004 0.6949125528335571 0.606302797794342 0.5784246325492859 0.008375436067581177\n",
            "Losses Breakdown: 6.056116580963135 4.169405937194824 0.6979108452796936 0.6061992645263672 0.5742085576057434 0.008391806855797768\n",
            "Losses Breakdown: 6.053420066833496 4.169776439666748 0.7004833817481995 0.6052119135856628 0.5695949196815491 0.008353544399142265\n",
            "Losses Breakdown: 6.090898036956787 4.169964790344238 0.7426545023918152 0.6049036383628845 0.5650011301040649 0.008374322205781937\n",
            "Losses Breakdown: 6.0530571937561035 4.1696672439575195 0.7064445614814758 0.6058021783828735 0.5628257393836975 0.008317425847053528\n",
            "Losses Breakdown: 6.035908222198486 4.16973876953125 0.69573974609375 0.6037120819091797 0.5584559440612793 0.008261648006737232\n",
            "Losses Breakdown: 6.085306644439697 4.16953706741333 0.7466937303543091 0.6042183041572571 0.5565784573554993 0.008278832770884037\n",
            "Losses Breakdown: 6.031105995178223 4.169485569000244 0.6948761940002441 0.6045284867286682 0.553979218006134 0.008236289955675602\n",
            "Losses Breakdown: 6.037346363067627 4.17655086517334 0.6965001821517944 0.604535698890686 0.5515738129615784 0.008185897953808308\n",
            "Losses Breakdown: 6.026926040649414 4.169874668121338 0.6940985918045044 0.6049952507019043 0.5497908592224121 0.008166663348674774\n",
            "Losses Breakdown: 6.114566326141357 4.170981407165527 0.7835955619812012 0.6037665605545044 0.5480928421020508 0.00812991987913847\n",
            "Losses Breakdown: 6.023703575134277 4.170668125152588 0.6945496797561646 0.6029337048530579 0.5474468469619751 0.008105223067104816\n",
            "Losses Breakdown: 6.211376667022705 4.169534206390381 0.8812402486801147 0.6081854701042175 0.5443869829177856 0.008030019700527191\n",
            "Losses Breakdown: 6.9903082847595215 4.169984340667725 1.6618539094924927 0.6069438457489014 0.5435011982917786 0.008025039918720722\n",
            "Losses Breakdown: 6.11436653137207 4.169494152069092 0.7837590575218201 0.6106206774711609 0.5424315333366394 0.008061470463871956\n",
            "Losses Breakdown: 6.251852989196777 4.169785022735596 0.9313884973526001 0.6024835109710693 0.5402354598045349 0.007960347458720207\n",
            "Losses Breakdown: 6.480866432189941 4.169902801513672 1.1627404689788818 0.6008750200271606 0.5394396781921387 0.007908377796411514\n",
            "early stopping\n",
            "Outer CV: 3\n",
            "Inner CV: 0\n",
            "Losses Breakdown: 42.970523834228516 27.25686264038086 0.7516797780990601 0.6748523712158203 7.151155948638916 7.1359710693359375\n",
            "Losses Breakdown: 13.277944564819336 8.640301704406738 1.3301244974136353 0.9561454653739929 2.299304962158203 0.05206751823425293\n",
            "Losses Breakdown: 8.000844955444336 4.796840190887451 1.0742465257644653 0.7888214588165283 1.335345983505249 0.0055899787694215775\n",
            "Losses Breakdown: 6.643847942352295 4.313371658325195 0.6958171725273132 0.6619033217430115 0.9702183604240417 0.002537909895181656\n",
            "early stopping\n",
            "Inner CV: 1\n",
            "Losses Breakdown: 43.98491668701172 28.234132766723633 0.7800177335739136 0.6805924773216248 7.149896621704102 7.140275955200195\n",
            "Losses Breakdown: 15.498525619506836 11.940863609313965 0.7153086066246033 0.6178392171859741 2.1845431327819824 0.03997212275862694\n",
            "Losses Breakdown: 7.8887505531311035 5.180310249328613 0.7898183465003967 0.632135808467865 1.2833331823349 0.003152938559651375\n",
            "Losses Breakdown: 6.51950216293335 4.227991580963135 0.7297028303146362 0.6208903789520264 0.9391819834709167 0.0017357906326651573\n",
            "early stopping\n",
            "Inner CV: 2\n",
            "Losses Breakdown: 44.96834945678711 29.176977157592773 0.7451497316360474 0.7026869654655457 7.146745681762695 7.196787357330322\n",
            "Losses Breakdown: 8.992196083068848 5.032332897186279 0.8870792984962463 0.7782840132713318 2.2363440990448 0.05815570801496506\n",
            "Losses Breakdown: 7.7593512535095215 5.102241039276123 0.6994760036468506 0.6389314532279968 1.3129489421844482 0.005753876641392708\n",
            "Losses Breakdown: 6.847139358520508 4.554863452911377 0.7097252011299133 0.6212400794029236 0.9583984017372131 0.00291190086863935\n",
            "Losses Breakdown: 6.4799933433532715 4.32820463180542 0.7122498750686646 0.612919270992279 0.8242208957672119 0.002399092772975564\n",
            "Losses Breakdown: 6.301263332366943 4.219950199127197 0.6991280913352966 0.6105129718780518 0.7689560651779175 0.002715825801715255\n",
            "Losses Breakdown: 6.223067283630371 4.184226036071777 0.7076463103294373 0.607653021812439 0.7202522158622742 0.003289710497483611\n",
            "Losses Breakdown: 6.1829833984375 4.188995838165283 0.6988778114318848 0.6062567830085754 0.6847702264785767 0.004082502331584692\n",
            "Losses Breakdown: 6.18882417678833 4.1852126121521 0.7347080707550049 0.6055078506469727 0.6583737730979919 0.005021408200263977\n",
            "Losses Breakdown: 6.134466171264648 4.183846950531006 0.6997116804122925 0.6058558225631714 0.6391205787658691 0.0059308554045856\n",
            "Losses Breakdown: 6.127853870391846 4.184295177459717 0.7123269438743591 0.5998614430427551 0.6246920228004456 0.006678333040326834\n",
            "Losses Breakdown: 6.112090110778809 4.183273792266846 0.7048594951629639 0.6049426198005676 0.6118283867835999 0.007185833528637886\n",
            "Losses Breakdown: 6.176577091217041 4.182127952575684 0.7806422114372253 0.6045624017715454 0.6016907691955566 0.0075541953556239605\n",
            "Losses Breakdown: 6.129027843475342 4.182475566864014 0.7404517531394958 0.6045095324516296 0.5937536954879761 0.007837324403226376\n",
            "Losses Breakdown: 6.075486660003662 4.182310581207275 0.6972265243530273 0.6022579073905945 0.5857232809066772 0.007968580350279808\n",
            "Losses Breakdown: 6.084009647369385 4.182214260101318 0.711244523525238 0.6026008725166321 0.5799316167831421 0.008017836138606071\n",
            "Losses Breakdown: 6.075130462646484 4.181994438171387 0.7061838507652283 0.6041745543479919 0.5747123956680298 0.008065184578299522\n",
            "Losses Breakdown: 6.060708522796631 4.18265962600708 0.6976270079612732 0.6030688285827637 0.569307804107666 0.008045014925301075\n",
            "Losses Breakdown: 6.103576183319092 4.18158483505249 0.7464109659194946 0.6021483540534973 0.5653613209724426 0.008070459589362144\n",
            "Losses Breakdown: 6.066014766693115 4.181668758392334 0.7134444713592529 0.6002541184425354 0.5625813007354736 0.008066861890256405\n",
            "Losses Breakdown: 6.0462517738342285 4.182363986968994 0.6972048282623291 0.5998706221580505 0.558781623840332 0.008030461147427559\n",
            "Losses Breakdown: 6.189293384552002 4.181551933288574 0.8415510058403015 0.6019430756568909 0.5562179088592529 0.008029310964047909\n",
            "Losses Breakdown: 6.038455009460449 4.181503772735596 0.6949059367179871 0.6007868647575378 0.5532520413398743 0.008006725460290909\n",
            "Losses Breakdown: 6.0560102462768555 4.1815714836120605 0.7157332301139832 0.6008625626564026 0.5499457716941833 0.007897255010902882\n",
            "Losses Breakdown: 6.047365188598633 4.1815056800842285 0.7063496112823486 0.603267252445221 0.5483810901641846 0.007860936224460602\n",
            "Losses Breakdown: 6.030129909515381 4.18132209777832 0.6958019733428955 0.5981309413909912 0.5470097661018372 0.00786516722291708\n",
            "Losses Breakdown: 6.032153606414795 4.181272983551025 0.6955788135528564 0.6023110151290894 0.5451648831367493 0.007825137116014957\n",
            "Losses Breakdown: 6.0269880294799805 4.181100845336914 0.6943696737289429 0.5997942686080933 0.5438995361328125 0.007823629304766655\n",
            "Losses Breakdown: 6.103348255157471 4.181230068206787 0.7736133337020874 0.5992996692657471 0.5414304733276367 0.007774837780743837\n",
            "Losses Breakdown: 6.027543544769287 4.1808695793151855 0.6959244608879089 0.6019783616065979 0.5409935116767883 0.00777774304151535\n",
            "Losses Breakdown: 6.0216193199157715 4.180722713470459 0.6945945024490356 0.5979867577552795 0.5405616164207458 0.007753842510282993\n",
            "Losses Breakdown: 6.047033309936523 4.180688381195068 0.720679759979248 0.5976433157920837 0.5402908325195312 0.0077310409396886826\n",
            "Losses Breakdown: 6.028554916381836 4.180761337280273 0.7021317481994629 0.5987895727157593 0.5391680002212524 0.007704456802457571\n",
            "Losses Breakdown: 6.044490814208984 4.180724620819092 0.7226381897926331 0.5956278443336487 0.5378034114837646 0.007697056978940964\n",
            "Losses Breakdown: 6.042335510253906 4.180685043334961 0.7166403532028198 0.6012481451034546 0.536132276058197 0.007629277650266886\n",
            "Losses Breakdown: 6.048886775970459 4.180986404418945 0.7240142822265625 0.5983036756515503 0.5379638671875 0.007618567440658808\n",
            "early stopping\n",
            "Inner CV: 3\n",
            "Losses Breakdown: 46.36812210083008 30.697376251220703 0.739875853061676 0.6821768283843994 7.136268615722656 7.112424373626709\n",
            "Losses Breakdown: 14.190147399902344 10.181546211242676 1.0217665433883667 0.6799528002738953 2.248063564300537 0.058818407356739044\n",
            "Losses Breakdown: 7.18936824798584 4.477204322814941 0.746100127696991 0.6488860249519348 1.3104891777038574 0.006688668858259916\n",
            "Losses Breakdown: 6.479135513305664 4.210630416870117 0.6973191499710083 0.6151633858680725 0.9531229734420776 0.0028997508343309164\n",
            "Losses Breakdown: 6.237724304199219 4.101701259613037 0.6974852681159973 0.6125867366790771 0.8234776854515076 0.002472870983183384\n",
            "Losses Breakdown: 6.1611151695251465 4.0723700523376465 0.7133153080940247 0.6038981080055237 0.7687316536903381 0.002800042973831296\n",
            "Losses Breakdown: 6.111067771911621 4.080369472503662 0.7006517052650452 0.6045905947685242 0.7220714092254639 0.003384524257853627\n",
            "Losses Breakdown: 6.088956356048584 4.077469825744629 0.7177757024765015 0.6034312844276428 0.6860702037811279 0.004208924248814583\n",
            "Losses Breakdown: 6.0333733558654785 4.071792125701904 0.6949766874313354 0.6024520993232727 0.6590396761894226 0.005112804006785154\n",
            "Losses Breakdown: 6.03361701965332 4.072319984436035 0.711723804473877 0.6029297709465027 0.6406046152114868 0.006039239466190338\n",
            "Losses Breakdown: 6.005763530731201 4.07145357131958 0.6994630694389343 0.6026158928871155 0.6253415942192078 0.006889886688441038\n",
            "Losses Breakdown: 5.991824626922607 4.071428298950195 0.699087381362915 0.6011313796043396 0.612786591053009 0.007391063962131739\n",
            "Losses Breakdown: 5.978733062744141 4.071527004241943 0.6944634914398193 0.6021229028701782 0.6029834151268005 0.007636649534106255\n",
            "Losses Breakdown: 5.968971252441406 4.071319580078125 0.6939651370048523 0.6024820804595947 0.5933233499526978 0.007881867699325085\n",
            "Losses Breakdown: 5.960870742797852 4.071126937866211 0.6936376690864563 0.6001870632171631 0.5879170298576355 0.008002014830708504\n",
            "Losses Breakdown: 5.965926170349121 4.071132659912109 0.7030603289604187 0.6019306778907776 0.5816636085510254 0.00813903659582138\n",
            "Losses Breakdown: 5.947707176208496 4.071218967437744 0.6950575709342957 0.5983494520187378 0.5749779939651489 0.008103319443762302\n",
            "Losses Breakdown: 5.954041957855225 4.0712127685546875 0.7053682208061218 0.599802553653717 0.5695247650146484 0.008133962750434875\n",
            "Losses Breakdown: 5.946979522705078 4.071127414703369 0.7001670002937317 0.6013804078102112 0.5662457942962646 0.008058504201471806\n",
            "Losses Breakdown: 5.940186977386475 4.071088790893555 0.7005181908607483 0.5975621938705444 0.5628983378410339 0.00811954215168953\n",
            "early stopping\n",
            "Inner CV: 4\n",
            "Losses Breakdown: 43.645328521728516 27.93205451965332 0.7413890361785889 0.6763692498207092 7.153292179107666 7.1422224044799805\n",
            "Losses Breakdown: 14.144572257995605 10.535239219665527 0.7145692706108093 0.6361607313156128 2.2057647705078125 0.05283831059932709\n",
            "Losses Breakdown: 7.1889166831970215 4.509275913238525 0.7506805658340454 0.6318923234939575 1.2930737733840942 0.003994310274720192\n",
            "Losses Breakdown: 6.484488487243652 4.196115970611572 0.7005009651184082 0.6350815296173096 0.9507054686546326 0.002084711566567421\n",
            "Losses Breakdown: 6.45145320892334 4.256556510925293 0.7476963996887207 0.6252736449241638 0.8198394775390625 0.002087092027068138\n",
            "Losses Breakdown: 6.321865081787109 4.214641094207764 0.7218172550201416 0.6159672737121582 0.7669960260391235 0.002443247474730015\n",
            "Losses Breakdown: 6.290358543395996 4.1791582107543945 0.7774755954742432 0.6119908094406128 0.7186124324798584 0.0031214228365570307\n",
            "Losses Breakdown: 6.205125331878662 4.166051387786865 0.7427587509155273 0.6099748611450195 0.6824116110801697 0.003928530029952526\n",
            "Losses Breakdown: 6.2066192626953125 4.164167881011963 0.7726795077323914 0.6079912781715393 0.6569857597351074 0.004794885870069265\n",
            "Losses Breakdown: 6.114417552947998 4.163501262664795 0.6993162631988525 0.6075907349586487 0.6384270191192627 0.005582394078373909\n",
            "Losses Breakdown: 6.125982761383057 4.16345739364624 0.7249600887298584 0.6082683801651001 0.6230491995811462 0.006248035002499819\n",
            "Losses Breakdown: 6.081928730010986 4.16288423538208 0.6943687200546265 0.605884313583374 0.6119672060012817 0.006823859177529812\n",
            "Losses Breakdown: 6.082686901092529 4.162980556488037 0.7007189989089966 0.6098033785820007 0.6020450592041016 0.007139264140278101\n",
            "Losses Breakdown: 6.092989921569824 4.163422584533691 0.7225296497344971 0.6059921979904175 0.5936968326568604 0.0073488373309373856\n",
            "Losses Breakdown: 6.109111785888672 4.162750244140625 0.7480011582374573 0.60598224401474 0.5848942399024963 0.007483833935111761\n",
            "Losses Breakdown: 6.057602405548096 4.163037300109863 0.7049383521080017 0.6026003956794739 0.5794112086296082 0.007614979986101389\n",
            "Losses Breakdown: 6.042243480682373 4.162326812744141 0.695716381072998 0.6032636761665344 0.5732668042182922 0.007669399026781321\n",
            "Losses Breakdown: 6.039205551147461 4.162411689758301 0.6945115923881531 0.6048842072486877 0.5697054862976074 0.007692776620388031\n",
            "Losses Breakdown: 6.035221099853516 4.162413597106934 0.693705677986145 0.6057068705558777 0.5656253695487976 0.007770045660436153\n",
            "Losses Breakdown: 6.034832000732422 4.162665843963623 0.6965955495834351 0.6054161190986633 0.5623866319656372 0.007767831906676292\n",
            "Losses Breakdown: 6.03762149810791 4.162320137023926 0.7040241956710815 0.6040421724319458 0.5594993233680725 0.007735603954643011\n",
            "Losses Breakdown: 6.034527778625488 4.162070274353027 0.7056161165237427 0.6040587425231934 0.5551012754440308 0.007681334391236305\n",
            "Losses Breakdown: 6.022630214691162 4.162165641784668 0.6937084197998047 0.6048350930213928 0.5541687607765198 0.00775220338255167\n",
            "early stopping\n",
            "Outer CV: 4\n",
            "Inner CV: 0\n",
            "Losses Breakdown: 44.87984085083008 29.00420379638672 0.7353017926216125 0.8087970614433289 7.16803503036499 7.163502216339111\n",
            "Losses Breakdown: 8.173340797424316 4.387551307678223 0.8088102340698242 0.7326493263244629 2.1990504264831543 0.04527988284826279\n",
            "Losses Breakdown: 6.998721122741699 4.396224021911621 0.7021377682685852 0.6131376624107361 1.2827937602996826 0.004427840001881123\n",
            "Losses Breakdown: 6.501672267913818 4.203819274902344 0.7315167188644409 0.6147946119308472 0.9494400024414062 0.002101616933941841\n",
            "early stopping\n",
            "Inner CV: 1\n",
            "Losses Breakdown: 45.28009033203125 29.419832229614258 0.7277878522872925 0.7445220947265625 7.163190841674805 7.224756717681885\n",
            "Losses Breakdown: 7.752204895019531 4.168490409851074 0.708392322063446 0.6188986301422119 2.208913803100586 0.04751008376479149\n",
            "Losses Breakdown: 7.200054168701172 4.572428226470947 0.7141527533531189 0.6203618049621582 1.289266586303711 0.0038447449915111065\n",
            "Losses Breakdown: 6.494187831878662 4.168120384216309 0.740191638469696 0.6318740248680115 0.9518118500709534 0.002190131228417158\n",
            "Losses Breakdown: 6.251958847045898 4.117554664611816 0.7029128670692444 0.6098140478134155 0.8196179270744324 0.002059351885691285\n",
            "Losses Breakdown: 6.1829118728637695 4.10781192779541 0.6983253955841064 0.6100674271583557 0.7644469141960144 0.002260759240016341\n",
            "Losses Breakdown: 6.142326831817627 4.109605312347412 0.6981106400489807 0.6097536087036133 0.7216798067092896 0.0031777052208781242\n",
            "Losses Breakdown: 6.122226715087891 4.100986003875732 0.7301883101463318 0.6060884594917297 0.6815659999847412 0.003398069180548191\n",
            "Losses Breakdown: 6.078013896942139 4.096969127655029 0.7125014066696167 0.6037391424179077 0.6585166454315186 0.006286864168941975\n",
            "Losses Breakdown: 6.097904205322266 4.0971527099609375 0.7552599310874939 0.6010830998420715 0.6394637823104858 0.0049443538300693035\n",
            "Losses Breakdown: 6.112327575683594 4.097104072570801 0.7817195653915405 0.6033610105514526 0.6244879961013794 0.005655019078403711\n",
            "Losses Breakdown: 6.168005466461182 4.097332954406738 0.8481628894805908 0.604011595249176 0.612350583076477 0.0061480216681957245\n",
            "Losses Breakdown: 6.003890514373779 4.096604347229004 0.699006199836731 0.5996047854423523 0.602100133895874 0.006575320847332478\n",
            "Losses Breakdown: 6.247863292694092 4.096717834472656 0.9517380595207214 0.5989185571670532 0.5937278866767883 0.0067610060796141624\n",
            "Losses Breakdown: 6.024966239929199 4.095954418182373 0.7360104918479919 0.5997022986412048 0.586392879486084 0.006906058639287949\n",
            "Losses Breakdown: 5.983729839324951 4.096249580383301 0.7004064321517944 0.6014097332954407 0.5786548852920532 0.007008908316493034\n",
            "Losses Breakdown: 6.319109916687012 4.096379280090332 1.0457051992416382 0.5963170528411865 0.5735557675361633 0.007152792531996965\n",
            "Losses Breakdown: 6.000491619110107 4.096364974975586 0.7244934439659119 0.6038057208061218 0.5685942769050598 0.0072329360991716385\n",
            "Losses Breakdown: 6.166563510894775 4.096692085266113 0.8989571332931519 0.5970590114593506 0.5665332078933716 0.007321926299482584\n",
            "Losses Breakdown: 5.990786552429199 4.096771717071533 0.7275122404098511 0.5971125960350037 0.5621635913848877 0.007225769106298685\n",
            "Losses Breakdown: 6.278372764587402 4.096163272857666 1.013481616973877 0.6012423038482666 0.5601847767829895 0.007301046047359705\n",
            "Losses Breakdown: 5.995815753936768 4.096127033233643 0.7350130081176758 0.6019498705863953 0.5554366111755371 0.007289260625839233\n",
            "Losses Breakdown: 6.053536891937256 4.095576763153076 0.8012480139732361 0.5962477326393127 0.5531958341598511 0.0072685666382312775\n",
            "Losses Breakdown: 5.973296165466309 4.0958356857299805 0.7194345593452454 0.5999278426170349 0.5508435964584351 0.007254115771502256\n",
            "Losses Breakdown: 6.081624507904053 4.095308780670166 0.8289666175842285 0.6008632183074951 0.5492324233055115 0.007252995856106281\n",
            "early stopping\n",
            "Inner CV: 2\n",
            "Losses Breakdown: 43.1861572265625 27.307592391967773 0.725675106048584 0.8373106122016907 7.141095161437988 7.1744842529296875\n",
            "Losses Breakdown: 15.520672798156738 11.472023010253906 0.8226675987243652 0.6487033367156982 2.5059077739715576 0.07136980444192886\n",
            "Losses Breakdown: 9.251728057861328 5.983409881591797 1.1911327838897705 0.6542538404464722 1.4165350198745728 0.006396620534360409\n",
            "Losses Breakdown: 6.652775287628174 4.168661117553711 0.8525720834732056 0.6203198432922363 1.008427619934082 0.0027948198840022087\n",
            "Losses Breakdown: 6.647562503814697 4.485302925109863 0.7008982300758362 0.6144919395446777 0.8443342447280884 0.0025351871736347675\n",
            "Losses Breakdown: 6.22352409362793 4.130406856536865 0.6987401843070984 0.6098681092262268 0.7817293405532837 0.0027797722723335028\n",
            "Losses Breakdown: 6.199575424194336 4.1558837890625 0.6973930597305298 0.607807993888855 0.7351405024528503 0.0033503021113574505\n",
            "Losses Breakdown: 6.14980411529541 4.13193416595459 0.711243748664856 0.6072648763656616 0.695155918598175 0.004205326549708843\n",
            "Losses Breakdown: 6.093188762664795 4.120100498199463 0.6943441033363342 0.6064200401306152 0.6671906113624573 0.005133689846843481\n",
            "Losses Breakdown: 6.070763111114502 4.118776321411133 0.6935577988624573 0.6056606769561768 0.6466605067253113 0.006108170375227928\n",
            "Losses Breakdown: 6.066403865814209 4.121779918670654 0.7001996636390686 0.6049861311912537 0.6324207782745361 0.007017586380243301\n",
            "Losses Breakdown: 6.068050384521484 4.123623371124268 0.7133493423461914 0.6056589484214783 0.617769718170166 0.00764873344451189\n",
            "Losses Breakdown: 6.033860206604004 4.118884563446045 0.6957360506057739 0.6034259796142578 0.6077717542648315 0.00804163422435522\n",
            "Losses Breakdown: 6.044955253601074 4.118842124938965 0.7162236571311951 0.6035913825035095 0.5980306267738342 0.008267169818282127\n",
            "Losses Breakdown: 6.015331268310547 4.117595672607422 0.6950287818908691 0.6035906672477722 0.5907480716705322 0.008368444629013538\n",
            "Losses Breakdown: 6.008778095245361 4.118132591247559 0.6953386664390564 0.6031908392906189 0.5836814045906067 0.008434347808361053\n",
            "Losses Breakdown: 5.997109889984131 4.115699291229248 0.6933480501174927 0.6019056439399719 0.5777103304862976 0.008446548134088516\n",
            "Losses Breakdown: 5.994919776916504 4.117281436920166 0.6935569047927856 0.6024165153503418 0.5731852054595947 0.008479835465550423\n",
            "Losses Breakdown: 5.987381458282471 4.1149444580078125 0.69374018907547 0.6011121273040771 0.5691096782684326 0.008475002832710743\n",
            "Losses Breakdown: 6.006987571716309 4.117653846740723 0.7148072719573975 0.6018800735473633 0.5642141103744507 0.008431730791926384\n",
            "Losses Breakdown: 5.982933521270752 4.114320755004883 0.6967871785163879 0.602415919303894 0.560994565486908 0.008414817973971367\n",
            "Losses Breakdown: 5.982235431671143 4.114493370056152 0.7006075978279114 0.6008214354515076 0.557947039604187 0.008365520276129246\n",
            "Losses Breakdown: 5.972681045532227 4.1133503913879395 0.6930786371231079 0.6025859713554382 0.5553290843963623 0.008337214589118958\n",
            "Losses Breakdown: 5.974769115447998 4.113183498382568 0.7001714706420898 0.6006402969360352 0.5524635910987854 0.008310160599648952\n",
            "Losses Breakdown: 5.978499412536621 4.113679885864258 0.7061116695404053 0.599494993686676 0.550940752029419 0.008271997794508934\n",
            "Losses Breakdown: 5.965105056762695 4.113692283630371 0.6931315660476685 0.6008849740028381 0.5491392016410828 0.008257418870925903\n",
            "Losses Breakdown: 5.98444938659668 4.114145755767822 0.7133892774581909 0.6014828681945801 0.5472424626350403 0.008188691921532154\n",
            "Losses Breakdown: 5.960752964019775 4.1139092445373535 0.6935352683067322 0.6000370383262634 0.5451197624206543 0.008151762187480927\n",
            "Losses Breakdown: 5.976051330566406 4.1237382888793945 0.7007385492324829 0.5995960235595703 0.5438536405563354 0.008124865591526031\n",
            "Losses Breakdown: 5.956794261932373 4.113411903381348 0.6931962370872498 0.5999667048454285 0.5421454906463623 0.008074114099144936\n",
            "Losses Breakdown: 5.963405132293701 4.1129279136657715 0.7005554437637329 0.6005728840827942 0.5412736535072327 0.008075357414782047\n",
            "Losses Breakdown: 5.9556450843811035 4.114602088928223 0.6934909820556641 0.5995107889175415 0.5400116443634033 0.008029636926949024\n",
            "Losses Breakdown: 5.957417011260986 4.114419937133789 0.6957319974899292 0.5993927717208862 0.5398788452148438 0.007993357256054878\n",
            "Losses Breakdown: 5.956017971038818 4.114091873168945 0.695550799369812 0.6003519296646118 0.5380272269248962 0.007996131666004658\n",
            "Losses Breakdown: 6.079404354095459 4.1359429359436035 0.7987471222877502 0.5994675755500793 0.5372864603996277 0.007960432209074497\n",
            "Losses Breakdown: 5.953505516052246 4.116415977478027 0.6931888461112976 0.599280834197998 0.5366907119750977 0.00792944896966219\n",
            "Losses Breakdown: 5.994473457336426 4.113977432250977 0.7373417019844055 0.600170373916626 0.5351048111915588 0.00787923950701952\n",
            "Losses Breakdown: 5.963020324707031 4.121250152587891 0.6993354558944702 0.5989087820053101 0.5356260538101196 0.007899880409240723\n",
            "early stopping\n",
            "Inner CV: 3\n",
            "Losses Breakdown: 44.43008804321289 28.810686111450195 0.7283482551574707 0.6535237431526184 7.156499862670898 7.081027030944824\n",
            "Losses Breakdown: 13.006919860839844 8.84378719329834 0.7299700379371643 1.0883970260620117 2.2878806591033936 0.05688498914241791\n",
            "Losses Breakdown: 7.833765506744385 4.978227615356445 0.7229558825492859 0.792641818523407 1.3349862098693848 0.004954423755407333\n",
            "Losses Breakdown: 6.761160373687744 4.445349216461182 0.7133277058601379 0.6275253891944885 0.9721605181694031 0.00279775308445096\n",
            "Losses Breakdown: 6.44008207321167 4.232819080352783 0.7561174035072327 0.6172905564308167 0.831499457359314 0.0023555343504995108\n",
            "Losses Breakdown: 6.261908054351807 4.155094146728516 0.7196636199951172 0.6094976663589478 0.7748697996139526 0.0027828544843941927\n",
            "Losses Breakdown: 6.191487789154053 4.119460582733154 0.7324888706207275 0.6075311303138733 0.7287057042121887 0.003301250981166959\n",
            "Losses Breakdown: 6.158662796020508 4.107850551605225 0.7500800490379333 0.6089109182357788 0.6877349019050598 0.004086107015609741\n",
            "Losses Breakdown: 6.130473613739014 4.11069393157959 0.7451552152633667 0.6062718629837036 0.6632499098777771 0.005102653056383133\n",
            "Losses Breakdown: 6.070079803466797 4.109185218811035 0.7034319639205933 0.6086260676383972 0.6427788138389587 0.006057908292859793\n",
            "Losses Breakdown: 6.064591884613037 4.108141899108887 0.7169079780578613 0.6036694645881653 0.6289647221565247 0.006907532457262278\n",
            "Losses Breakdown: 6.028581619262695 4.108161449432373 0.695133626461029 0.6030896306037903 0.6148064732551575 0.0073899864219129086\n",
            "Losses Breakdown: 6.025146961212158 4.108218193054199 0.699144721031189 0.6041899919509888 0.6058377623558044 0.007756376173347235\n",
            "Losses Breakdown: 6.027944087982178 4.107935428619385 0.7124180197715759 0.602469801902771 0.5971621870994568 0.00795828178524971\n",
            "Losses Breakdown: 6.004578113555908 4.107651710510254 0.6965574026107788 0.6046037673950195 0.5876938104629517 0.008071455173194408\n",
            "Losses Breakdown: 5.996475696563721 4.10797643661499 0.6949741840362549 0.6029929518699646 0.582403302192688 0.008128427900373936\n",
            "Losses Breakdown: 6.001166820526123 4.107663631439209 0.7057056427001953 0.6015200614929199 0.5780607461929321 0.008216919377446175\n",
            "Losses Breakdown: 5.998958587646484 4.107570171356201 0.7101338505744934 0.6024459600448608 0.5706204175949097 0.008187811821699142\n",
            "Losses Breakdown: 5.978890895843506 4.107598781585693 0.694316565990448 0.6011004447937012 0.5676979422569275 0.008177182637155056\n",
            "Losses Breakdown: 5.987241268157959 4.107504844665527 0.7055953145027161 0.600519061088562 0.5654561519622803 0.008166024461388588\n",
            "Losses Breakdown: 5.971294403076172 4.107637405395508 0.6940869688987732 0.6006097793579102 0.5607292652130127 0.008231108076870441\n",
            "Losses Breakdown: 5.972125053405762 4.107516288757324 0.6964845657348633 0.6029818058013916 0.5569888949394226 0.00815370213240385\n",
            "Losses Breakdown: 5.967307090759277 4.107511043548584 0.6952965259552002 0.6018842458724976 0.5544918775558472 0.008123078383505344\n",
            "Losses Breakdown: 5.97055196762085 4.107600688934326 0.6981542706489563 0.6024709343910217 0.554238498210907 0.008087548427283764\n",
            "Losses Breakdown: 5.958493709564209 4.107337474822998 0.6933538913726807 0.5999787449836731 0.5498272180557251 0.007996165193617344\n",
            "Losses Breakdown: 5.973800182342529 4.107423782348633 0.7094271183013916 0.5999959111213684 0.5489197373390198 0.008033590391278267\n",
            "Losses Breakdown: 5.957022666931152 4.107650279998779 0.6933912634849548 0.6020215153694153 0.545978307723999 0.007981465198099613\n",
            "early stopping\n",
            "Inner CV: 4\n",
            "Losses Breakdown: 44.6046142578125 28.862701416015625 0.7457669973373413 0.686621367931366 7.153001308441162 7.15652322769165\n",
            "Losses Breakdown: 19.04493522644043 15.408703804016113 0.7025073766708374 0.6305927038192749 2.2449231147766113 0.058208782225847244\n",
            "Losses Breakdown: 8.232284545898438 5.515913009643555 0.7356477975845337 0.666623055934906 1.3088315725326538 0.00526926014572382\n",
            "Losses Breakdown: 6.413578987121582 4.134459018707275 0.7013515830039978 0.6153845191001892 0.959778904914856 0.0026050901506096125\n",
            "Losses Breakdown: 6.2989044189453125 4.102309226989746 0.757135808467865 0.6099079847335815 0.8272337913513184 0.0023173948284238577\n",
            "Losses Breakdown: 6.221730709075928 4.113698959350586 0.7271259427070618 0.6081918478012085 0.7700744867324829 0.002639272017404437\n",
            "Losses Breakdown: 6.110190391540527 4.079537391662598 0.6951543688774109 0.608035683631897 0.7241285443305969 0.003334680339321494\n",
            "Losses Breakdown: 6.061801910400391 4.062876224517822 0.6940807104110718 0.6126238107681274 0.6880635619163513 0.004157667979598045\n",
            "Losses Breakdown: 6.033581733703613 4.055834770202637 0.7090675234794617 0.6041287779808044 0.6595160365104675 0.005034717731177807\n",
            "Losses Breakdown: 6.057882308959961 4.05361795425415 0.7520685195922852 0.6039956212043762 0.6420717835426331 0.00612828740850091\n",
            "Losses Breakdown: 5.993171691894531 4.052315711975098 0.7053742408752441 0.6029127240180969 0.6257734894752502 0.006795283406972885\n",
            "Losses Breakdown: 5.985175132751465 4.052221298217773 0.7061461210250854 0.6047564148902893 0.6147582530975342 0.0072927107103168964\n",
            "Losses Breakdown: 5.958860874176025 4.052250385284424 0.6945180296897888 0.6018281579017639 0.6027049422264099 0.00755936186760664\n",
            "Losses Breakdown: 5.948685169219971 4.05218505859375 0.6947776079177856 0.599188506603241 0.5947452187538147 0.007789258379489183\n",
            "Losses Breakdown: 5.939813137054443 4.052157402038574 0.6937154531478882 0.5993513464927673 0.5866256952285767 0.00796297937631607\n",
            "Losses Breakdown: 5.93943452835083 4.052131652832031 0.6983281373977661 0.6005394458770752 0.5804747939109802 0.00796071533113718\n",
            "Losses Breakdown: 5.933018684387207 4.0521159172058105 0.6995943188667297 0.597687304019928 0.5755723118782043 0.008048616349697113\n",
            "Losses Breakdown: 5.931921005249023 4.052195072174072 0.6976590752601624 0.6024487614631653 0.5714432001113892 0.008174906484782696\n",
            "Losses Breakdown: 5.9266533851623535 4.052063941955566 0.7030652165412903 0.5978137850761414 0.5656465291976929 0.00806370098143816\n",
            "Losses Breakdown: 5.915035247802734 4.051954746246338 0.6934643983840942 0.5996554493904114 0.5618743896484375 0.008086002431809902\n",
            "Losses Breakdown: 5.91288948059082 4.051987171173096 0.6931110620498657 0.6002904176712036 0.559417188167572 0.008083917200565338\n",
            "Losses Breakdown: 5.904886722564697 4.052037239074707 0.6933470964431763 0.595209538936615 0.5562629103660583 0.008029836229979992\n",
            "Losses Breakdown: 5.917420864105225 4.05206823348999 0.7030779719352722 0.5998585820198059 0.5544096231460571 0.00800637248903513\n",
            "Losses Breakdown: 5.904875755310059 4.052082061767578 0.6955533623695374 0.5972729921340942 0.552002489566803 0.007964925840497017\n",
            "Losses Breakdown: 5.913702964782715 4.051971912384033 0.7083066701889038 0.5973285436630249 0.5482003092765808 0.007895322516560555\n",
            "Losses Breakdown: 5.895960330963135 4.051994323730469 0.6941637396812439 0.594629168510437 0.5472485423088074 0.007924787700176239\n",
            "Losses Breakdown: 5.88854455947876 4.051946640014648 0.6936178207397461 0.5909328460693359 0.5441923141479492 0.00785486213862896\n",
            "Losses Breakdown: 5.886745929718018 4.051982879638672 0.6936838626861572 0.590613603591919 0.5426300764083862 0.007835828699171543\n",
            "early stopping\n",
            "Test Accuracy: 80.4310073852539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QRldK-LtjEq",
        "outputId": "6795bf68-311a-40a1-d876-533b28a34f10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[GAT(\n",
              "   (node_norm): NodeNorm()\n",
              "   (conv1): GATConv(1024, 109, heads=9)\n",
              "   (conv2): GATConv(981, 256, heads=10)\n",
              " ), GAT(\n",
              "   (node_norm): NodeNorm()\n",
              "   (conv1): GATConv(1024, 109, heads=9)\n",
              "   (conv2): GATConv(981, 256, heads=10)\n",
              " ), GAT(\n",
              "   (node_norm): NodeNorm()\n",
              "   (conv1): GATConv(1024, 109, heads=9)\n",
              "   (conv2): GATConv(981, 256, heads=10)\n",
              " ), GAT(\n",
              "   (node_norm): NodeNorm()\n",
              "   (conv1): GATConv(1024, 109, heads=9)\n",
              "   (conv2): GATConv(981, 256, heads=10)\n",
              " ), GAT(\n",
              "   (node_norm): NodeNorm()\n",
              "   (conv1): GATConv(1024, 109, heads=9)\n",
              "   (conv2): GATConv(981, 256, heads=10)\n",
              " )]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhC-MgANwqzB"
      },
      "source": [
        "# average the frame embeddings from the five trained GAT models\n",
        "outs = []\n",
        "for model in models:\n",
        "    model.eval()\n",
        "    outs.append(model(data, training=False))\n",
        "out = torch.mean(torch.stack(outs), dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDCVNTyMeQRP",
        "outputId": "a128f32e-ecaa-4bdb-bf24-3852876b62a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "# UMAP Representations of Semantic Frame Embeddings\n",
        "reducer = umap.UMAP(random_state=2020)\n",
        "umap_embedding = reducer.fit_transform(out.cpu().detach().numpy())\n",
        "\n",
        "# plot the graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(\n",
        "    umap_embedding[:, 0],\n",
        "    umap_embedding[:, 1],\n",
        "    color='grey',\n",
        "    alpha=0.03,\n",
        ")\n",
        "\n",
        "# commerce\n",
        "commerce_fn_labels = [\n",
        "    'Commerce_buy', \n",
        "    'Commerce_sell', \n",
        "    'Commerce_goods-transfer', \n",
        "    'Commerce_scenario',\n",
        "    'Price_per_unit',\n",
        "    'Renting_out',\n",
        "    'Shopping'\n",
        "]\n",
        "commerce_IDs = list()\n",
        "for fn_label in commerce_fn_labels:\n",
        "    f = fn.frames(fn_label)[0]\n",
        "    print(f.name, f.ID)\n",
        "    commerce_IDs.append(f.ID)\n",
        "    \n",
        "for i, ID in enumerate(commerce_IDs):\n",
        "    idx = nodes_to_x[ID]\n",
        "    ax.scatter(umap_embedding[:, 0][idx], umap_embedding[:, 1][idx], color='red')\n",
        "    # if commerce_IDs[i] == 1729:\n",
        "    #     ax.annotate(commerce_IDs[i], xy = (umap_embedding[:, 0][idx], umap_embedding[:, 1][idx]), textcoords='data')\n",
        "\n",
        "# vocation\n",
        "vocation_fn_labels = [\n",
        "    'People_by_vocation',\n",
        "    'Becoming_a_member',\n",
        "    'Being_employed',\n",
        "    'Employee_scenario',\n",
        "    'Member_of_military',\n",
        "    'Medical_professionals',\n",
        "]\n",
        "vocation_IDs = list()\n",
        "for fn_label in vocation_fn_labels:\n",
        "    f = fn.frames(fn_label)[0]\n",
        "    print(f.name, f.ID)\n",
        "    vocation_IDs.append(f.ID)\n",
        "    \n",
        "for i, ID in enumerate(vocation_IDs):\n",
        "    idx = nodes_to_x[ID]\n",
        "    ax.scatter(umap_embedding[:, 0][idx], umap_embedding[:, 1][idx], color='green')\n",
        "    # if vocation_IDs[i] == 1733:\n",
        "    #     ax.annotate(vocation_IDs[i], xy = (umap_embedding[:, 0][idx], umap_embedding[:, 1][idx]), textcoords='data')\n",
        "\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "ax.set_facecolor('white')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Commerce_buy 171\n",
            "Commerce_sell 172\n",
            "Commerce_goods-transfer 211\n",
            "Commerce_scenario 82\n",
            "Price_per_unit 2430\n",
            "Renting_out 1729\n",
            "Shopping 1882\n",
            "People_by_vocation 1071\n",
            "Becoming_a_member 1733\n",
            "Being_employed 282\n",
            "Employee_scenario 1061\n",
            "Member_of_military 2670\n",
            "Medical_professionals 255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}