{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Training_and_Evaluation_Multi_task_Learning_GAT_FrameNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT48URqEgNdx"
      },
      "source": [
        "## Download Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaLKIUg85u6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31342d16-6c3b-44a0-b953-c3a3bd72e1e0"
      },
      "source": [
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.1\n",
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSUjlM_66HLg"
      },
      "source": [
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install torch-geometric\n",
        "!pip install flair\n",
        "!pip install laserembeddings\n",
        "!pip install dataclasses\n",
        "!pip install dill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da262N3UESEU"
      },
      "source": [
        "### **Load FrameNet Graph into Graph Attention Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JdDHW6Mcv0C"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(2020) # seed for reproducible numbers\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib notebook\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"framenet_v17\")\n",
        "from nltk.corpus import framenet as fn\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "print(\"...creating networkx FN...\")\n",
        "G = nx.DiGraph()\n",
        "for frame in fn.frames():\n",
        "    G.add_node(frame.ID)\n",
        "    for adj in frame.frameRelations:\n",
        "        G.add_edge(adj.superFrame.ID, adj.subFrame.ID)\n",
        "        G.add_edge(adj.subFrame.ID, adj.superFrame.ID)\n",
        "\n",
        "# initialize frame embeddings with LASER sentence representations \n",
        "print(\"...embedding frames...\")\n",
        "!python -m laserembeddings download-models\n",
        "from laserembeddings import Laser\n",
        "laser = Laser()\n",
        "sentences = [fn.frame(frameID).definition for frameID in G.nodes]\n",
        "frame_embeddings = laser.embed_sentences(sentences, lang='en')\n",
        "\n",
        "# convert networkx G into torch.geometric graph\n",
        "print(\"...generating torch_geometric graph...\")\n",
        "\n",
        "x = torch.from_numpy(frame_embeddings)  # x.shape = (1221, 1024)\n",
        "nodes_to_x = {node: i for i, node in enumerate(G.nodes)}  # map frame ID to index position in x\n",
        "x_to_nodes = {i: node for i, node in enumerate(G.nodes)}  # reverse of nodes_to_x\n",
        "edge_index = torch.Tensor(list(set([(nodes_to_x[src], nodes_to_x[tgt]) for src, tgt in G.edges]))).long()\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = data.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sHklG7UcYK2"
      },
      "source": [
        "### **Auxiliary Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDX9J3_IfrMz"
      },
      "source": [
        "#### Any-Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tDPuWh8H7N2"
      },
      "source": [
        "# monolingual task\n",
        "import nltk\n",
        "nltk.download('framenet_v17')\n",
        "from nltk.corpus import framenet as fn\n",
        "import torch\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    annofile: str\n",
        "    frameName: str\n",
        "    luName: str = ''\n",
        "    lu_idx: list = field(default_factory=list)  # [(start_LU_idx, end_LU_idx [exclusive of space], id), ...]\n",
        "    fe_idx: list = field(default_factory=list) # [(start_FE_idx, end_FE_idx [exclusive of space], feName, id), ...]\n",
        "\n",
        "    # tokenized by flair\n",
        "    tokenized_text: str = ''\n",
        "    tokenized_lu_idx: list = field(default_factory=list)  # [(token_idx, LU), ...]\n",
        "    tokenized_frame_idx: list = field(default_factory=list)  # [(token_idx, frame), ...]\n",
        "    tokenized_fe_idx: list = field(default_factory=list)  # [(token_idx, FE), ...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KKvsjfo2lbK"
      },
      "source": [
        "#### Load Auxiliary Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvKC1HpeZ6MU"
      },
      "source": [
        "any_annos = torch.load(\"any-language-frames/annos_fn_pos_tags.pt\")\n",
        "bfn_annos = torch.load(\"/content/Capstone/frame_embeddings_BFN/annos.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th32t1MXd-0e"
      },
      "source": [
        "### **Actual Task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s85_wAytgYLo"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK24PCHwFWR6"
      },
      "source": [
        "# actual task\n",
        "import nltk\n",
        "nltk.download('framenet_v17')\n",
        "from nltk.corpus import framenet as fn\n",
        "import globalfn\n",
        "from globalfn.annotations import annotation, all_annotations, annotation_annoID\n",
        "from globalfn.alignments import all_alignments\n",
        "import re\n",
        "\n",
        "def extract_annoID(line):\n",
        "    \"\"\"Extract annoID from line\"\"\"\n",
        "    return int(re.findall(r'\\d+', line)[0])\n",
        "\n",
        "div_D = {}\n",
        "\n",
        "# load same en-pt annotation pairs\n",
        "print(\"Load en-pt.results.txt\")\n",
        "with open(\"Capstone/en-pt.results.txt\", \"r\") as rf:\n",
        "    anno1 = anno2 = None\n",
        "    for line in rf:\n",
        "        if line.strip() == \"===============================\":\n",
        "            anno1 = anno2 = None\n",
        "\n",
        "        if \"annoID\" in line and anno1 is None:\n",
        "            anno1 = extract_annoID(line)\n",
        "        elif \"annoID\" in line and anno2 is None:\n",
        "            anno2 = extract_annoID(line)\n",
        "\n",
        "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
        "            tgt_frame_id = fn.frames(annotation_annoID('pt', anno2).frameName)[0].ID\n",
        "            div_D[(anno1, anno2, 'pt')] = (src_frame_id, tgt_frame_id)\n",
        "\n",
        "# load same en-de annotation pairs\n",
        "print(\"Load en-de.results.txt\")\n",
        "with open(\"Capstone/en-de.results.txt\", \"r\") as rf:\n",
        "    anno1 = anno2 = None\n",
        "    for line in rf:\n",
        "        if line.strip() == \"===============================\":\n",
        "            anno1 = anno2 = None\n",
        "\n",
        "        if \"annoID\" in line and anno1 is None:\n",
        "            anno1 = extract_annoID(line)\n",
        "        elif \"annoID\" in line and anno2 is None:\n",
        "            anno2 = extract_annoID(line)\n",
        "\n",
        "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
        "            tgt_frame_id = fn.frames(annotation_annoID('de', anno2).frameName)[0].ID\n",
        "            div_D[(anno1, anno2, 'de')] = (src_frame_id, tgt_frame_id)\n",
        "\n",
        "# load diverging frames en-pt annotation pairs\n",
        "print(\"Load en-pt.same.results.txt\")\n",
        "with open(\"Capstone/en-pt.same.results.txt\", \"r\") as rf:\n",
        "    anno1 = anno2 = None\n",
        "    for line in rf:\n",
        "        if line.strip() == \"===============================\":\n",
        "            anno1 = anno2 = None\n",
        "\n",
        "        if \"annoID\" in line and anno1 is None:\n",
        "            anno1 = extract_annoID(line)\n",
        "        elif \"annoID\" in line and anno2 is None:\n",
        "            anno2 = extract_annoID(line)\n",
        "\n",
        "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
        "            tgt_frame_id = fn.frames(annotation_annoID('pt', anno2).frameName)[0].ID\n",
        "            div_D[(anno1, anno2, 'pt')] = (src_frame_id, tgt_frame_id)\n",
        "\n",
        "# load diverging frames en-de annotation pairs\n",
        "print(\"Load en-de.same.results.txt\")\n",
        "with open(\"Capstone/en-de.same.results.txt\", \"r\") as rf:\n",
        "    anno1 = anno2 = None\n",
        "    for line in rf:\n",
        "        if line.strip() == \"===============================\":\n",
        "            anno1 = anno2 = None\n",
        "\n",
        "        if \"annoID\" in line and anno1 is None:\n",
        "            anno1 = extract_annoID(line)\n",
        "        elif \"annoID\" in line and anno2 is None:\n",
        "            anno2 = extract_annoID(line)\n",
        "\n",
        "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
        "            tgt_frame_id = fn.frames(annotation_annoID('de', anno2).frameName)[0].ID\n",
        "            div_D[(anno1, anno2, 'de')] = (src_frame_id, tgt_frame_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb0aKoqrKzj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b28cf7-4178-4eda-d7c9-827c510da433"
      },
      "source": [
        "# load frames, LUs, and sentence annotations\n",
        "frames = set()\n",
        "lus = set()\n",
        "sents = set()\n",
        "\n",
        "for (anno1, anno2, lang), (src_frame_id, tgt_frame_id) in div_D.items():\n",
        "    frames.add(src_frame_id)\n",
        "    frames.add(tgt_frame_id)\n",
        "\n",
        "    lus.add(annotation_annoID('en', anno1).luName)\n",
        "    lus.add(annotation_annoID(lang, anno2).luName)\n",
        "\n",
        "    sents.add(anno1)\n",
        "    sents.add(anno2)\n",
        "print(len(frames), len(lus), len(sents))  # check length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179 476 788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrXSz16T4JjA"
      },
      "source": [
        "# load multilingual BERT model for LU embedding\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "mbert = TransformerWordEmbeddings('distilbert-base-multilingual-cased', layers='-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peQZE00iM7kz"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_lu_embedding(lang_model, anno_lang, anno_ID):\n",
        "    \"\"\"\n",
        "    Create embeddings for LUs given language model, the annotation language, and the\n",
        "    annotation ID\n",
        "    \"\"\"\n",
        "    tokenized_text = annotation_annoID(anno_lang, anno_ID).tokenized_text\n",
        "    sent = Sentence(tokenized_text, use_tokenizer=False)\n",
        "    lang_model.embed(sent)\n",
        "\n",
        "    tmp_embeds = list()\n",
        "    for i, tok in enumerate(sent):\n",
        "        if annotation_annoID(anno_lang, anno_ID).tokenized_frame_idx[i] != '-':\n",
        "            tmp_embeds.append(tok.embedding)\n",
        "    final_embeds = torch.mean(torch.stack(tmp_embeds), dim=0)\n",
        "    return final_embeds\n",
        "\n",
        "# target lexical units\n",
        "tgt_L = list()\n",
        "for _, anno_ID, lang in div_D.keys():\n",
        "    print(anno_ID)\n",
        "    embedding = get_lu_embedding(mbert, lang, anno_ID)\n",
        "    tgt_L.append(embedding)\n",
        "\n",
        "# source lexical units\n",
        "src_L = list()\n",
        "for anno_ID, _, lang in div_D.keys():\n",
        "    print(anno_ID)\n",
        "    embedding = get_lu_embedding(mbert, 'en', anno_ID)\n",
        "    src_L.append(embedding)\n",
        "\n",
        "tgt_lus = torch.stack(tgt_L).float().to(device)\n",
        "src_lus = torch.stack(src_L).float().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XYOVdKnhKC9"
      },
      "source": [
        "# Load frames for both source and target LUs\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = data.to(device)\n",
        "src_frames = torch.Tensor([nodes_to_x[src_frame_id] for src_frame_id, _ in div_D.values()]).long().to(device)\n",
        "tgt_frames = torch.Tensor([nodes_to_x[tgt_frame_id] for _, tgt_frame_id in div_D.values()]).long().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfexGc5SWuKm"
      },
      "source": [
        "# Load POS tags for both source and target LUs\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pos_to_ind = {\n",
        "    'a': 2,\n",
        "    'adv': 4,\n",
        "    'art': 3,\n",
        "    'c': 8,\n",
        "    'idio': 6,\n",
        "    'intj': 10,\n",
        "    'n': 1,\n",
        "    'num': 9,\n",
        "    'prep': 5,\n",
        "    'pron': 11,\n",
        "    'scon': 7,\n",
        "    'v': 0,\n",
        "    'avp': 12\n",
        "}\n",
        "\n",
        "src_pos = list()\n",
        "tgt_pos = list()\n",
        "\n",
        "# convert part of speech tags associated with LUs into integers\n",
        "for _, anno_ID, lang in div_D.keys():\n",
        "    tgt_pos.append(pos_to_ind[annotation_annoID(lang, anno_ID).luName.split('.')[1]])\n",
        "\n",
        "for anno_ID, _, lang in div_D.keys():\n",
        "    src_pos.append(pos_to_ind[annotation_annoID('en', anno_ID).luName.split('.')[1]])\n",
        "\n",
        "src_pos = torch.LongTensor(src_pos).to(device)\n",
        "tgt_pos = torch.LongTensor(tgt_pos).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi0iC9Rhq0Ga"
      },
      "source": [
        "## **Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO0777-ylbYe"
      },
      "source": [
        "from torch_geometric.utils import dropout_adj\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class NodeNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Node normalization (regularization technique)\n",
        "    \"\"\"\n",
        "    def __init__(self, unbiased=False, eps=1e-5):\n",
        "        super(NodeNorm, self).__init__()\n",
        "        self.unbiased = unbiased\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = torch.mean(x, dim=1, keepdim=True)\n",
        "        std = (torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps).sqrt()\n",
        "        x = (x - mean) / std\n",
        "        return x\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Network\n",
        "    \"\"\"\n",
        "    def __init__(self, data, hid=109, hid2=256, in_head=9, out_head=10):\n",
        "        super(GAT, self).__init__()\n",
        "        self.hid = hid\n",
        "        self.hid2 = hid2\n",
        "        self.in_head = in_head\n",
        "        self.out_head = out_head\n",
        "        \n",
        "        self.node_norm = NodeNorm()\n",
        "        self.conv1 = GATConv(data.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
        "        self.conv2 = GATConv(self.hid*self.in_head, self.hid2, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "    def forward(self, data, training=True):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        \n",
        "        # DropEdge\n",
        "        edge_index, _ = dropout_adj(data.edge_index, training=training)\n",
        "\n",
        "        # Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.\n",
        "        # One can skip them if the dataset is sufficiently large.\n",
        "        x = nn.Dropout(p=0.4)(x)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.gelu(x)\n",
        "        x = self.node_norm(x)\n",
        "        x = nn.Dropout(p=0.4)(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.node_norm(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BekqXgEAfT07"
      },
      "source": [
        "### Auxiliary Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoeTBbyj8XR8"
      },
      "source": [
        "def get_lu_embedding_helper(lang_model, anno):\n",
        "    \"\"\"\n",
        "    Use the `lang_model` to embed the sentence in `anno` to retrieve the word embedding\n",
        "    for the lexical unit.\n",
        "    \"\"\"\n",
        "    tokenized_text = anno.tokenized_text\n",
        "    sent = Sentence(tokenized_text, use_tokenizer=False)\n",
        "    lang_model.embed(sent)\n",
        "\n",
        "    tmp_embeds = list()\n",
        "    for i, tok in enumerate(sent):\n",
        "        if anno.tokenized_frame_idx[i] != '-':\n",
        "            tmp_embeds.append(tok.embedding)\n",
        "    \n",
        "    try:\n",
        "        final_embeds = torch.mean(torch.stack(tmp_embeds), dim=0)\n",
        "    except:\n",
        "        print(anno)\n",
        "        for i, tok in enumerate(sent):\n",
        "            print(tok, anno.tokenized_frame_idx[i])\n",
        "\n",
        "        print(tmp_embeds, len(anno.tokenized_frame_idx))\n",
        "        assert False\n",
        "    return final_embeds\n",
        "\n",
        "def get_lu_embeddings(annos, start, end):\n",
        "    \"\"\"\n",
        "    Get embeddings for lexical units for annos[start:end]\n",
        "    annos: list of annotations\n",
        "    start: start index\n",
        "    end: end index\n",
        "    \"\"\"\n",
        "    L = list()\n",
        "    for i, anno in enumerate(annos[start:end]):\n",
        "        if i % 1000 == 0:\n",
        "            print(anno)\n",
        "        embedding = get_lu_embedding_helper(mbert, anno)\n",
        "        L.append(embedding)\n",
        "    pre_lus = torch.stack(L).float().to(device)  # shape: (19927, 768)\n",
        "    return pre_lus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVgAjE1r8gzd"
      },
      "source": [
        "def get_src_tgt_frames(annos, start, end):\n",
        "    \"\"\"\n",
        "    Get source and target frames for annos[start:end]\n",
        "    annos: list of annotations\n",
        "    start: start index\n",
        "    end: end index\n",
        "    \"\"\"\n",
        "    fn15_to_fn17_mapping = torch.load(\"any-language-frames/fn15_to_fn17_mapping.pt\")\n",
        "    pre_src_frames = list()\n",
        "    unavailable_frames = set()\n",
        "    for i, anno in enumerate(annos[start:end]):\n",
        "        if i % 1000 == 0:\n",
        "            print(anno)\n",
        "            \n",
        "        frameName = anno.frameName\n",
        "        frameName = fn15_to_fn17_mapping.get(frameName, frameName)\n",
        "        retrieved_frame = fn.frames(frameName)[0]\n",
        "        pre_src_frames.append(retrieved_frame.ID)\n",
        "        try:\n",
        "            assert retrieved_frame.name == frameName\n",
        "        except:\n",
        "            print(frameName, retrieved_frame.name)\n",
        "            print(fn.frames(frameName))\n",
        "\n",
        "    # correct frame-to-frame\n",
        "    pre_src_frames = torch.Tensor([nodes_to_x[src_frame_id] for src_frame_id in pre_src_frames]).long().to(device)\n",
        "    pre_tgt_frames = pre_src_frames.clone()\n",
        "    return pre_src_frames, pre_tgt_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3zbiykYfIp3"
      },
      "source": [
        "def get_pos(annos, start, end):\n",
        "    \"\"\"\n",
        "    Get source and target POS tags for annos[start:end]\n",
        "    annos: list of annotations\n",
        "    start: start index\n",
        "    end: end index\n",
        "    \"\"\"\n",
        "    pos_to_ind = {\n",
        "        'a': 2,\n",
        "        'adv': 4,\n",
        "        'art': 3,\n",
        "        'c': 8,\n",
        "        'idio': 6,\n",
        "        'intj': 10,\n",
        "        'n': 1,\n",
        "        'num': 9,\n",
        "        'prep': 5,\n",
        "        'pron': 11,\n",
        "        'scon': 7,\n",
        "        'v': 0,\n",
        "        'avp': 12\n",
        "    }\n",
        "\n",
        "    pre_pos = list()\n",
        "    for i, anno in enumerate(annos[start:end]):\n",
        "        pre_pos.append(pos_to_ind[anno.luName.split('.')[1]])\n",
        "\n",
        "    pre_pos = torch.LongTensor(pre_pos).to(device)\n",
        "    return pre_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLVVTr3tndn4"
      },
      "source": [
        "import random\n",
        "chosen_bfn_annos = random.choices(bfn_annos, k=15000)\n",
        "annos = chosen_bfn_annos + any_annos\n",
        "pre_lus = get_lu_embeddings(annos, 0, len(annos))\n",
        "pre_src_frames, pre_tgt_frames = get_src_tgt_frames(annos, 0, len(annos))\n",
        "pre_pos = get_pos(annos, 0, len(annos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hN9n9DmuSSW"
      },
      "source": [
        "class MultiTaskLossWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-task Loss Function weighted by homoscedastic uncertainty\n",
        "    \"\"\"\n",
        "    def __init__(self, task_num):\n",
        "        super(MultiTaskLossWrapper, self).__init__()\n",
        "        self.task_num = task_num\n",
        "        self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
        "\n",
        "    def forward(self, losses):\n",
        "        total_loss = 0\n",
        "        for i in range(len(losses)):\n",
        "            precision = torch.exp(-self.log_vars[i])\n",
        "            total_loss += torch.sum(precision * losses[i] + self.log_vars[i], -1)\n",
        "        total_loss = torch.mean(total_loss)\n",
        "        return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrJHGf-H4Bpz"
      },
      "source": [
        "### All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJLEN2-7APfS"
      },
      "source": [
        "##### All + Nested CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O876ZQYZZytI"
      },
      "source": [
        "import random\n",
        "def frame_pairs_in_relation_helper(frame_id1, frame_id2):\n",
        "    frame1 = fn.frame(frame_id1)\n",
        "    for frame_x in frame1.frameRelations:\n",
        "        if frame_id2 == frame_x.subID or frame_id2 == frame_x.supID:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def generate_frame_pairs_in_relation(num_pairs):\n",
        "    \"\"\"\n",
        "    Generate frame-LU pairs for binary frame prediction.\n",
        "    num_pairs: number of frame-LU pairs\n",
        "    \"\"\"\n",
        "    frame_pairs_in_relation = list()\n",
        "    count = 0\n",
        "    while len(frame_pairs_in_relation) < num_pairs:\n",
        "        if random.random() < 0.5:\n",
        "            f1 = random.choice(fn.frames()).ID\n",
        "            f2 = random.choice(fn.frames()).ID\n",
        "        else:\n",
        "            f1 = random.choice(fn.frames())\n",
        "            while not f1.frameRelations:\n",
        "                f1 = random.choice(fn.frames())\n",
        "            f2 = random.choice(f1.frameRelations)\n",
        "            f2 = f2.subID if f2.subID != f1.ID else f2.supID\n",
        "            f1 = f1.ID\n",
        "\n",
        "        if f1 == f2:\n",
        "            continue\n",
        "        \n",
        "        count += frame_pairs_in_relation_helper(f1, f2)\n",
        "        frame_pairs_in_relation.append((f1, f2, frame_pairs_in_relation_helper(f1, f2)))\n",
        "\n",
        "    fr_frame_1 = torch.LongTensor([nodes_to_x[f1] for f1, _, _ in frame_pairs_in_relation]).to(device)\n",
        "    fr_frame_2 = torch.LongTensor([nodes_to_x[f1] for _, f2, _ in frame_pairs_in_relation]).to(device)\n",
        "    fr_targets = torch.LongTensor([target for _, _, target in frame_pairs_in_relation]).to(device)\n",
        "    return fr_frame_1, fr_frame_2, fr_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkAVDT02-c1s"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"framenet_v17\")\n",
        "from nltk.corpus import framenet as fn\n",
        "import networkx as nx\n",
        "\n",
        "# Load FrameNet into networkx graph\n",
        "G = nx.DiGraph()\n",
        "for frame in fn.frames():\n",
        "    G.add_node(frame.ID)\n",
        "    for adj in frame.frameRelations:\n",
        "        G.add_edge(adj.superFrame.ID, adj.subFrame.ID)\n",
        "        G.add_edge(adj.subFrame.ID, adj.superFrame.ID)\n",
        "\n",
        "def generate_fr_dist_data(G, size=20000):\n",
        "    \"\"\"\n",
        "    Generate frame-to-frame relations as auxiliary training data.\n",
        "    G: FrameNet graph (in networkx)\n",
        "    \"\"\"\n",
        "    src_nodes = []\n",
        "    tgt_nodes = []\n",
        "    dist = []\n",
        "    all_nodes = list(G)\n",
        "    while len(dist) < size:\n",
        "        src_node = random.choice(all_nodes)\n",
        "        tgt_node = random.choice(all_nodes)\n",
        "        if src_node == tgt_node:\n",
        "            continue\n",
        "        src_nodes.append(nodes_to_x[src_node])\n",
        "        tgt_nodes.append(nodes_to_x[tgt_node])\n",
        "        if not nx.has_path(G, src_node, tgt_node):\n",
        "            dist.append(0)\n",
        "        else:\n",
        "            dist.append(nx.shortest_path_length(G, src_node, tgt_node))\n",
        "\n",
        "    src_nodes = torch.LongTensor(src_nodes).to(device)\n",
        "    tgt_nodes = torch.LongTensor(tgt_nodes).to(device)\n",
        "    dist = torch.FloatTensor(dist).to(device)\n",
        "    return src_nodes, tgt_nodes, dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUCAJvIuaRMR"
      },
      "source": [
        "def scramble_frames(pre_tgt_frames, p=0.3):\n",
        "    \"\"\"\n",
        "    Randomly perturb frames for frame label reconstruction\n",
        "    pre_tgt_frames: correct frame labels for annotations\n",
        "    p: probability of perturbation\n",
        "    \"\"\"\n",
        "    import random\n",
        "    pre_scrambled_tgt_frames = list()\n",
        "    pre_scrambled_targets = list()\n",
        "\n",
        "    for i in range(len(pre_tgt_frames)):\n",
        "        if random.random() < p:\n",
        "            tmp = random.choice(list(x_to_nodes.keys()))\n",
        "            pre_scrambled_tgt_frames.append(tmp)\n",
        "            if tmp != pre_tgt_frames[i].item():\n",
        "                pre_scrambled_targets.append(0)\n",
        "            else:\n",
        "                pre_scrambled_targets.append(1)\n",
        "        else:\n",
        "            pre_scrambled_tgt_frames.append(pre_tgt_frames[i].item())\n",
        "            pre_scrambled_targets.append(1)\n",
        "\n",
        "    pre_scrambled_tgt_frames = torch.LongTensor(pre_scrambled_tgt_frames).to(device)\n",
        "    pre_scrambled_targets = torch.LongTensor(pre_scrambled_targets).to(device)\n",
        "    return pre_scrambled_tgt_frames, pre_scrambled_targets\n",
        "\n",
        "pre_scrambled_tgt_frames, pre_scrambled_targets = scramble_frames(pre_tgt_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMX-UjzDNoP6"
      },
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].flatten().float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLGh7Zbk7JXk"
      },
      "source": [
        "class FFN_frame(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-class classifier layer for frame shift prediction and frame label reconstruction\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_class, pos_dim=16):\n",
        "        super(FFN_frame, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_class = num_class\n",
        "        self.pos_dim = pos_dim\n",
        "\n",
        "        self.pos_embedding = nn.Embedding(13, pos_dim)\n",
        "        self.linear1 = nn.Linear(self.input_dim + pos_dim * 2, self.num_class)\n",
        "    \n",
        "    def forward(self, x, src_pos, tgt_pos):\n",
        "        src_pos = self.pos_embedding(src_pos)\n",
        "        tgt_pos = self.pos_embedding(tgt_pos)\n",
        "        x = torch.cat([x, src_pos, tgt_pos], dim=1)\n",
        "        out = self.linear1(x)\n",
        "        return out\n",
        "\n",
        "class FFN_pred_frame(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Binary frame prediction classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_class, pos_dim=16):\n",
        "        super(FFN_pred_frame, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.pos_embedding = nn.Embedding(13, pos_dim)\n",
        "        self.linear1 = nn.Linear(self.input_dim + pos_dim, 2)\n",
        "    \n",
        "    def forward(self, x, pos):\n",
        "        pos = self.pos_embedding(pos)\n",
        "        x = torch.cat([x, pos], dim=1)\n",
        "        out = self.linear1(x)\n",
        "        return out\n",
        "\n",
        "class FFN_relation_classifier(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Binary frame-to-frame relation classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, ffn_hid = 8, pos_dim=16):\n",
        "        super(FFN_relation_classifier, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(self.input_dim, 2)\n",
        "    \n",
        "    def forward(self, frame1, frame2):\n",
        "        x = torch.cat([frame1, frame2], dim=1)\n",
        "        out = self.linear1(x)\n",
        "        return out\n",
        "\n",
        "class FFN_pred_fr_dist(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Nodes-apart frame distance prediction layer\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(FFN_pred_fr_dist, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.linear1 = nn.Linear(self.input_dim, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 1)\n",
        "    \n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        out = self.linear1(x)\n",
        "        out = nn.ReLU()(out)\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def cross_val_split(data, fold, i):\n",
        "    \"\"\"\n",
        "    Cross-validation split\n",
        "    data: data (training/evaluation split)\n",
        "    fold: total number of folds\n",
        "    i: i-th fold\n",
        "    \"\"\"\n",
        "    splits = [len(data) // fold] * (fold - 1) + [len(data) - sum([len(data) // fold] * (fold - 1))]\n",
        "    held_out = data[i * splits[i]:(i + 1) * splits[i]]\n",
        "    non_held_out = torch.cat([data[:i * splits[i]], data[(i + 1) * splits[i]:]])\n",
        "    return non_held_out, held_out\n",
        "\n",
        "\n",
        "def run():\n",
        "    OUTER_FOLD = 5\n",
        "    INNER_FOLD = 5\n",
        "    ES_PATIENCE = 2\n",
        "    test_accs = list()\n",
        "    for fold_idx in range(OUTER_FOLD):\n",
        "        print(\"Outer CV:\", fold_idx)\n",
        "\n",
        "        # outer CV\n",
        "        all_train_src_frames, test_src_frames = cross_val_split(src_frames, OUTER_FOLD, fold_idx)\n",
        "        all_train_src_lus, test_src_lus = cross_val_split(src_lus, OUTER_FOLD, fold_idx)\n",
        "        all_train_src_pos, test_src_pos = cross_val_split(src_pos, OUTER_FOLD, fold_idx)\n",
        "        all_train_tgt_lus, test_tgt_lus = cross_val_split(tgt_lus, OUTER_FOLD, fold_idx)\n",
        "        all_train_tgt_frames, test_tgt_frames = cross_val_split(tgt_frames, OUTER_FOLD, fold_idx)\n",
        "        all_train_tgt_pos, test_tgt_pos = cross_val_split(tgt_pos, OUTER_FOLD, fold_idx)\n",
        "\n",
        "        # inner CV: features selection\n",
        "        features = {}\n",
        "        for inner_fold_idx in range(INNER_FOLD):\n",
        "            # inner CV\n",
        "            print(\"Inner CV:\", inner_fold_idx)\n",
        "\n",
        "            # initialize models, criterion, and optimizer\n",
        "            model = GAT(data).to(device)\n",
        "            aux_fr_dist_classifier = FFN_pred_fr_dist(256 * 2).to(device)\n",
        "            aux_fr_classifier = FFN_relation_classifier(256 * 2).to(device)\n",
        "            aux_frame_classifier = FFN_pred_frame(256 + 768, 1221).to(device)\n",
        "            frame_classifier = FFN_frame(256 + 768 + 768, 1221).to(device)\n",
        "\n",
        "            aux_fr_dist_criterion = nn.MSELoss()\n",
        "            aux_fr_criterion = nn.CrossEntropyLoss()\n",
        "            aux_frame_criterion = nn.CrossEntropyLoss()\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            wrapper = MultiTaskLossWrapper(5)\n",
        "\n",
        "            optimizer = torch.optim.Adam(list(model.parameters()) + list(frame_classifier.parameters()) + list(aux_frame_classifier.parameters()) + list(aux_fr_classifier.parameters()) + list(aux_fr_dist_classifier.parameters()), \n",
        "                                        lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "            # data for actual\n",
        "            train_src_frames, val_src_frames = cross_val_split(all_train_src_frames, INNER_FOLD, inner_fold_idx)\n",
        "            train_src_lus, val_src_lus = cross_val_split(all_train_src_lus, INNER_FOLD, inner_fold_idx)\n",
        "            train_src_pos, val_src_pos = cross_val_split(all_train_src_pos, INNER_FOLD, inner_fold_idx)\n",
        "            train_tgt_pos, val_tgt_pos = cross_val_split(all_train_tgt_pos, INNER_FOLD, inner_fold_idx)\n",
        "            train_tgt_lus, val_tgt_lus = cross_val_split(all_train_tgt_lus, INNER_FOLD, inner_fold_idx)\n",
        "            train_tgt_frames, val_tgt_frames = cross_val_split(all_train_tgt_frames, INNER_FOLD, inner_fold_idx)\n",
        "\n",
        "            # randomly generated data for aux\n",
        "            fr_frame_1, fr_frame_2, fr_targets = generate_frame_pairs_in_relation(20000)  # aux - fr\n",
        "            src_nodes, tgt_nodes, dist = generate_fr_dist_data(G)  # aux - fr dist\n",
        "            pre_scrambled_tgt_frames, pre_scrambled_targets = scramble_frames(pre_tgt_frames)  # aux - frame\n",
        "\n",
        "            # model development\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)  \n",
        "\n",
        "\n",
        "            min_val_loss = float('inf')\n",
        "            epochs_no_improve = 0\n",
        "            for epoch in range(1000):\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                out = model(data)\n",
        "\n",
        "                # auxiliary task training - fr dist\n",
        "                y = aux_fr_dist_classifier(out[src_nodes], out[tgt_nodes])\n",
        "                aux_fr_dist_loss = aux_fr_dist_criterion(y, dist)\n",
        "\n",
        "                # auxiliary task training - fr binary\n",
        "                y = aux_fr_classifier(out[fr_frame_1], out[fr_frame_2])\n",
        "                aux_fr_loss = aux_fr_criterion(y, fr_targets)\n",
        "\n",
        "                # auxiliary task training - binary frame induction prediction \n",
        "                tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus], dim=1)\n",
        "                y = aux_frame_classifier(tmp_out, pre_pos)\n",
        "                aux_frame_loss = aux_frame_criterion(y, pre_scrambled_targets)\n",
        "\n",
        "                # auxiliary task training - frame restoration\n",
        "                tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus, pre_lus], dim=1)\n",
        "                y = frame_classifier(tmp_out, pre_pos, pre_pos)\n",
        "                aux_frame_2_loss = criterion(y, pre_tgt_frames)\n",
        "\n",
        "                # actual task training\n",
        "                tmp_out = torch.cat([out[train_src_frames], train_src_lus, train_tgt_lus], dim=1)\n",
        "                y = frame_classifier(tmp_out, train_src_pos, train_tgt_pos)\n",
        "                main_loss = criterion(y, train_tgt_frames)\n",
        "\n",
        "                # compute total loss, which is the sum of the loss from the main \n",
        "                # FSP task and the auxiliary tasks\n",
        "                total_loss = wrapper([aux_fr_dist_loss, aux_fr_loss, aux_frame_loss, aux_frame_2_loss, main_loss])\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(\"Losses Breakdown:\", total_loss.item(), aux_fr_dist_loss.item(), aux_fr_loss.item(), aux_frame_loss.item(), aux_frame_2_loss.item(), main_loss.item())\n",
        "                    with torch.no_grad():\n",
        "                        model.eval()\n",
        "                        out = model(data, training=False)\n",
        "                        out = torch.cat([out[train_src_frames], train_src_lus, train_tgt_lus], dim=1)\n",
        "                        y = frame_classifier(out, train_src_pos, train_tgt_pos)\n",
        "                        train_acc = sum(torch.argmax(y, dim=1) == train_tgt_frames).item()/torch.argmax(y, dim=1).shape[0]\n",
        "\n",
        "                        out = model(data, training=False)\n",
        "                        out = torch.cat([out[val_src_frames], val_src_lus, val_tgt_lus], dim=1)\n",
        "                        y = frame_classifier(out, val_src_pos, val_tgt_pos)\n",
        "                        val_acc = sum(torch.argmax(y, dim=1) == val_tgt_frames).item()/torch.argmax(y, dim=1).shape[0]\n",
        "                        val_loss = criterion(y, val_tgt_frames)\n",
        "\n",
        "                        # early stopping\n",
        "                        if val_loss.item() < min_val_loss:\n",
        "                            min_val_loss = val_loss.item()\n",
        "                            epochs_no_improve = 0\n",
        "                        else:\n",
        "                            epochs_no_improve += 1\n",
        "                            if epochs_no_improve >= ES_PATIENCE:\n",
        "                                print(\"early stopping\")\n",
        "                                features[val_loss.item()] = {\"best_epoches\": epoch, \n",
        "                                                            'aux_fr': (fr_frame_1, fr_frame_2, fr_targets), \n",
        "                                                            'aux_fr_dist': (src_nodes, tgt_nodes, dist), \n",
        "                                                            'aux_frame': (pre_scrambled_tgt_frames, pre_scrambled_targets)}\n",
        "                                break\n",
        "                \n",
        "                        \n",
        "        ##################################################################################################################\n",
        "        # model testing: retrain with all data -> eval with test\n",
        "        # choose features\n",
        "        min_val_loss = min(features.keys())\n",
        "        best_features = features[min_val_loss]\n",
        "\n",
        "        # initialize models, criterion, and optimizer\n",
        "        model = GAT(data).to(device)\n",
        "        aux_fr_dist_classifier = FFN_pred_fr_dist(256 * 2).to(device)\n",
        "        aux_fr_classifier = FFN_relation_classifier(256 * 2).to(device)\n",
        "        aux_frame_classifier = FFN_pred_frame(256 + 768, 1221).to(device)\n",
        "        frame_classifier = FFN_frame(256 + 768 + 768, 1221).to(device)\n",
        "\n",
        "        aux_fr_dist_criterion = nn.MSELoss()\n",
        "        aux_fr_criterion = nn.CrossEntropyLoss()\n",
        "        aux_frame_criterion = nn.CrossEntropyLoss()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        wrapper = MultiTaskLossWrapper(5)\n",
        "\n",
        "        optimizer = torch.optim.Adam(list(model.parameters()) + list(frame_classifier.parameters()) + list(aux_frame_classifier.parameters()) + list(aux_fr_classifier.parameters()) + list(aux_fr_dist_classifier.parameters()), \n",
        "                                    lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "        fr_frame_1, fr_frame_2, fr_targets = best_features['aux_fr']\n",
        "        src_nodes, tgt_nodes, dist = best_features['aux_fr_dist']\n",
        "        pre_scrambled_tgt_frames, pre_scrambled_targets = best_features['aux_frame']\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(best_features['best_epoches']):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "\n",
        "            # auxiliary task training - fr dist\n",
        "            y = aux_fr_dist_classifier(out[src_nodes], out[tgt_nodes])\n",
        "            aux_fr_dist_loss = aux_fr_dist_criterion(y, dist)\n",
        "\n",
        "            # auxiliary task training - fr\n",
        "            y = aux_fr_classifier(out[fr_frame_1], out[fr_frame_2])\n",
        "            aux_fr_loss = aux_fr_criterion(y, fr_targets)\n",
        "\n",
        "            # auxiliary task training - binary frame\n",
        "            tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus], dim=1)\n",
        "            y = aux_frame_classifier(tmp_out, pre_pos)\n",
        "            aux_frame_loss = aux_frame_criterion(y, pre_scrambled_targets)\n",
        "\n",
        "            # auxiliary task training - restoration\n",
        "            tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus, pre_lus], dim=1)\n",
        "            y = frame_classifier(tmp_out, pre_pos, pre_pos)\n",
        "            aux_frame_2_loss = criterion(y, pre_tgt_frames)\n",
        "\n",
        "            # actual task training\n",
        "            tmp_out = torch.cat([out[all_train_src_frames], all_train_src_lus, all_train_tgt_lus], dim=1)\n",
        "            y = frame_classifier(tmp_out, all_train_src_pos, all_train_tgt_pos)\n",
        "            main_loss = criterion(y, all_train_tgt_frames)\n",
        "\n",
        "            # compute total loss, which is the sum of the loss from the main \n",
        "            # FSP task and the auxiliary tasks\n",
        "            total_loss = wrapper([aux_fr_loss, aux_fr_dist_loss, aux_frame_loss, aux_frame_2_loss, main_loss])\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            out = model(data, training=False)\n",
        "            out = torch.cat([out[test_src_frames], test_src_lus, test_tgt_lus], dim=1)\n",
        "            y = frame_classifier(out, test_src_pos, test_tgt_pos)\n",
        "            test_accs.append(accuracy(y, test_tgt_frames, topk=(5,))[0])\n",
        "                \n",
        "    return sum(test_accs)/len(test_accs)\n",
        "\n",
        "results = [run() for _ in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSw-NyXWGqTp"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}