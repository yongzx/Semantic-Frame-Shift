{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie1KhuhinkEJ"
   },
   "source": [
    "## Download Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaLKIUg85u6J",
    "outputId": "36e33e37-ee8c-410e-d7c3-01dcd034dd79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1\n",
      "1.7.0+cu101\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.version.cuda)\"\n",
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSUjlM_66HLg",
    "outputId": "9fa18b94-b577-490c-db45-1ff41f3454af"
   },
   "outputs": [],
   "source": [
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
    "!pip install torch-geometric\n",
    "!pip install flair\n",
    "!pip install laserembeddings\n",
    "!pip install dataclasses\n",
    "!pip install dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Da262N3UESEU"
   },
   "source": [
    "### **Load FrameNet Graph into Graph Attention Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JdDHW6Mcv0C",
    "outputId": "e01e68e0-3763-4da8-fc1b-b14cba7c80bc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(2020) # seed for reproducible numbers\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"framenet_v17\")\n",
    "from nltk.corpus import framenet as fn\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "print(\"...creating networkx FN...\")\n",
    "G = nx.DiGraph()\n",
    "for frame in fn.frames():\n",
    "    G.add_node(frame.ID)\n",
    "    for adj in frame.frameRelations:\n",
    "        G.add_edge(adj.superFrame.ID, adj.subFrame.ID)\n",
    "        G.add_edge(adj.subFrame.ID, adj.superFrame.ID)\n",
    "\n",
    "# initialize frame embeddings with LASER sentence representations \n",
    "print(\"...embedding frames...\")\n",
    "!python -m laserembeddings download-models\n",
    "from laserembeddings import Laser\n",
    "laser = Laser()\n",
    "sentences = [fn.frame(frameID).definition for frameID in G.nodes]\n",
    "frame_embeddings = laser.embed_sentences(sentences, lang='en')\n",
    "\n",
    "# convert networkx G into torch.geometric graph\n",
    "print(\"...generating torch_geometric graph...\")\n",
    "\n",
    "x = torch.from_numpy(frame_embeddings)  # x.shape = (1221, 1024)\n",
    "nodes_to_x = {node: i for i, node in enumerate(G.nodes)}  # map frame ID to index position in x\n",
    "x_to_nodes = {i: node for i, node in enumerate(G.nodes)}  # reverse of nodes_to_x\n",
    "edge_index = torch.Tensor(list(set([(nodes_to_x[src], nodes_to_x[tgt]) for src, tgt in G.edges]))).long()\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sHklG7UcYK2"
   },
   "source": [
    "### **Auxiliary Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDX9J3_IfrMz"
   },
   "source": [
    "#### Any-Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tDPuWh8H7N2",
    "outputId": "d8c92caf-a47d-4312-99c2-fcc61b677681"
   },
   "outputs": [],
   "source": [
    "# monolingual task\n",
    "import nltk\n",
    "nltk.download('framenet_v17')\n",
    "from nltk.corpus import framenet as fn\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Annotation:\n",
    "    annofile: str\n",
    "    frameName: str\n",
    "    luName: str = ''\n",
    "    lu_idx: list = field(default_factory=list)  # [(start_LU_idx, end_LU_idx [exclusive of space], id), ...]\n",
    "    fe_idx: list = field(default_factory=list) # [(start_FE_idx, end_FE_idx [exclusive of space], feName, id), ...]\n",
    "\n",
    "    # tokenized by flair\n",
    "    tokenized_text: str = ''\n",
    "    tokenized_lu_idx: list = field(default_factory=list)  # [(token_idx, LU), ...]\n",
    "    tokenized_frame_idx: list = field(default_factory=list)  # [(token_idx, frame), ...]\n",
    "    tokenized_fe_idx: list = field(default_factory=list)  # [(token_idx, FE), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KKvsjfo2lbK"
   },
   "source": [
    "#### Load Auxiliary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvKC1HpeZ6MU"
   },
   "outputs": [],
   "source": [
    "# torch.save(annos, \"any-language-frames/annos.pt\")\n",
    "any_annos = torch.load(\"any-language-frames/annos_fn_pos_tags.pt\")\n",
    "bfn_annos = torch.load(\"/content/Capstone/frame_embeddings_BFN/annos.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th32t1MXd-0e"
   },
   "source": [
    "### **Actual Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2UhFAddnyto"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KK24PCHwFWR6",
    "outputId": "c63cba6d-7e81-4336-ae51-e8e6305a2bb1"
   },
   "outputs": [],
   "source": [
    "# actual task\n",
    "import nltk\n",
    "nltk.download('framenet_v17')\n",
    "from nltk.corpus import framenet as fn\n",
    "import globalfn\n",
    "from globalfn.annotations import annotation, all_annotations, annotation_annoID\n",
    "from globalfn.alignments import all_alignments\n",
    "import re\n",
    "\n",
    "def extract_annoID(line):\n",
    "    \"\"\"Extract annoID from line\"\"\"\n",
    "    return int(re.findall(r'\\d+', line)[0])\n",
    "\n",
    "div_D = {}\n",
    "\n",
    "# same en-pt annotation pairs\n",
    "print(\"Load en-pt.results.txt\")\n",
    "with open(\"Capstone/en-pt.results.txt\", \"r\") as rf:\n",
    "    anno1 = anno2 = None\n",
    "    for line in rf:\n",
    "        if line.strip() == \"===============================\":\n",
    "            anno1 = anno2 = None\n",
    "\n",
    "        if \"annoID\" in line and anno1 is None:\n",
    "            anno1 = extract_annoID(line)\n",
    "        elif \"annoID\" in line and anno2 is None:\n",
    "            anno2 = extract_annoID(line)\n",
    "\n",
    "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
    "            tgt_frame_id = fn.frames(annotation_annoID('pt', anno2).frameName)[0].ID\n",
    "            div_D[(anno1, anno2, 'pt')] = (src_frame_id, tgt_frame_id)\n",
    "\n",
    "# same en-de annotation pairs\n",
    "print(\"Load en-de.results.txt\")\n",
    "with open(\"Capstone/en-de.results.txt\", \"r\") as rf:\n",
    "    anno1 = anno2 = None\n",
    "    for line in rf:\n",
    "        if line.strip() == \"===============================\":\n",
    "            anno1 = anno2 = None\n",
    "\n",
    "        if \"annoID\" in line and anno1 is None:\n",
    "            anno1 = extract_annoID(line)\n",
    "        elif \"annoID\" in line and anno2 is None:\n",
    "            anno2 = extract_annoID(line)\n",
    "\n",
    "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
    "            tgt_frame_id = fn.frames(annotation_annoID('de', anno2).frameName)[0].ID\n",
    "            div_D[(anno1, anno2, 'de')] = (src_frame_id, tgt_frame_id)\n",
    "\n",
    "# diverging frames en-pt annotation pairs\n",
    "print(\"Load en-pt.same.results.txt\")\n",
    "with open(\"Capstone/en-pt.same.results.txt\", \"r\") as rf:\n",
    "    anno1 = anno2 = None\n",
    "    for line in rf:\n",
    "        if line.strip() == \"===============================\":\n",
    "            anno1 = anno2 = None\n",
    "\n",
    "        if \"annoID\" in line and anno1 is None:\n",
    "            anno1 = extract_annoID(line)\n",
    "        elif \"annoID\" in line and anno2 is None:\n",
    "            anno2 = extract_annoID(line)\n",
    "\n",
    "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
    "            tgt_frame_id = fn.frames(annotation_annoID('pt', anno2).frameName)[0].ID\n",
    "            div_D[(anno1, anno2, 'pt')] = (src_frame_id, tgt_frame_id)\n",
    "\n",
    "# diverging frames en-de annotation pairs\n",
    "print(\"Load en-de.same.results.txt\")\n",
    "with open(\"Capstone/en-de.same.results.txt\", \"r\") as rf:\n",
    "    anno1 = anno2 = None\n",
    "    for line in rf:\n",
    "        if line.strip() == \"===============================\":\n",
    "            anno1 = anno2 = None\n",
    "\n",
    "        if \"annoID\" in line and anno1 is None:\n",
    "            anno1 = extract_annoID(line)\n",
    "        elif \"annoID\" in line and anno2 is None:\n",
    "            anno2 = extract_annoID(line)\n",
    "\n",
    "            src_frame_id = fn.frames(annotation_annoID('en', anno1).frameName)[0].ID\n",
    "            tgt_frame_id = fn.frames(annotation_annoID('de', anno2).frameName)[0].ID\n",
    "            div_D[(anno1, anno2, 'de')] = (src_frame_id, tgt_frame_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fb0aKoqrKzj_",
    "outputId": "46eba287-5e04-4a88-979c-d393bd2f1d83"
   },
   "outputs": [],
   "source": [
    "# load frames, LUs, and sentence annotations\n",
    "frames = set()\n",
    "lus = set()\n",
    "sents = set()\n",
    "\n",
    "for (anno1, anno2, lang), (src_frame_id, tgt_frame_id) in div_D.items():\n",
    "    frames.add(src_frame_id)\n",
    "    frames.add(tgt_frame_id)\n",
    "\n",
    "    lus.add(annotation_annoID('en', anno1).luName)\n",
    "    lus.add(annotation_annoID(lang, anno2).luName)\n",
    "\n",
    "    sents.add(anno1)\n",
    "    sents.add(anno2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166,
     "referenced_widgets": [
      "813026cde56c4ea2b1f014ac05461257",
      "8569dd16accc4196bbd02fb6cded321d",
      "502e86512b7e4c0c9af833112313b95c",
      "bab9dacd86d644c6a58ed4b1255b97c8",
      "69f53b2b2c814dd4871684cbd94ed397",
      "5fa94da568b248a7a6e52b105f7e840c",
      "a65f5d3b4c8c45acb2f495e0d9b6ab98",
      "e90b87f836994e99ac464ffc73a11b31",
      "a0e76bdbf0d14c00b3b14eda83c261fd",
      "f9137988060e4ac9a779fde44cac9a5c",
      "8818fc5f0a1b47709b1e7ed51a7c1645",
      "65ef6888909340c5a7f9834cc70c4063",
      "505a8736c0c344db900a68ba92173c53",
      "427118a9f92f42a7bae489951b8acc43",
      "c9277ee7a4e84536945deff49933e58b",
      "e00aed8c1683446cbbc93505c5fb1ab9",
      "7ac024736f594b6ea33d7efcbce89f7f",
      "f93cb0f5cd784602abffc32c7bc38a60",
      "9c306f5750c1455abc48f1fb0f5d7cfb",
      "46e1d348965c4483a37cd6b4a18b98af",
      "accfc19b31244075a228c49edf441c64",
      "5a53ee3be2ce4817bf5ada5aab64b32e",
      "c8be7833d20543458cf843ce3b5d412f",
      "a2d08293152341ef965e9de1de552a4b"
     ]
    },
    "id": "YrXSz16T4JjA",
    "outputId": "d65d1b9d-694f-4cd4-f024-a749085f00a1"
   },
   "outputs": [],
   "source": [
    "# load multilingual BERT model for LU embedding\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "mbert = TransformerWordEmbeddings('distilbert-base-multilingual-cased', layers='-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peQZE00iM7kz",
    "outputId": "0c60ff81-4e4a-4159-8139-1b272b898697"
   },
   "outputs": [],
   "source": [
    "# create embeddings for LUs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_lu_embedding(lang_model, anno_lang, anno_ID):\n",
    "    tokenized_text = annotation_annoID(anno_lang, anno_ID).tokenized_text\n",
    "    sent = Sentence(tokenized_text, use_tokenizer=False)\n",
    "    lang_model.embed(sent)\n",
    "\n",
    "    tmp_embeds = list()\n",
    "    for i, tok in enumerate(sent):\n",
    "        if annotation_annoID(anno_lang, anno_ID).tokenized_frame_idx[i] != '-':\n",
    "            tmp_embeds.append(tok.embedding)\n",
    "    final_embeds = torch.mean(torch.stack(tmp_embeds), dim=0)\n",
    "    return final_embeds\n",
    "\n",
    "tgt_L = list()\n",
    "for _, anno_ID, lang in div_D.keys():\n",
    "    print(anno_ID)\n",
    "    embedding = get_lu_embedding(mbert, lang, anno_ID)\n",
    "    tgt_L.append(embedding)\n",
    "\n",
    "src_L = list()\n",
    "for anno_ID, _, lang in div_D.keys():\n",
    "    print(anno_ID)\n",
    "    embedding = get_lu_embedding(mbert, 'en', anno_ID)\n",
    "    src_L.append(embedding)\n",
    "\n",
    "tgt_lus = torch.stack(tgt_L).float().to(device)\n",
    "src_lus = torch.stack(src_L).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XYOVdKnhKC9"
   },
   "outputs": [],
   "source": [
    "# Load frames for both source and target LUs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "src_frames = torch.Tensor([nodes_to_x[src_frame_id] for src_frame_id, _ in div_D.values()]).long().to(device)\n",
    "tgt_frames = torch.Tensor([nodes_to_x[tgt_frame_id] for _, tgt_frame_id in div_D.values()]).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfexGc5SWuKm"
   },
   "outputs": [],
   "source": [
    "# Load POS tags for both source and target LUs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pos_to_ind = {\n",
    "    'a': 2,\n",
    "    'adv': 4,\n",
    "    'art': 3,\n",
    "    'c': 8,\n",
    "    'idio': 6,\n",
    "    'intj': 10,\n",
    "    'n': 1,\n",
    "    'num': 9,\n",
    "    'prep': 5,\n",
    "    'pron': 11,\n",
    "    'scon': 7,\n",
    "    'v': 0,\n",
    "    'avp': 12\n",
    "}\n",
    "\n",
    "src_pos = list()\n",
    "tgt_pos = list()\n",
    "\n",
    "for _, anno_ID, lang in div_D.keys():\n",
    "    tgt_pos.append(pos_to_ind[annotation_annoID(lang, anno_ID).luName.split('.')[1]])\n",
    "\n",
    "for anno_ID, _, lang in div_D.keys():\n",
    "    src_pos.append(pos_to_ind[annotation_annoID('en', anno_ID).luName.split('.')[1]])\n",
    "\n",
    "src_pos = torch.LongTensor(src_pos).to(device)\n",
    "tgt_pos = torch.LongTensor(tgt_pos).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi0iC9Rhq0Ga"
   },
   "source": [
    "## **Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_PfQTQFqzmI"
   },
   "outputs": [],
   "source": [
    "# Graph Attention Network models\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class NodeNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Node normalization (regularization technique)\n",
    "    \"\"\"\n",
    "    def __init__(self, unbiased=False, eps=1e-5):\n",
    "        super(NodeNorm, self).__init__()\n",
    "        self.unbiased = unbiased\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        std = (torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps).sqrt()\n",
    "        x = (x - mean) / std\n",
    "        return x\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, data, hid=109, hid2=256, in_head=9, out_head=10):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = hid\n",
    "        self.hid2 = hid2\n",
    "        self.in_head = in_head\n",
    "        self.out_head = out_head\n",
    "        \n",
    "        self.node_norm = NodeNorm()\n",
    "        self.conv1 = GATConv(data.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, self.hid2, concat=False,\n",
    "                             heads=self.out_head, dropout=0.6)\n",
    "\n",
    "    def forward(self, data, training=True):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # DropEdge\n",
    "        edge_index, _ = dropout_adj(data.edge_index, training=training)\n",
    "\n",
    "        # Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.\n",
    "        # One can skip them if the dataset is sufficiently large.\n",
    "        x = nn.Dropout(p=0.4)(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.gelu(x)\n",
    "        x = self.node_norm(x)\n",
    "        x = nn.Dropout(p=0.4)(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.node_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BekqXgEAfT07"
   },
   "source": [
    "### Multi-Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoeTBbyj8XR8"
   },
   "outputs": [],
   "source": [
    "def get_lu_embedding_helper(lang_model, anno):\n",
    "    \"\"\"\n",
    "    Use the `lang_model` to embed the sentence in `anno` to retrieve the word embedding\n",
    "    for the lexical unit.\n",
    "    \"\"\"\n",
    "    tokenized_text = anno.tokenized_text\n",
    "    sent = Sentence(tokenized_text, use_tokenizer=False)\n",
    "    lang_model.embed(sent)\n",
    "\n",
    "    tmp_embeds = list()\n",
    "    for i, tok in enumerate(sent):\n",
    "        if anno.tokenized_frame_idx[i] != '-':\n",
    "            tmp_embeds.append(tok.embedding)\n",
    "    \n",
    "    try:\n",
    "        final_embeds = torch.mean(torch.stack(tmp_embeds), dim=0)\n",
    "    except:\n",
    "        print(anno)\n",
    "        for i, tok in enumerate(sent):\n",
    "            print(tok, anno.tokenized_frame_idx[i])\n",
    "\n",
    "        print(tmp_embeds, len(anno.tokenized_frame_idx))\n",
    "        assert False\n",
    "    return final_embeds\n",
    "\n",
    "def get_lu_embeddings(annos, start, end):\n",
    "    \"\"\"\n",
    "    Get embeddings for lexical units for annos[start:end]\n",
    "    annos: list of annotations\n",
    "    start: start index\n",
    "    end: end index\n",
    "    \"\"\"\n",
    "    L = list()\n",
    "    for i, anno in enumerate(annos[start:end]):\n",
    "        if i % 1000 == 0:\n",
    "            print(anno)\n",
    "        embedding = get_lu_embedding_helper(mbert, anno)\n",
    "        L.append(embedding)\n",
    "    pre_lus = torch.stack(L).float().to(device)  # shape: (19927, 768)\n",
    "    return pre_lus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVgAjE1r8gzd"
   },
   "outputs": [],
   "source": [
    "def get_src_tgt_frames(annos, start, end):\n",
    "    \"\"\"\n",
    "    Get source and target frames for annos[start:end]\n",
    "    annos: list of annotations\n",
    "    start: start index\n",
    "    end: end index\n",
    "    \"\"\"\n",
    "    fn15_to_fn17_mapping = torch.load(\"any-language-frames/fn15_to_fn17_mapping.pt\")\n",
    "    pre_src_frames = list()\n",
    "    unavailable_frames = set()\n",
    "    for i, anno in enumerate(annos[start:end]):\n",
    "        if i % 1000 == 0:\n",
    "            print(anno)\n",
    "            \n",
    "        frameName = anno.frameName\n",
    "        frameName = fn15_to_fn17_mapping.get(frameName, frameName)\n",
    "        retrieved_frame = fn.frames(frameName)[0]\n",
    "        pre_src_frames.append(retrieved_frame.ID)\n",
    "        try:\n",
    "            assert retrieved_frame.name == frameName\n",
    "        except:\n",
    "            print(frameName, retrieved_frame.name)\n",
    "            print(fn.frames(frameName))\n",
    "\n",
    "    # correct frame-to-frame\n",
    "    pre_src_frames = torch.Tensor([nodes_to_x[src_frame_id] for src_frame_id in pre_src_frames]).long().to(device)\n",
    "    pre_tgt_frames = pre_src_frames.clone()\n",
    "    return pre_src_frames, pre_tgt_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3zbiykYfIp3"
   },
   "outputs": [],
   "source": [
    "def get_pos(annos, start, end):\n",
    "    \"\"\"\n",
    "    Get source and target POS tags for annos[start:end]\n",
    "    annos: list of annotations\n",
    "    start: start index\n",
    "    end: end index\n",
    "    \"\"\"\n",
    "    pos_to_ind = {\n",
    "        'a': 2,\n",
    "        'adv': 4,\n",
    "        'art': 3,\n",
    "        'c': 8,\n",
    "        'idio': 6,\n",
    "        'intj': 10,\n",
    "        'n': 1,\n",
    "        'num': 9,\n",
    "        'prep': 5,\n",
    "        'pron': 11,\n",
    "        'scon': 7,\n",
    "        'v': 0,\n",
    "        'avp': 12\n",
    "    }\n",
    "\n",
    "    pre_pos = list()\n",
    "    for i, anno in enumerate(annos[start:end]):\n",
    "        pre_pos.append(pos_to_ind[anno.luName.split('.')[1]])\n",
    "\n",
    "    pre_pos = torch.LongTensor(pre_pos).to(device)\n",
    "    return pre_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLVVTr3tndn4",
    "outputId": "c9f31a81-2115-464f-84e2-61f1634054e3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "chosen_bfn_annos = random.choices(bfn_annos, k=15000)\n",
    "annos = chosen_bfn_annos + any_annos\n",
    "pre_lus = get_lu_embeddings(annos, 0, len(annos))\n",
    "pre_src_frames, pre_tgt_frames = get_src_tgt_frames(annos, 0, len(annos))\n",
    "pre_pos = get_pos(annos, 0, len(annos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hN9n9DmuSSW"
   },
   "outputs": [],
   "source": [
    "class MultiTaskLossWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task Loss Function weighted by homoscedastic uncertainty\n",
    "    \"\"\"\n",
    "    def __init__(self, task_num):\n",
    "        super(MultiTaskLossWrapper, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
    "\n",
    "    def forward(self, losses):\n",
    "        total_loss = 0\n",
    "        for i in range(len(losses)):\n",
    "            precision = torch.exp(-self.log_vars[i])\n",
    "            total_loss += torch.sum(precision * losses[i] + self.log_vars[i], -1)\n",
    "        total_loss = torch.mean(total_loss)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrJHGf-H4Bpz"
   },
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJLEN2-7APfS"
   },
   "source": [
    "##### All + Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O876ZQYZZytI"
   },
   "outputs": [],
   "source": [
    "# for c in nx.connected_components(G.to_undirected()):\n",
    "#     print(nx.diameter(G.subgraph(c)))  # result: 0 - 14\n",
    "\n",
    "import random\n",
    "def frame_pairs_in_relation_helper(frame_id1, frame_id2):\n",
    "    frame1 = fn.frame(frame_id1)\n",
    "    for frame_x in frame1.frameRelations:\n",
    "        if frame_id2 == frame_x.subID or frame_id2 == frame_x.supID:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def generate_frame_pairs_in_relation(num_pairs):\n",
    "    \"\"\"\n",
    "    Generate frame-LU pairs for binary frame prediction.\n",
    "    num_pairs: number of frame-LU pairs\n",
    "    \"\"\"\n",
    "    frame_pairs_in_relation = list()\n",
    "    count = 0\n",
    "    while len(frame_pairs_in_relation) < num_pairs:\n",
    "        if random.random() < 0.5:\n",
    "            f1 = random.choice(fn.frames()).ID\n",
    "            f2 = random.choice(fn.frames()).ID\n",
    "        else:\n",
    "            f1 = random.choice(fn.frames())\n",
    "            while not f1.frameRelations:\n",
    "                f1 = random.choice(fn.frames())\n",
    "            f2 = random.choice(f1.frameRelations)\n",
    "            f2 = f2.subID if f2.subID != f1.ID else f2.supID\n",
    "            f1 = f1.ID\n",
    "\n",
    "        if f1 == f2:\n",
    "            continue\n",
    "        \n",
    "        count += frame_pairs_in_relation_helper(f1, f2)\n",
    "        frame_pairs_in_relation.append((f1, f2, frame_pairs_in_relation_helper(f1, f2)))\n",
    "\n",
    "    fr_frame_1 = torch.LongTensor([nodes_to_x[f1] for f1, _, _ in frame_pairs_in_relation]).to(device)\n",
    "    fr_frame_2 = torch.LongTensor([nodes_to_x[f1] for _, f2, _ in frame_pairs_in_relation]).to(device)\n",
    "    fr_targets = torch.LongTensor([target for _, _, target in frame_pairs_in_relation]).to(device)\n",
    "    return fr_frame_1, fr_frame_2, fr_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkAVDT02-c1s",
    "outputId": "08c1aea9-540c-4a30-dd51-7a046f4aaecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"framenet_v17\")\n",
    "from nltk.corpus import framenet as fn\n",
    "import networkx as nx\n",
    "\n",
    "# Load FrameNet into networkx graph\n",
    "G = nx.DiGraph()\n",
    "for frame in fn.frames():\n",
    "    G.add_node(frame.ID)\n",
    "    for adj in frame.frameRelations:\n",
    "        G.add_edge(adj.superFrame.ID, adj.subFrame.ID)\n",
    "        G.add_edge(adj.subFrame.ID, adj.superFrame.ID)\n",
    "\n",
    "def generate_fr_dist_data(G, size=20000):\n",
    "    \"\"\"\n",
    "    Generate frame-to-frame relations as auxiliary training data.\n",
    "    G: FrameNet graph (in networkx)\n",
    "    \"\"\"\n",
    "    src_nodes = []\n",
    "    tgt_nodes = []\n",
    "    dist = []\n",
    "    all_nodes = list(G)\n",
    "    while len(dist) < size:\n",
    "        src_node = random.choice(all_nodes)\n",
    "        tgt_node = random.choice(all_nodes)\n",
    "        if src_node == tgt_node:\n",
    "            continue\n",
    "        src_nodes.append(nodes_to_x[src_node])\n",
    "        tgt_nodes.append(nodes_to_x[tgt_node])\n",
    "        if not nx.has_path(G, src_node, tgt_node):\n",
    "            dist.append(0)\n",
    "        else:\n",
    "            dist.append(nx.shortest_path_length(G, src_node, tgt_node))\n",
    "\n",
    "    src_nodes = torch.LongTensor(src_nodes).to(device)\n",
    "    tgt_nodes = torch.LongTensor(tgt_nodes).to(device)\n",
    "    dist = torch.FloatTensor(dist).to(device)\n",
    "    return src_nodes, tgt_nodes, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUCAJvIuaRMR"
   },
   "outputs": [],
   "source": [
    "def scramble_frames(pre_tgt_frames, p=0.3):\n",
    "    \"\"\"\n",
    "    Randomly perturb frames for frame label reconstruction\n",
    "    pre_tgt_frames: correct frame labels for annotations\n",
    "    p: probability of perturbation\n",
    "    \"\"\"\n",
    "    import random\n",
    "    pre_scrambled_tgt_frames = list()\n",
    "    pre_scrambled_targets = list()\n",
    "\n",
    "    for i in range(len(pre_tgt_frames)):\n",
    "        if random.random() < p:\n",
    "            tmp = random.choice(list(x_to_nodes.keys()))\n",
    "            pre_scrambled_tgt_frames.append(tmp)\n",
    "            if tmp != pre_tgt_frames[i].item():\n",
    "                pre_scrambled_targets.append(0)\n",
    "            else:\n",
    "                pre_scrambled_targets.append(1)\n",
    "        else:\n",
    "            pre_scrambled_tgt_frames.append(pre_tgt_frames[i].item())\n",
    "            pre_scrambled_targets.append(1)\n",
    "\n",
    "    pre_scrambled_tgt_frames = torch.LongTensor(pre_scrambled_tgt_frames).to(device)\n",
    "    pre_scrambled_targets = torch.LongTensor(pre_scrambled_targets).to(device)\n",
    "    return pre_scrambled_tgt_frames, pre_scrambled_targets\n",
    "\n",
    "pre_scrambled_tgt_frames, pre_scrambled_targets = scramble_frames(pre_tgt_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMX-UjzDNoP6"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].flatten().float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLGh7Zbk7JXk",
    "outputId": "7bd1846f-fed0-4d9e-8abb-9a04592c6174"
   },
   "outputs": [],
   "source": [
    "### all with nested CV\n",
    "class FFN_frame(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-class classifier layer for frame shift prediction and frame label reconstruction\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_class, pos_dim=16):\n",
    "        super(FFN_frame, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_class = num_class\n",
    "        self.pos_dim = pos_dim\n",
    "\n",
    "        self.pos_embedding = nn.Embedding(13, pos_dim)\n",
    "        self.linear1 = nn.Linear(self.input_dim + pos_dim * 2, self.num_class)\n",
    "    \n",
    "    def forward(self, x, src_pos, tgt_pos):\n",
    "        src_pos = self.pos_embedding(src_pos)\n",
    "        tgt_pos = self.pos_embedding(tgt_pos)\n",
    "        x = torch.cat([x, src_pos, tgt_pos], dim=1)\n",
    "        out = self.linear1(x)\n",
    "        return out\n",
    "\n",
    "class FFN_pred_frame(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Binary frame prediction classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_class, pos_dim=16):\n",
    "        super(FFN_pred_frame, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.pos_embedding = nn.Embedding(13, pos_dim)\n",
    "        self.linear1 = nn.Linear(self.input_dim + pos_dim, 2)\n",
    "    \n",
    "    def forward(self, x, pos):\n",
    "        pos = self.pos_embedding(pos)\n",
    "        x = torch.cat([x, pos], dim=1)\n",
    "        out = self.linear1(x)\n",
    "        return out\n",
    "\n",
    "class FFN_relation_classifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Binary frame-to-frame relation classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, ffn_hid = 8, pos_dim=16):\n",
    "        super(FFN_relation_classifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.linear1 = nn.Linear(self.input_dim, 2)\n",
    "    \n",
    "    def forward(self, frame1, frame2):\n",
    "        x = torch.cat([frame1, frame2], dim=1)\n",
    "        out = self.linear1(x)\n",
    "        return out\n",
    "\n",
    "class FFN_pred_fr_dist(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Nodes-apart frame distance prediction layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(FFN_pred_fr_dist, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.linear1 = nn.Linear(self.input_dim, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 1)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        out = self.linear1(x)\n",
    "        out = nn.ReLU()(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def cross_val_split(data, fold, i):\n",
    "    \"\"\"\n",
    "    Cross-validation split\n",
    "    data: data (training/evaluation split)\n",
    "    fold: total number of folds\n",
    "    i: i-th fold\n",
    "    \"\"\"\n",
    "    splits = [len(data) // fold] * (fold - 1) + [len(data) - sum([len(data) // fold] * (fold - 1))]\n",
    "    held_out = data[i * splits[i]:(i + 1) * splits[i]]\n",
    "    non_held_out = torch.cat([data[:i * splits[i]], data[(i + 1) * splits[i]:]])\n",
    "    return non_held_out, held_out\n",
    "\n",
    "\n",
    "def run():\n",
    "    OUTER_FOLD = 5\n",
    "    INNER_FOLD = 5\n",
    "    ES_PATIENCE = 2\n",
    "    test_accs = list()\n",
    "    for fold_idx in range(OUTER_FOLD):\n",
    "        print(\"Outer CV:\", fold_idx)\n",
    "\n",
    "        # outer CV\n",
    "        all_train_src_frames, test_src_frames = cross_val_split(src_frames, OUTER_FOLD, fold_idx)\n",
    "        all_train_src_lus, test_src_lus = cross_val_split(src_lus, OUTER_FOLD, fold_idx)\n",
    "        all_train_src_pos, test_src_pos = cross_val_split(src_pos, OUTER_FOLD, fold_idx)\n",
    "        all_train_tgt_lus, test_tgt_lus = cross_val_split(tgt_lus, OUTER_FOLD, fold_idx)\n",
    "        all_train_tgt_frames, test_tgt_frames = cross_val_split(tgt_frames, OUTER_FOLD, fold_idx)\n",
    "        all_train_tgt_pos, test_tgt_pos = cross_val_split(tgt_pos, OUTER_FOLD, fold_idx)\n",
    "\n",
    "        # inner CV: features selection\n",
    "        features = {}\n",
    "        for inner_fold_idx in range(INNER_FOLD):\n",
    "            # inner CV\n",
    "            print(\"Inner CV:\", inner_fold_idx)\n",
    "\n",
    "            # initialize models, criterion, and optimizer\n",
    "            model = GAT(data).to(device)\n",
    "            aux_fr_dist_classifier = FFN_pred_fr_dist(256 * 2).to(device)\n",
    "            aux_fr_classifier = FFN_relation_classifier(256 * 2).to(device)\n",
    "            aux_frame_classifier = FFN_pred_frame(256 + 768, 1221).to(device)\n",
    "            frame_classifier = FFN_frame(256 + 768 + 768, 1221).to(device)\n",
    "\n",
    "            aux_fr_dist_criterion = nn.MSELoss()\n",
    "            aux_fr_criterion = nn.CrossEntropyLoss()\n",
    "            aux_frame_criterion = nn.CrossEntropyLoss()\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            wrapper = MultiTaskLossWrapper(5)\n",
    "\n",
    "            optimizer = torch.optim.Adam(list(model.parameters()) + list(frame_classifier.parameters()) + list(aux_frame_classifier.parameters()) + list(aux_fr_classifier.parameters()) + list(aux_fr_dist_classifier.parameters()), \n",
    "                                        lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "            # data for actual\n",
    "            train_src_frames, val_src_frames = cross_val_split(all_train_src_frames, INNER_FOLD, inner_fold_idx)\n",
    "            train_src_lus, val_src_lus = cross_val_split(all_train_src_lus, INNER_FOLD, inner_fold_idx)\n",
    "            train_src_pos, val_src_pos = cross_val_split(all_train_src_pos, INNER_FOLD, inner_fold_idx)\n",
    "            train_tgt_pos, val_tgt_pos = cross_val_split(all_train_tgt_pos, INNER_FOLD, inner_fold_idx)\n",
    "            train_tgt_lus, val_tgt_lus = cross_val_split(all_train_tgt_lus, INNER_FOLD, inner_fold_idx)\n",
    "            train_tgt_frames, val_tgt_frames = cross_val_split(all_train_tgt_frames, INNER_FOLD, inner_fold_idx)\n",
    "\n",
    "            # randomly generated data for aux\n",
    "            fr_frame_1, fr_frame_2, fr_targets = generate_frame_pairs_in_relation(20000)  # aux - fr\n",
    "            src_nodes, tgt_nodes, dist = generate_fr_dist_data(G)  # aux - fr dist\n",
    "            pre_scrambled_tgt_frames, pre_scrambled_targets = scramble_frames(pre_tgt_frames)  # aux - frame\n",
    "\n",
    "            # model development\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)  \n",
    "\n",
    "\n",
    "            min_val_loss = float('inf')\n",
    "            epochs_no_improve = 0\n",
    "            for epoch in range(1000):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "\n",
    "                # auxiliary task training - fr dist\n",
    "                y = aux_fr_dist_classifier(out[src_nodes], out[tgt_nodes])\n",
    "                aux_fr_dist_loss = aux_fr_dist_criterion(y, dist)\n",
    "\n",
    "                # auxiliary task training - fr binary\n",
    "                y = aux_fr_classifier(out[fr_frame_1], out[fr_frame_2])\n",
    "                aux_fr_loss = aux_fr_criterion(y, fr_targets)\n",
    "\n",
    "                # auxiliary task training - binary frame induction prediction \n",
    "                tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus], dim=1)\n",
    "                y = aux_frame_classifier(tmp_out, pre_pos)\n",
    "                aux_frame_loss = aux_frame_criterion(y, pre_scrambled_targets)\n",
    "\n",
    "                # auxiliary task training - frame restoration\n",
    "                tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus, pre_lus], dim=1)\n",
    "                y = frame_classifier(tmp_out, pre_pos, pre_pos)\n",
    "                aux_frame_2_loss = criterion(y, pre_tgt_frames)\n",
    "\n",
    "                # actual task training\n",
    "                tmp_out = torch.cat([out[train_src_frames], train_src_lus, train_tgt_lus], dim=1)\n",
    "                y = frame_classifier(tmp_out, train_src_pos, train_tgt_pos)\n",
    "                main_loss = criterion(y, train_tgt_frames)\n",
    "\n",
    "                total_loss = wrapper([aux_fr_dist_loss, aux_fr_loss, aux_frame_loss, aux_frame_2_loss, main_loss])\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Losses Breakdown:\", total_loss.item(), aux_fr_dist_loss.item(), aux_fr_loss.item(), aux_frame_loss.item(), aux_frame_2_loss.item(), main_loss.item())\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        out = model(data, training=False)\n",
    "                        out = torch.cat([out[train_src_frames], train_src_lus, train_tgt_lus], dim=1)\n",
    "                        y = frame_classifier(out, train_src_pos, train_tgt_pos)\n",
    "                        train_acc = sum(torch.argmax(y, dim=1) == train_tgt_frames).item()/torch.argmax(y, dim=1).shape[0]\n",
    "                        # print(\"Actual Train Loss:\", loss.item())\n",
    "                        # print(f\"Actual Train Accuracy: {train_acc}\")\n",
    "\n",
    "                        out = model(data, training=False)\n",
    "                        out = torch.cat([out[val_src_frames], val_src_lus, val_tgt_lus], dim=1)\n",
    "                        y = frame_classifier(out, val_src_pos, val_tgt_pos)\n",
    "                        val_acc = sum(torch.argmax(y, dim=1) == val_tgt_frames).item()/torch.argmax(y, dim=1).shape[0]\n",
    "                        val_loss = criterion(y, val_tgt_frames)\n",
    "                        # print(f\">>>> Val Accuracy: {val_acc}\")\n",
    "\n",
    "                        # early stopping\n",
    "                        if val_loss.item() < min_val_loss:\n",
    "                            min_val_loss = val_loss.item()\n",
    "                            epochs_no_improve = 0\n",
    "                        else:\n",
    "                            epochs_no_improve += 1\n",
    "                            if epochs_no_improve >= ES_PATIENCE:\n",
    "                                print(\"early stopping\")\n",
    "                                features[val_loss.item()] = {\"best_epoches\": epoch, \n",
    "                                                            'aux_fr': (fr_frame_1, fr_frame_2, fr_targets), \n",
    "                                                            'aux_fr_dist': (src_nodes, tgt_nodes, dist), \n",
    "                                                            'aux_frame': (pre_scrambled_tgt_frames, pre_scrambled_targets)}\n",
    "                                break\n",
    "                \n",
    "                        \n",
    "        ##################################################################################################################\n",
    "        # model testing: retrain with all data -> eval with test\n",
    "        # choose features\n",
    "        min_val_loss = min(features.keys())\n",
    "        best_features = features[min_val_loss]\n",
    "\n",
    "        # initialize models, criterion, and optimizer\n",
    "        model = GAT(data).to(device)\n",
    "        aux_fr_dist_classifier = FFN_pred_fr_dist(256 * 2).to(device)\n",
    "        aux_fr_classifier = FFN_relation_classifier(256 * 2).to(device)\n",
    "        aux_frame_classifier = FFN_pred_frame(256 + 768, 1221).to(device)\n",
    "        frame_classifier = FFN_frame(256 + 768 + 768, 1221).to(device)\n",
    "\n",
    "        aux_fr_dist_criterion = nn.MSELoss()\n",
    "        aux_fr_criterion = nn.CrossEntropyLoss()\n",
    "        aux_frame_criterion = nn.CrossEntropyLoss()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        wrapper = MultiTaskLossWrapper(5)\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(frame_classifier.parameters()) + list(aux_frame_classifier.parameters()) + list(aux_fr_classifier.parameters()) + list(aux_fr_dist_classifier.parameters()), \n",
    "                                    lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "        fr_frame_1, fr_frame_2, fr_targets = best_features['aux_fr']\n",
    "        src_nodes, tgt_nodes, dist = best_features['aux_fr_dist']\n",
    "        pre_scrambled_tgt_frames, pre_scrambled_targets = best_features['aux_frame']\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(best_features['best_epoches']):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "\n",
    "            # auxiliary task training - fr dist\n",
    "            y = aux_fr_dist_classifier(out[src_nodes], out[tgt_nodes])\n",
    "            aux_fr_dist_loss = aux_fr_dist_criterion(y, dist)\n",
    "\n",
    "            # auxiliary task training - fr\n",
    "            y = aux_fr_classifier(out[fr_frame_1], out[fr_frame_2])\n",
    "            aux_fr_loss = aux_fr_criterion(y, fr_targets)\n",
    "\n",
    "            # auxiliary task training - binary frame\n",
    "            tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus], dim=1)\n",
    "            y = aux_frame_classifier(tmp_out, pre_pos)\n",
    "            aux_frame_loss = aux_frame_criterion(y, pre_scrambled_targets)\n",
    "\n",
    "            # auxiliary task training - restoration\n",
    "            tmp_out = torch.cat([out[pre_scrambled_tgt_frames], pre_lus, pre_lus], dim=1)\n",
    "            y = frame_classifier(tmp_out, pre_pos, pre_pos)\n",
    "            aux_frame_2_loss = criterion(y, pre_tgt_frames)\n",
    "\n",
    "            # actual task training\n",
    "            tmp_out = torch.cat([out[all_train_src_frames], all_train_src_lus, all_train_tgt_lus], dim=1)\n",
    "            y = frame_classifier(tmp_out, all_train_src_pos, all_train_tgt_pos)\n",
    "            main_loss = criterion(y, all_train_tgt_frames)\n",
    "            \n",
    "            total_loss = wrapper([aux_fr_loss, aux_fr_dist_loss, aux_frame_loss, aux_frame_2_loss, main_loss])\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(data, training=False)\n",
    "            out = torch.cat([out[test_src_frames], test_src_lus, test_tgt_lus], dim=1)\n",
    "            y = frame_classifier(out, test_src_pos, test_tgt_pos)\n",
    "            test_accs.append(accuracy(y, test_tgt_frames, topk=(5,))[0])\n",
    "                \n",
    "    return sum(test_accs)/len(test_accs)\n",
    "\n",
    "results = [run() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhC-MgANwqzB"
   },
   "outputs": [],
   "source": [
    "# average the frame embeddings from the five trained GAT models\n",
    "outs = []\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    outs.append(model(data, training=False))\n",
    "out = torch.mean(torch.stack(outs), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "VDCVNTyMeQRP",
    "outputId": "a128f32e-ecaa-4bdb-bf24-3852876b62a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commerce_buy 171\n",
      "Commerce_sell 172\n",
      "Commerce_goods-transfer 211\n",
      "Commerce_scenario 82\n",
      "Price_per_unit 2430\n",
      "Renting_out 1729\n",
      "Shopping 1882\n",
      "People_by_vocation 1071\n",
      "Becoming_a_member 1733\n",
      "Being_employed 282\n",
      "Employee_scenario 1061\n",
      "Member_of_military 2670\n",
      "Medical_professionals 255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UMAP Representations of Semantic Frame Embeddings\n",
    "reducer = umap.UMAP(random_state=2020)\n",
    "umap_embedding = reducer.fit_transform(out.cpu().detach().numpy())\n",
    "\n",
    "# plot the graph\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(\n",
    "    umap_embedding[:, 0],\n",
    "    umap_embedding[:, 1],\n",
    "    color='grey',\n",
    "    alpha=0.03,\n",
    ")\n",
    "\n",
    "# commerce\n",
    "commerce_fn_labels = [\n",
    "    'Commerce_buy', \n",
    "    'Commerce_sell', \n",
    "    'Commerce_goods-transfer', \n",
    "    'Commerce_scenario',\n",
    "    'Price_per_unit',\n",
    "    'Renting_out',\n",
    "    'Shopping'\n",
    "]\n",
    "\n",
    "commerce_IDs = list()\n",
    "for fn_label in commerce_fn_labels:\n",
    "    f = fn.frames(fn_label)[0]\n",
    "    print(f.name, f.ID)\n",
    "    commerce_IDs.append(f.ID)\n",
    "    \n",
    "for i, ID in enumerate(commerce_IDs):\n",
    "    idx = nodes_to_x[ID]\n",
    "    ax.scatter(umap_embedding[:, 0][idx], umap_embedding[:, 1][idx], color='red')\n",
    "    # if commerce_IDs[i] == 1729:\n",
    "    #     ax.annotate(commerce_IDs[i], xy = (umap_embedding[:, 0][idx], umap_embedding[:, 1][idx]), textcoords='data')\n",
    "\n",
    "# vocation\n",
    "vocation_fn_labels = [\n",
    "    'People_by_vocation',\n",
    "    'Becoming_a_member',\n",
    "    'Being_employed',\n",
    "    'Employee_scenario',\n",
    "    'Member_of_military',\n",
    "    'Medical_professionals',\n",
    "]\n",
    "vocation_IDs = list()\n",
    "for fn_label in vocation_fn_labels:\n",
    "    f = fn.frames(fn_label)[0]\n",
    "    print(f.name, f.ID)\n",
    "    vocation_IDs.append(f.ID)\n",
    "    \n",
    "for i, ID in enumerate(vocation_IDs):\n",
    "    idx = nodes_to_x[ID]\n",
    "    ax.scatter(umap_embedding[:, 0][idx], umap_embedding[:, 1][idx], color='green')\n",
    "    # if vocation_IDs[i] == 1733:\n",
    "    #     ax.annotate(vocation_IDs[i], xy = (umap_embedding[:, 0][idx], umap_embedding[:, 1][idx]), textcoords='data')\n",
    "\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "ax.set_facecolor('white')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "UMAP-frame-embeddings",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "427118a9f92f42a7bae489951b8acc43": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46e1d348965c4483a37cd6b4a18b98af": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2d08293152341ef965e9de1de552a4b",
      "placeholder": "​",
      "style": "IPY_MODEL_c8be7833d20543458cf843ce3b5d412f",
      "value": " 542M/542M [00:08&lt;00:00, 62.6MB/s]"
     }
    },
    "502e86512b7e4c0c9af833112313b95c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fa94da568b248a7a6e52b105f7e840c",
      "max": 466,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69f53b2b2c814dd4871684cbd94ed397",
      "value": 466
     }
    },
    "505a8736c0c344db900a68ba92173c53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5a53ee3be2ce4817bf5ada5aab64b32e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fa94da568b248a7a6e52b105f7e840c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65ef6888909340c5a7f9834cc70c4063": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e00aed8c1683446cbbc93505c5fb1ab9",
      "placeholder": "​",
      "style": "IPY_MODEL_c9277ee7a4e84536945deff49933e58b",
      "value": " 996k/996k [00:00&lt;00:00, 3.62MB/s]"
     }
    },
    "69f53b2b2c814dd4871684cbd94ed397": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7ac024736f594b6ea33d7efcbce89f7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c306f5750c1455abc48f1fb0f5d7cfb",
       "IPY_MODEL_46e1d348965c4483a37cd6b4a18b98af"
      ],
      "layout": "IPY_MODEL_f93cb0f5cd784602abffc32c7bc38a60"
     }
    },
    "813026cde56c4ea2b1f014ac05461257": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_502e86512b7e4c0c9af833112313b95c",
       "IPY_MODEL_bab9dacd86d644c6a58ed4b1255b97c8"
      ],
      "layout": "IPY_MODEL_8569dd16accc4196bbd02fb6cded321d"
     }
    },
    "8569dd16accc4196bbd02fb6cded321d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8818fc5f0a1b47709b1e7ed51a7c1645": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_427118a9f92f42a7bae489951b8acc43",
      "max": 995526,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_505a8736c0c344db900a68ba92173c53",
      "value": 995526
     }
    },
    "9c306f5750c1455abc48f1fb0f5d7cfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a53ee3be2ce4817bf5ada5aab64b32e",
      "max": 541808922,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_accfc19b31244075a228c49edf441c64",
      "value": 541808922
     }
    },
    "a0e76bdbf0d14c00b3b14eda83c261fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8818fc5f0a1b47709b1e7ed51a7c1645",
       "IPY_MODEL_65ef6888909340c5a7f9834cc70c4063"
      ],
      "layout": "IPY_MODEL_f9137988060e4ac9a779fde44cac9a5c"
     }
    },
    "a2d08293152341ef965e9de1de552a4b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a65f5d3b4c8c45acb2f495e0d9b6ab98": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "accfc19b31244075a228c49edf441c64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bab9dacd86d644c6a58ed4b1255b97c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e90b87f836994e99ac464ffc73a11b31",
      "placeholder": "​",
      "style": "IPY_MODEL_a65f5d3b4c8c45acb2f495e0d9b6ab98",
      "value": " 466/466 [00:12&lt;00:00, 36.0B/s]"
     }
    },
    "c8be7833d20543458cf843ce3b5d412f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9277ee7a4e84536945deff49933e58b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e00aed8c1683446cbbc93505c5fb1ab9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e90b87f836994e99ac464ffc73a11b31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9137988060e4ac9a779fde44cac9a5c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f93cb0f5cd784602abffc32c7bc38a60": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
